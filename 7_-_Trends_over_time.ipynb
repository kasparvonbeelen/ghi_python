{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/kasparvonbeelen/ghi_python/4-tables?labpath=7_-_Trends_over_time.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 7: Studying trends over time in Political Manifestos (1964-2020)\n",
    "\n",
    "\n",
    "## Text Mining for Historians (with Python)\n",
    "## A Gentle Introduction to Working with Textual Data in Python\n",
    "\n",
    "### Created by Kaspar Beelen and Luke Blaxill\n",
    "\n",
    "### For the German Historical Institute, London\n",
    "\n",
    "<img align=\"left\" src=\"https://www.ghil.ac.uk/typo3conf/ext/wacon_ghil/Resources/Public/Images/institute_icon_small.png\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook combines the insights and techniques from previous sessions and explores a more realistic research example that focuses on studying trends over time in the UK Political Manifestos between 1964 and 2020. By creatively putting the stuff together you've seen so far, you can start to interrogate these corpora from a longitudinal perspective.\n",
    "\n",
    "This notebook will focus on plotting timelines showing (relative) frequencies for selected (or target) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51110_201505.txt 51320_201505.txt 51421_200505.txt 51620_201706.txt\r\n",
      "51110_201706.txt 51320_201706.txt 51421_201505.txt 51620_201912.txt\r\n",
      "51110_201912.txt 51320_201912.txt 51421_201706.txt 51621_201505.txt\r\n",
      "51210_201505.txt 51330_198306.txt 51421_201912.txt 51901_201505.txt\r\n",
      "51210_201706.txt 51330_198706.txt 51430_201912.txt 51901_201706.txt\r\n",
      "51210_201912.txt 51340_201505.txt 51620_196410.txt 51901_201912.txt\r\n",
      "51320_196603.txt 51340_201912.txt 51620_197006.txt 51902_199705.txt\r\n",
      "51320_197006.txt 51420_196603.txt 51620_197402.txt 51902_200106.txt\r\n",
      "51320_197402.txt 51420_197006.txt 51620_197410.txt 51902_201505.txt\r\n",
      "51320_197410.txt 51420_197402.txt 51620_197905.txt 51902_201706.txt\r\n",
      "51320_197905.txt 51420_197410.txt 51620_198306.txt 51902_201912.txt\r\n",
      "51320_198306.txt 51420_197905.txt 51620_198706.txt 51903_201505.txt\r\n",
      "51320_198706.txt 51420_198306.txt 51620_199204.txt 51903_201706.txt\r\n",
      "51320_199204.txt 51420_198706.txt 51620_199705.txt 51903_201912.txt\r\n",
      "51320_199705.txt 51421_199204.txt 51620_200106.txt 51951_200106.txt\r\n",
      "51320_200106.txt 51421_199705.txt 51620_200505.txt 51951_201505.txt\r\n",
      "51320_200505.txt 51421_200106.txt 51620_201505.txt 51951_201706.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/manifestos/Processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start with collecting paths to our manifesto files. These are stored as separate `.txt` files one for each manifesto. As seen previously, we rely on an external library that we need to import first.\n",
    "\n",
    "We first define where our data is stored and create a `Path()` object to which we apply `Path.glob()`, with `\"*.txt\"` as the argument. The asterisk `*` serves as a wild card. Basically what are saying here is: in the folder `'working_data/Manifestos/Processed/'` return as files ending with `.txt` (we don't care what precedes the `.txt` extension).\n",
    "\n",
    "There is one last technicality we can ignore for now: we convert the result of `.glob()` to a `list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "folder = Path('working_data/manifestos/Processed/')\n",
    "files = folder.glob('*.txt')\n",
    "file_paths = list(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count the number of documents in our corpus by applying the `len()` function to the variable `file_paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, using index notation, we print the path to the first file in our corpus (not the content itself, this is for later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've used several tools such as `word_tokenize()` and `Counter()` to count tokens. In this Notebook, we want to repeat this procedure for all documents in our collection. We can write a **function** that takes a path as an argument and returns a mapping between word types and their frequencies. \n",
    "\n",
    "The code cell below takes care of this we first import the tools we need and create a function to open a document and compute word frequencies. To create this function, we use the `def` keyword followed by a name, parentheses and a colon. The word between the parenthesis defines the arguments this function accepts.\n",
    "\n",
    "All the code inside this function should look familiar. For each given path, the function executes the sequence of statements (note the indentation) and returns the variable `word_counts`.\n",
    "\n",
    "It is important to run the cell below (but only once). Python will then store the function `count_words` into memory so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # import Counter for computing word frequencies\n",
    "import nltk # import nltk\n",
    "nltk.download('punkt') # we need this download to use the tokenizer\n",
    "from nltk.tokenize import word_tokenize # impor the tool for tokenizing strings\n",
    "\n",
    "def count_words(path): # define the function and the arguments it accepts\n",
    "    text = open(path,'r').read() # open the document\n",
    "    text_lowercase = text.lower() # lowercase the text and store in text_lowercase\n",
    "    tokens = word_tokenize(text_lowercase) # get the tokens as a list\n",
    "    word_counts = Counter(tokens) # compute word frequencies with Counter()\n",
    "    return word_counts # return the word frequencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the function we created is ready, print`count_words`. You'll see that it is now stored as a `function` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this function to the first document by passing `file_paths[0]` as an argument. The second line prints the ten most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = count_words(file_paths[0])\n",
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to count words, but the goal is to plot word counts over time. When you look more closely at the path and file name (i.e. the part after the last `/` you'll notice that it encodes a time-stamp.\n",
    "\n",
    "The filenames are structured as follows `{party identifier}_{year}{month}.{extension}`. Such a setup—where the filename encodes metadata is rather common when working with heritage collections. We can now start writing a function that extracts the time-stamp from the filename.\n",
    "\n",
    "First, we print get the file name from the full path (we use the `.stem` attribute here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file = file_paths[0]\n",
    "print(first_file)\n",
    "file_name = first_file.stem\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need to extract the date information from the file name. We slice the first four characters from the string and convert it to an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name[-6:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(file_name[-6:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly changing the indices we can capture both year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = int(file_name[-6:-2])\n",
    "month = int(file_name[-2:])\n",
    "print(year, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the very last step, we use a library `datetime` to convert these numbers (year and month are just integer values) to a proper timestamp. Since we don't have access to the exact day we just use the first day of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "time_stamp = datetime.datetime(year,month,1)\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That were many steps! Since we like to repear it for each file, it is again useful to package them in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path2timestamp(path):\n",
    "    file_name = path.stem\n",
    "    year = int(file_name[-6:-2])\n",
    "    month = int(file_name[-2:])\n",
    "    time_stamp = datetime.datetime(year,month,1)\n",
    "    return time_stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to explore our corpus! We iterate over our corpus with a `for` and apply both our functions to each document. To keep track of our yearly counts we store the results in a dictionary to map time-stamps to word counts.\n",
    "\n",
    "The code below reminds you how iteration works. We loop over each element in `file_paths`, and `p` (the loop variable) takes each value (path) in turn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in file_paths[:10]:\n",
    "    print(p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to choose any name for `p` as long as you use it consistently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in file_paths[:10]:\n",
    "    print(fp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `for` loop below raises a `NameError` as we did not define `pp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in file_paths[:10]:\n",
    "    print(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need one more element to make our word-counting program complete: we need to keep track of word counts for each year. For this, we can use a dictionary and increment the count while looping over the text files.\n",
    "\n",
    "To see how incremental counting works, first create a new variable `i` with the value zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we run the cell below, we add `1` to `i` using the `+=` operator. Run the cell multiple time until you understand how this works!\n",
    "\n",
    "Note the difference between `=` and `+=`, former is used to declare variable, the latter to incrementally **add** a value to `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply incremental counting also the values of dictionaries. This will be necessary as we traverse each file in our collection, look up the year it was written and how often our query appears, and add this value to the dictionary, meaning we increment the count for that specific year. \n",
    "\n",
    "We demonstrate this with a practical example. We have a dictionary, with keys referring to years (1950 and 1951) and values to word counts (1 and 10), meaning that in 1950 our query appeared once and ten times one year later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year2counts = {1950:1,1951:10}\n",
    "print(year2counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we open a new document in which our query appears twice. We can now add two for 1950."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year2counts[1950]+=1\n",
    "print(year2counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the value for 1950 is 3. We can, of course, do the same for 1951."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year2counts[1951]+=5\n",
    "year2counts[1951]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incremental counting of word frequencies happens in line 8 in the code cell below. Instead of dictionary with use a `Counter()` but you can ignore the difference for now, `Counter()` objects are just a bit more flexible but behave similar to dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_term = 'people' # define query as a string\n",
    "results = Counter() # here we keep track of our word counts by mapping years to word frequencies\n",
    "\n",
    "for fp in file_paths: # iterate over each path in our collection\n",
    "    time_stamp = path2timestamp(fp) # returns a time stamp given a path\n",
    "    word_counts = count_words(fp) # count the words in our document\n",
    "    count_for_target = word_counts[target_term] # get frequency of target term\n",
    "    results[time_stamp]+= count_for_target  # incrementally add the word count for our target term to results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small program returns a mapping between time-stamps and word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an external library `pandas` to plot the timeline. We'll see more of `pandas` in part II.\n",
    "\n",
    "The first line in the cell below (starting with `%`) is necessary to plot the figure in the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import pandas as pd # import pandas\n",
    "pd.Series(results).plot() # convert Counter to a pandas.Series and apply the `.plot()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "We create a few more examples that extend the functionalities of our word-counting program.\n",
    "\n",
    "### Multiple words over time\n",
    "\n",
    "Instead of plotting the evolution of just one word, we track the evolution of a group of words over time. The main differences with the previous program are:\n",
    "- `target_terms` refers to a list and not a string (line 1)\n",
    "- the code cell now contains a double `for` loop: we iterate over all files (line 4) and within this block (i.e. for each file) we iterate over each item in `target_terms` (we collect the counts for each query term). Please note the use of indentation (the second for loop is followed by extra white space). Again, this is not just to make the code more readable, it is part of the Python syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terms = ['poverty','poor'] # define query terms as a list\n",
    "results = Counter() # create Counter to keep track of word counts\n",
    "\n",
    "for fp in file_paths: # iterate over paths\n",
    "    ts = path2timestamp(fp) # get time stamp from path\n",
    "    word_counts = count_words(fp) # get word counts for document\n",
    "    for target_term in target_terms: # iterate over the query terms\n",
    "        count_for_target = word_counts[target_term] # get word count for a query\n",
    "        results[ts]+= count_for_target # increment count for the specific time stamp\n",
    "\n",
    "pd.Series(results).plot() # plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative frequencies\n",
    "\n",
    "The upward trend in the previous could be misleading: is poverty really becoming a more important issue? Or are the manifestos just getting longer and thus containing more words? To control for the size of the corpus, we can compute the **relative** frequency of terms: we count how often a word appears and divide this by the total number of tokens).\n",
    "\n",
    "In the example below, we slightly change the function for counting words. Instead of only returning dictionary (or Counter) with word type frequencies, it also returns the total number of tokens. We name this extended function `count_words2` not to overwrite the function we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words2(path):\n",
    "    text = open(path,'r').read() # open and read the document \n",
    "    text_lowercase = text.lower() # lowercase the text file\n",
    "    tokens = word_tokenize(text_lowercase) # get tokens\n",
    "    word_counts = Counter(tokens) # count tokens\n",
    "    return word_counts,len(tokens) # return word frequencies and the total number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add one more `Counter()` to keep track of the total number of tokens for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terms = ['austerity','cuts'] # formulate queries\n",
    "word_counts = Counter() # create dictionary for storing the frequency of our query for earch time stamp\n",
    "total_counts = Counter() #  create dictionary for storing total word counts for earch time stamp\n",
    "\n",
    "\n",
    "for fp in file_paths: # iterate over paths\n",
    "    year = path2timestamp(fp) # get timestamp from path\n",
    "    word_count,total_count = count_words2(fp) # get the word counts and the total number of tokens\n",
    "    for target_term in target_terms: # \n",
    "        count_for_target = word_count[target_term]\n",
    "        word_counts[year]+= count_for_target\n",
    "        total_counts[year]+= total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we plot the results, we need to divide the frequency of our query by the total number of words. Please have conulst the Breakout on dictionaries for my information about the how to work with this data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # empty dictionary where we save relative frequency\n",
    "for year in word_counts.keys(): # iterate over all time stamps in the word counts dictionary\n",
    "    results[year] = word_counts[year] / total_counts[year] # divide the frequency of query by the total number of word for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(results).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
