{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnDacKh44M9h"
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/kasparvonbeelen/ghi_python/4-tables?labpath=13_-_Supervised_Learning.ipynb)\n",
    "\n",
    "\n",
    "# Lecture 13: Supervised Learning\n",
    "\n",
    "## Data Science for Historians (with Python)\n",
    "\n",
    "## A Gentle Introduction to Working with Data in Python\n",
    "\n",
    "### Created by Kaspar Beelen and Luke Blaxill\n",
    "\n",
    "### For the German Historical Institute, London\n",
    "\n",
    "<img align=\"left\" src=\"https://www.ghil.ac.uk/typo3conf/ext/wacon_ghil/Resources/Public/Images/institute_icon_small.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook provides a concise introduction to supervised learning in Python.\n",
    "\n",
    "You'll learn:\n",
    "- the basic components of a supervised classification pipeline\n",
    "- how to load and preprocess your data\n",
    "- how to vectorize data\n",
    "- how to train a text classification model\n",
    "- how to assess if your model works\n",
    "\n",
    "Supervised learning is probably the most common type of machine learning. In this scenario, **we want to \"teach\" a machine to learn from labelled examples**.\n",
    "\n",
    "When applied to textual data, we want a computer to learn classification, based on a set of labelled examples.\n",
    "\n",
    "Image your corpus looks like this:\n",
    "\n",
    "```python\n",
    "train_corpus = [[\"I am happy happy !\", \"Pos\"],\n",
    "          [\"I am sad sad\", \"Neg\"],\n",
    "          [\"he is not happy\", \"Neg\"],\n",
    "          [\"that is not bad\", \"Pos\"],\n",
    "          [\"urrrrrggghh :'(\", \"Neg\"],\n",
    "          ['this is AWESOME ! ! !',\"Pos\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2ua81TssXwl"
   },
   "source": [
    "With these data—a small set of examples, with very short exclamations, and labels ('Neg' and 'Pos')—we want to teach the computer to recognize emotion in texts (a task commonly referred to as **emotion mining**). \n",
    "\n",
    "To build an emotion classifier, we apply an algorithm that learns the relation between the content of a document (think of words) and the label. This step is called **training** or **fitting** the model. \n",
    "\n",
    "To goal of training is to detect a pattern in the documents that \"betray\" the label. This pattern is commonly referred to as the **signal** (words like \"happy\" and \"sad\"), other words, that don't convey emotions, are **noise** (\"he\", \"I\"). \n",
    "\n",
    "Machine learning models are engineered to efficiently distinguish signal from noise. In the above example, it will learn that the word \"happy\" corresponds with the `'Pos'` label, while sad is associated with `'Neg`'.\n",
    "\n",
    "Training on labelled data creates a text classification model. This model is able to **predict the label** of a document given a text. The variable `clf` refers to a classification model. \n",
    "\n",
    "```python\n",
    "clf.predict(['Haha , he is happy !'])\n",
    "```\n",
    "\n",
    "Hopefully, it returns the label 'Pos' (if the model is properly fitted).\n",
    "\n",
    "To establish if our model works, we set aside a small sample of our labelled data for testing purposes: this sample is called the **test set**. We want to know how well our model performs on examples it hasn't seen during training, to determine its **out of sample accuracy**.\n",
    "\n",
    "```python\n",
    "test_corpus_text = ['he is not sad',\n",
    "               \t\t\t'the dog is happy',\n",
    "               \t\t\t'the puppy is sad']\n",
    "\n",
    "test_corpus_labels = ['Pos','Pos','Neg']\n",
    "\n",
    "pred = clf.predict([test_corpus])\n",
    "\n",
    "```\n",
    "\n",
    "The variable `pred` contains the predicted labels\n",
    "\n",
    "```python\n",
    "['Neg','Pos','Neg']\n",
    "```\n",
    "\n",
    "You can see that the model got the first sentence wrong (it saw 'sad' and probably missed it was preceded by a negation ('not'). So confusing!).\n",
    "\n",
    "Now we can compute the out of sample **accuracy** on the test set by comparing actual labels (`test_corpus_labels`) with predictions (`pred`). The model was correct in two or the three times, meaning it has an accuracy of 2/3 = 66.7%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEfDuQX8vt5R",
    "outputId": "9d277ad4-a2d1-4762-bc90-be9ba4635867"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(2/3*100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzldo-aliLMz"
   },
   "source": [
    "### When to use supervised classification\n",
    "\n",
    "- You know the categories of interest\n",
    "- Organize a large corpus of text\n",
    "- Detect things in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO3Pcu-d0QCz"
   },
   "source": [
    "# Task definition\n",
    "\n",
    "These steps are the basic elements of the supervised learning pipeline. But enough theory, let's work on a practical and more realistic example!\n",
    "\n",
    "![alt text](https://media.giphy.com/media/l41lLs970IkkBi6f6/giphy.gif)\n",
    "\n",
    "We want to train a model that predicts whether the sentence contains an animated machine (or not). Put more simply: **is the machine alive?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSB8IHp77ccl"
   },
   "source": [
    "This example consists of the following steps:\n",
    "  - Loading data\n",
    "  - Preprocessing\n",
    "  - Vectorization\n",
    "  - Training\n",
    "  - Evaluation\n",
    "  - Application and Inspection of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONuB5G_CcD-c"
   },
   "source": [
    "# Loading data\n",
    "\n",
    "In this part of the tutorial we continue working with the Living Machines dataset.\n",
    "- at the left-hand-side of the screen, you should see a **folder** icon.\n",
    "- click on the folder icon, a blade opens with a folder `sample_data` in it.\n",
    "- drag the `playing_animacy_data.tsv` to the **empty space under** this folder. \n",
    "- you may get a message telling you that data will be removed after recycling the Runtime, just click `OK`.\n",
    "\n",
    "This should work! Run the code below to check. It should return `True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kfDnrWGcAg5l",
    "outputId": "5e8b0e92-f66b-406c-a32b-edee68ae9725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('playing_animacy_data.tsv',sep='\\t',index_col=False, )\n",
    "isinstance(df,pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhvoeTaLcmKk"
   },
   "source": [
    "## Alternative ways to load data\n",
    "\n",
    "### Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WS6W0EpcgSVq"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6azhae85_fcR"
   },
   "source": [
    "### Import from Google Drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEGsYaCJ_Q7U"
   },
   "outputs": [],
   "source": [
    "#import packages and authorize connection to Google account:\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "gc = gspread.authorize(GoogleCredentials.get_application_default())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OuAHkpq_Xvm"
   },
   "outputs": [],
   "source": [
    "# We can load the data like this (where the URL is the address of your dataset):\n",
    "spreadsheet = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1VLw3x4mrg2IIFHliyLSAYMAHCpMYBkW_HDMQOPj4seQ/edit#gid=698458315\").get_worksheet(0)\n",
    "\n",
    "# We can read a tsv file using pandas library in this way. The resulting object is called a dataframe:\n",
    "df = pd.DataFrame(spreadsheet.get_all_records())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8104jE-Ebsbe"
   },
   "source": [
    "## Loading CSV as a Pandas DataFrame\n",
    "\n",
    "After uploading the data to Colab we can open it as a Pandas DataFrame.\n",
    "\n",
    "> What is Pandas?\n",
    ">\n",
    "> What is a DataFrame?\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr2JRtbZcDIR"
   },
   "outputs": [],
   "source": [
    "# load pandas using the abbreviation pd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-ve6Kp-g_65"
   },
   "outputs": [],
   "source": [
    "# help(pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEbxcyAnh3_6"
   },
   "source": [
    "To load the data we use the method `read_csv`, which takes three arguments:\n",
    "- a positional argument `path`, that indicates where the data is stored\n",
    "- a named argument `sep` that indicates how the columns are seperated (in this case a tab or `\\t` symbol, but often this is a comma)\n",
    "- index_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6i1s_31jcJeX"
   },
   "outputs": [],
   "source": [
    "# we store the animacy data in a variable with the name df\n",
    "# the data type of the variable is a pandas DataFrame\n",
    "df = pd.read_csv('playing_animacy_data.tsv',sep='\\t',index_col=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ0SzWc5jVWq"
   },
   "source": [
    "Check the type of the `df` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "FtZseG63hVCM",
    "outputId": "447e0cbe-7ae8-4938-e0cb-a3b6dd1dfe50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(df,pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laM2R56tjZkv"
   },
   "source": [
    "The `.head()` method, provides a way for inspecting the first n-rows of the `pandas.DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "oh--Tbz4eAoe",
    "outputId": "b61f3753-c826-4523-f973-f560bb7a0007"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextSnippet</th>\n",
       "      <th>MachineType</th>\n",
       "      <th>Date</th>\n",
       "      <th>Category</th>\n",
       "      <th>Humanness</th>\n",
       "      <th>Animacy</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.  and had almost resolved to go Wffik, when h...</td>\n",
       "      <td>locomotive</td>\n",
       "      <td>1890</td>\n",
       "      <td>machine as a human</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I once made an experiment of this kind on a ch...</td>\n",
       "      <td>machine</td>\n",
       "      <td>1887</td>\n",
       "      <td>machine is inanimate object without agency</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hot-house, the forced labour of the beast iu t...</td>\n",
       "      <td>machine</td>\n",
       "      <td>1836</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The next fifteen or twenty years may, therefor...</td>\n",
       "      <td>machines</td>\n",
       "      <td>1892</td>\n",
       "      <td>machine is inanimate object without agency</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THE LAST OF THE BARONS.  28 ray ; for Coniers ...</td>\n",
       "      <td>machinery</td>\n",
       "      <td>1895</td>\n",
       "      <td>human as a machine</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         TextSnippet  ...  split\n",
       "0  .  and had almost resolved to go Wffik, when h...  ...  train\n",
       "1  I once made an experiment of this kind on a ch...  ...  train\n",
       "2  hot-house, the forced labour of the beast iu t...  ...  train\n",
       "3  The next fifteen or twenty years may, therefor...  ...  train\n",
       "4  THE LAST OF THE BARONS.  28 ray ; for Coniers ...  ...  train\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXo451qh17E-"
   },
   "source": [
    "The `.shape` attribute shows the number of rows and columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rZSt4XyE2D6I",
    "outputId": "28584ca6-3c63-4383-a7f0-c4aac1e7f72d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDGWHn7wjvgW"
   },
   "source": [
    "# Preprocessing text data \n",
    "\n",
    "### Why preprocessing?\n",
    "\n",
    "In supervised text classification, we want to a model to find textual **patterns** that are predictive of a document's label, i.e., we want the algorithm to learn how tokens in a document correspond with the label.\n",
    "\n",
    "While machine learning models are mostly strong at recognizing such patterns in your data, **they can not do all the work for you**.\n",
    "\n",
    "The way you \"feed\" the data to the model does have an  impact on how well it will perform (i.e. predict the correct label given the (transformed) content of a document).\n",
    "\n",
    "Each task is different, and you need to adjust the preprocessing steps to the concepts you want to detect in your data.\n",
    "\n",
    "Please ask yourself, what aspects of the text help distinguishing the target categories:\n",
    "\n",
    "- **Capitals**: if names are an important feature, you don't want to lowercase your character. However for emotion detection, the difference between \"Hamburger\" and \"hamburger\" is less relevant.\n",
    "- **Parts-of-Speech**: emotion often resides in adjectives and adverbs, nouns are indicative of topic. You could discard all words of a particular part-of-speech to remove \"noise\".\n",
    "\n",
    "In the end, what works well is often an empirical question, but you can't examine all possible scenarios. Working on the basis of **intuitions** and assumptions is valid, as long as you are explicit about them.\n",
    "\n",
    "Machine learning is always influenced and manipulated by **human intervention**.\n",
    "\n",
    "But without further ado, let's go ahead with preprocessing our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6mXLLLZpBus"
   },
   "source": [
    "### Preprocessing texts with Pandas\n",
    "\n",
    "In what follows we use the `.apply()` method (attached to the DataFrame object) to preprocess our sentences. This section builds on the previous presentation on spaCy.\n",
    "\n",
    "`.apply()` applies (what's in a name!) a function (entered as an argument between the parentheses) to each cell in a column.\n",
    "\n",
    "For example, we can convert all strings to lowercase using the `str.lower()` method.\n",
    "\n",
    "Normally `.lower()` is applied to a string object as in the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zJ3CTNR65UU3",
    "outputId": "b97f9e94-24f6-453f-cec9-c1e901102f81"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'convert me to lowercase, please!'"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"CONVERT mE tO LoWERcAsE, PLEASE!\".lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDG6kRN259Z-"
   },
   "source": [
    "Which equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "R5JiaSgZ5-xG",
    "outputId": "1d8c45b0-a1f2-47ea-ac1c-b960faaf8a5c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'convert me to lowercase, please!'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.lower(\"CONVERT mE tO LoWERcAsE, PLEASE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot1CW7hP5jGo"
   },
   "source": [
    "To lowercase text in the `TextSnippet` column, simply apply `str.lower` (without the parentheses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "HU0t_Uww4HYx",
    "outputId": "36981c78-933e-4f1b-d382-2b2c97322c07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      .  and had almost resolved to go wffik, when h...\n",
       "1      i once made an experiment of this kind on a ch...\n",
       "2      hot-house, the forced labour of the beast iu t...\n",
       "3      the next fifteen or twenty years may, therefor...\n",
       "4      the last of the barons.  28 ray ; for coniers ...\n",
       "                             ...                        \n",
       "388    in spite of myt avish, i learned fishing tho r...\n",
       "389    for the best locomotive that could be made.  i...\n",
       "390    he chooses those modes of fighting in avhich t...\n",
       "391    others mimic the cries of barnyard fowl with m...\n",
       "392    well, good-bye till dinner-time,\" responded le...\n",
       "Name: TextSnippet, Length: 393, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.TextSnippet.apply(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5JSUTtg6SRF"
   },
   "source": [
    "Lowercasing is an inbuilt method attached the columns of the DataFrame (which are of type `pandas.Series`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "jYr_kJKA6LZz",
    "outputId": "1b33fd6f-58c2-45ee-a810-6489d4852995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      .  and had almost resolved to go wffik, when h...\n",
       "1      i once made an experiment of this kind on a ch...\n",
       "2      hot-house, the forced labour of the beast iu t...\n",
       "3      the next fifteen or twenty years may, therefor...\n",
       "4      the last of the barons.  28 ray ; for coniers ...\n",
       "                             ...                        \n",
       "388    in spite of myt avish, i learned fishing tho r...\n",
       "389    for the best locomotive that could be made.  i...\n",
       "390    he chooses those modes of fighting in avhich t...\n",
       "391    others mimic the cries of barnyard fowl with m...\n",
       "392    well, good-bye till dinner-time,\" responded le...\n",
       "Name: TextSnippet, Length: 393, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.TextSnippet.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QzNJIt669pS"
   },
   "source": [
    "You can use more string methods, to see which ones are available inspect the various help and documentation functions provided by Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2_6kiPh6x6m"
   },
   "outputs": [],
   "source": [
    "?df.TextSnippet.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8G4aea27wpH"
   },
   "outputs": [],
   "source": [
    "dir(df.TextSnippet.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFHyH4FC70r4"
   },
   "outputs": [],
   "source": [
    "help(df.TextSnippet.str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW7w-cQe76EX"
   },
   "source": [
    "Lowercasing text isn't enough. We'd like to use more of the NLP candy Mariona shared with us in a previous Notebook. This can be easily done by building a preprocessing function that combines various steps. \n",
    "\n",
    "Below we build the skeleton for such a function. it doesn't do anything yet, but shows how to document your code by using:\n",
    "- [`typing`](https://docs.python.org/3/library/typing.html): type hints in the creation of the function \n",
    "- Docstring: a summary of what the function does, what it expects as input and returns as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4tqvj608Wd1"
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence: str = ''):\n",
    "  \"\"\"preprocessing function that takes a string as input\n",
    "  perform steps X, Y, Z, and returns the converted sentence as a string.\n",
    "  Arguments:\n",
    "    sentence (str): input sentence\n",
    "  Returns:\n",
    "    a converted sentence as a str object\n",
    "  \"\"\"\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rf75QQH-4rZ"
   },
   "source": [
    "Please note that these ornaments are not required by the Python syntax. However, they convey that you take code seriously and would like others to understand what you are doing (this \"other\" could be you, a few months later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ7dzc1d_QkP"
   },
   "source": [
    "OK, let's add some more spaCy functionality. We want to:\n",
    "- lowercase the sentence\n",
    "- split it into tokens\n",
    "- get the lemma of each token\n",
    "- return the tokenized and lemmatized text as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZZId43-AVmz"
   },
   "outputs": [],
   "source": [
    "# load the spaCy library\n",
    "import spacy\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEzEmcLHABsf"
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence: str = ''):\n",
    "  \"\"\"preprocessing function that takes a string as input\n",
    "  perform lowercasing, tokenization and lemmatization\n",
    "  and returns the converted sentence as a string.\n",
    "  Arguments:\n",
    "    sentence (str): input sentence\n",
    "  Returns:\n",
    "    a converted sentence as a str object\n",
    "  \"\"\"\n",
    "  # the nlp function perform tokenization under the hood\n",
    "  sentence_nlp = nlp(sentence)\n",
    "  # use a list comprehension to collect all lemmas in a list \n",
    "  # lowercase the lemma\n",
    "  sentence = [token.lemma_.lower() for token in sentence_nlp]\n",
    "  # convert the list of lemmas to string\n",
    "  sentence = ' '.join(sentence)\n",
    "  # return the converted string\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMVmgfwWBVq-"
   },
   "source": [
    "Nice! To inspect the magic performed by this simple function, let's see how it handles the first sentence of our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "sFv1PCQnBoyW",
    "outputId": "7dce2edd-719d-45c8-df24-c2f673ffa38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  and had almost resolved to go Wffik, when he heard the faint, dis tant scream of a locomotive and the sound recalled him to himself.  He saw that there Was a man who Was carrying the lantern, and within a minutes more he had run up, almost out of breath, aud was brokenly telling his story of what he had seen ahead, to which ha trackman listened in silence.\n"
     ]
    }
   ],
   "source": [
    "sentence = str(df.iloc[0].TextSnippet)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "M7G5RHMlBzKF",
    "outputId": "d5057515-f879-45f5-fc4a-8890fb18c96a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".   and have almost resolve to go wffik , when -pron- hear the faint , dis tant scream of a locomotive and the sound recall -pron- to -pron- .   -pron- see that there be a man who be carry the lantern , and within a minute more -pron- have run up , almost out of breath , aud be brokenly tell -pron- story of what -pron- have see ahead , to which ha trackman listen in silence .\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkI0MQbsCHDE"
   },
   "source": [
    "### Taking a closer look\n",
    "\n",
    "If you are not familiar with Python, you may wonder why\n",
    "- there is no `for` loop as was the case in many of the earlier examples. We used a **list comprehension**, which has a more concise syntax and is faster. If the function is difficult to comprehend, I created an \"extended edition\" that writes out each step in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiEVgLv0HXdC"
   },
   "outputs": [],
   "source": [
    "def preprocess_extended(sentence: str = ''):\n",
    "  \"\"\"preprocessing function that takes a string as input\n",
    "  perform lowercasing, tokenization and lemmatization\n",
    "  and returns the converted sentence as a string.\n",
    "  Arguments:\n",
    "    sentence (str): input sentence\n",
    "  Returns:\n",
    "    a converted sentence as a str object\n",
    "  \"\"\"\n",
    "  # the nlp function perform tokenization under the hood\n",
    "  sentence_nlp = nlp(sentence)\n",
    "  \n",
    "  # create an empty in list where we save lowercased lemmas\n",
    "  sentence_out = []\n",
    "  # iterate over all tokens\n",
    "  for token in sentence_nlp:\n",
    "    lemma = token.lemma_\n",
    "    lemma_lower = lemma.lower()\n",
    "    sentence_out.append(lemma_lower)\n",
    "  \n",
    "  # convert the list of lemmas to string\n",
    "  sentence_out = ' '.join(sentence_out)\n",
    "  # return the converted string\n",
    "  return sentence_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFOky3W-HX2K"
   },
   "source": [
    "- However, you could make the code even more concise with a `lambda` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlVqiIdGFYXt"
   },
   "outputs": [],
   "source": [
    "preprocess_short = lambda x: ' '.join([token.lemma_.lower() for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "lAKUrXl5HjZl",
    "outputId": "b07d9c02-bede-48cd-e646-5a18cceccd8f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'.   and have almost resolve to go wffik , when -pron- hear the faint , dis tant scream of a locomotive and the sound recall -pron- to -pron- .   -pron- see that there be a man who be carry the lantern , and within a minute more -pron- have run up , almost out of breath , aud be brokenly tell -pron- story of what -pron- have see ahead , to which ha trackman listen in silence .'"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_short(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKZyQ4nDHceV"
   },
   "source": [
    "We can refine the preprocessing by attaching a part-of-speech tag to each lemma. Below I show the \"long version\", to make clear what is going on at each step, but you could rewrite the whole function in a onelambdaliner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNKLSM7LIJX2"
   },
   "outputs": [],
   "source": [
    "def refined_preprocess(sentence: str = ''):\n",
    "  \"\"\"preprocessing function that takes a string as input\n",
    "  perform lowercasing, tokenization, lemmatization, and p-o-s tagging\n",
    "  and returns the converted sentence (in which each lemma is associated \n",
    "  with the p-o-s tag) as a string.\n",
    "  Arguments:\n",
    "    sentence (str): input sentence\n",
    "  Returns:\n",
    "    a converted sentence as a str object\n",
    "  \"\"\"\n",
    "  # the nlp function perform tokenization under the hood\n",
    "  sentence_nlp = nlp(sentence)\n",
    "  # create an empty list in which you save processed tokens\n",
    "  sentence_out = []\n",
    "  # iterate over all tokens in the sentence_nlp object\n",
    "  for token in sentence_nlp:\n",
    "    # get the lemma and part-of-speech tag as tuple\n",
    "    lemma_pos = (token.lemma_,token.pos_)\n",
    "    # convert tuple to a string\n",
    "    lemma_pos_str = '_'.join(lemma_pos)\n",
    "    # lowercase the string\n",
    "    lemma_pos_lower = lemma_pos_str.lower()\n",
    "    # add lowercased string to sentence out list\n",
    "    sentence_out.append(lemma_pos_lower)\n",
    "  # convert the list of lemmas to string\n",
    "  sentence_out = ' '.join(sentence_out)\n",
    "  # return the converted string\n",
    "  return sentence_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "CvLFot14JwBP",
    "outputId": "7412868a-fbf9-4a77-fc4f-4243d3c7d2d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'._punct  _space and_cconj have_aux almost_adv resolve_verb to_part go_verb wffik_propn ,_punct when_adv -pron-_pron hear_verb the_det faint_adj ,_punct dis_propn tant_adj scream_noun of_adp a_det locomotive_adj and_cconj the_det sound_noun recall_verb -pron-_pron to_adp -pron-_pron ._punct  _space -pron-_pron see_verb that_sconj there_pron be_aux a_det man_noun who_pron be_aux carry_verb the_det lantern_noun ,_punct and_cconj within_adp a_det minute_noun more_adv -pron-_pron have_aux run_verb up_adp ,_punct almost_adv out_sconj of_adp breath_noun ,_punct aud_propn be_aux brokenly_adv tell_verb -pron-_det story_noun of_adp what_pron -pron-_pron have_aux see_verb ahead_adv ,_punct to_part which_det ha_propn trackman_propn listen_verb in_adp silence_noun ._punct'"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_preprocess(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9OtnTdgJ-lh"
   },
   "source": [
    "Or\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "wY3mpN90J-Fb",
    "outputId": "526589a9-fcf2-433b-8c80-600977069ed1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'._punct  _space and_cconj have_aux almost_adv resolve_verb to_part go_verb wffik_propn ,_punct when_adv -pron-_pron hear_verb the_det faint_adj ,_punct dis_propn tant_adj scream_noun of_adp a_det locomotive_adj and_cconj the_det sound_noun recall_verb -pron-_pron to_adp -pron-_pron ._punct  _space -pron-_pron see_verb that_sconj there_pron be_aux a_det man_noun who_pron be_aux carry_verb the_det lantern_noun ,_punct and_cconj within_adp a_det minute_noun more_adv -pron-_pron have_aux run_verb up_adp ,_punct almost_adv out_sconj of_adp breath_noun ,_punct aud_propn be_aux brokenly_adv tell_verb -pron-_det story_noun of_adp what_pron -pron-_pron have_aux see_verb ahead_adv ,_punct to_part which_det ha_propn trackman_propn listen_verb in_adp silence_noun ._punct'"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the short version of the refined preprocess function\n",
    "# it is concise, but is it still readable?\n",
    "rfs = lambda x: ' '.join(['_'.join((t.lemma_,t.pos_)).lower() for t in nlp(x)])\n",
    "rfs(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqoEICRTJ3oi"
   },
   "source": [
    "Once the preprocessing steps are defined, we can simply use `apply` (remember) and convert all sentences in the `TextSnippet` column. \n",
    "\n",
    "For sure we want to save the output. We, therefore, create a new column 'SentenceProcessed' in which we store the result of our text transformation.\n",
    "\n",
    "You don't have to worry about the order, Pandas makes sure all sentences end up in the correct row and columns. Simply run the code below (it can take a few seconds, don't worry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8za30AsLHeB"
   },
   "outputs": [],
   "source": [
    "df['SentenceProcessed'] = df[\"TextSnippet\"].apply(refined_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7sKkipiLXeK"
   },
   "source": [
    "Use the `.head()` method and, voila, there you have a new column with your processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "VoFE1RVkLP4n",
    "outputId": "5ac28be2-0f9e-42d3-c4a4-8cc50a6b26ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextSnippet</th>\n",
       "      <th>MachineType</th>\n",
       "      <th>Date</th>\n",
       "      <th>Category</th>\n",
       "      <th>Humanness</th>\n",
       "      <th>Animacy</th>\n",
       "      <th>split</th>\n",
       "      <th>SentenceProcessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.  and had almost resolved to go Wffik, when h...</td>\n",
       "      <td>locomotive</td>\n",
       "      <td>1890</td>\n",
       "      <td>machine as a human</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>._punct  _space and_cconj have_aux almost_adv ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I once made an experiment of this kind on a ch...</td>\n",
       "      <td>machine</td>\n",
       "      <td>1887</td>\n",
       "      <td>machine is inanimate object without agency</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>-pron-_pron once_adv make_verb an_det experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hot-house, the forced labour of the beast iu t...</td>\n",
       "      <td>machine</td>\n",
       "      <td>1836</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>hot_adj -_punct house_noun ,_punct the_det for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The next fifteen or twenty years may, therefor...</td>\n",
       "      <td>machines</td>\n",
       "      <td>1892</td>\n",
       "      <td>machine is inanimate object without agency</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>the_det next_adj fifteen_num or_cconj twenty_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THE LAST OF THE BARONS.  28 ray ; for Coniers ...</td>\n",
       "      <td>machinery</td>\n",
       "      <td>1895</td>\n",
       "      <td>human as a machine</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>the_det last_noun of_adp the_det barons_propn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         TextSnippet  ...                                  SentenceProcessed\n",
       "0  .  and had almost resolved to go Wffik, when h...  ...  ._punct  _space and_cconj have_aux almost_adv ...\n",
       "1  I once made an experiment of this kind on a ch...  ...  -pron-_pron once_adv make_verb an_det experime...\n",
       "2  hot-house, the forced labour of the beast iu t...  ...  hot_adj -_punct house_noun ,_punct the_det for...\n",
       "3  The next fifteen or twenty years may, therefor...  ...  the_det next_adj fifteen_num or_cconj twenty_n...\n",
       "4  THE LAST OF THE BARONS.  28 ray ; for Coniers ...  ...  the_det last_noun of_adp the_det barons_propn ...\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_nOL7_7LpEL"
   },
   "source": [
    "# From text to matrices\n",
    "\n",
    "## Documents as vectors\n",
    "Unfortunately, computers find it hard to read texts. They like numbers more. We can't just feed it the tokens but have to transform each sentence to a **vector**.\n",
    "\n",
    "A vector is just a list of numbers, such as [0, 10, 1, 15]. \n",
    "\n",
    "How to convert a text to a series of numbers is much debated. Below we show you the easiest and most common scenario: the **bag-of-words** approach.\n",
    "\n",
    "This approach assumes that a document can be adequately represented by simply counting the words they contain. We represent the document numerically by collecting the **token frequencies**. For example, the code below converts a sentence to a vector of term frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "oDNyeUBUO_-Y",
    "outputId": "75e93cfe-7cc7-41cb-a5ac-90004790e75b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'-pron-': 7, ',': 6, '.': 3, 'and': 3, 'have': 3, 'to': 3, 'the': 3, 'of': 3, 'a': 3, 'be': 3, 'almost': 2, 'see': 2, 'resolve': 1, 'go': 1, 'wffik': 1, 'when': 1, 'hear': 1, 'faint': 1, 'dis': 1, 'tant': 1, 'scream': 1, 'locomotive': 1, 'sound': 1, 'recall': 1, 'that': 1, 'there': 1, 'man': 1, 'who': 1, 'carry': 1, 'lantern': 1, 'within': 1, 'minute': 1, 'more': 1, 'run': 1, 'up': 1, 'out': 1, 'breath': 1, 'aud': 1, 'brokenly': 1, 'tell': 1, 'story': 1, 'what': 1, 'ahead': 1, 'which': 1, 'ha': 1, 'trackman': 1, 'listen': 1, 'in': 1, 'silence': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "fw = Counter(preprocess(sentence).split())\n",
    "print(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "edKBQsJePfGz",
    "outputId": "334470e3-279d-4f6d-bf74-8ce10f2300e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 2, 1, 3, 1, 1, 6, 1, 7, 1, 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(list(fw.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFU65ZAyPAXp"
   },
   "source": [
    "We can vectorize all documents, and construct a **document-term matrix**. A matrix is nothing more than a collection of individual vectors, stacked as rows on top of each other. \n",
    "\n",
    "Image our corpus consists of just two sentences: \"I like food\", \"Cats like like food\"\n",
    "\n",
    "Using the bag-of-words approach we can convert this corpus to the following document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "lXQmttIfQE44",
    "outputId": "8fdad379-9670-4c92-b01f-6856fe7a11f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>cats</th>\n",
       "      <th>food</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i like food</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cats like like food</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     i  cats  food  like\n",
       "i like food          1     0     1     1\n",
       "cats like like food  0     1     1     2"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[1,0,1,1],[0,1,1,2]],\n",
    "              columns=[\"i\",\"cats\",\"food\",\"like\"], \n",
    "              index=['i like food','cats like like food'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O8useuRQETb"
   },
   "source": [
    "  We can do the same for the sentences we stored in the `SentenceProcessed` column. And the good news is that you don't have to write much of the code, because `sklearn` has provided you with many tools that simplify this task a lot.\n",
    "\n",
    "  The cells below show how to vectorize your documents and generate a document-term matrix from your corpus. \n",
    "\n",
    "  The `CountVectorizer` class will convert each document to a vector of its token frequencies, just as in the previous example. Load the `CountVectorizer` by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlhXL009RP7m"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6h94SMTGRnqt"
   },
   "outputs": [],
   "source": [
    "# inspect the documentation\n",
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPChG_n2Pefc"
   },
   "source": [
    "As you noticed, the `CountVectorizer` has many arguments. Late on, you can adjust them and see how changing these settings improves or harms the performance of the classifier.\n",
    "\n",
    "We suggest having a closer look at:\n",
    "- `min_df` and `max_df`: discard words based on their document frequency. Words that occur only once or twice probably won't be important for predicting the label of a document. Discarding more frequent words is trickier and depends on the task at hand (sometimes function words convey important information!)\n",
    "- `ngram_range`: n-grams are chunks of n consecutive words. The bag-of-words approach largely ignores the order in which words appear. However, we retain some information on order by counting bigrams (or trigrams). For example,  a bigram model will contain the phrase \"not sad\" whereas a unigram model won't capture this negation (it counts \"not\" and \"sad\" separately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZJWcq1-VTiM"
   },
   "source": [
    "The code below converts all our processed documents into a document terms matrix (more specifically a dense matrix)\n",
    "\n",
    "We first create `vectorizer` an instance of the `CountVectorizer` class for which specified many of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRUzIoe9VfMR"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, \n",
    "                             max_df=0.9,\n",
    "                             ngram_range=(1,2),\n",
    "                             token_pattern=r\"\\S+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ScijfO_XEAs"
   },
   "source": [
    "What about the `token_pattern` argument you might wonder? Well, since we already tokenized the data, the whitespaces effectively indicate word boundaries. A token is everything between two whitespaces (or sentence boundaries). This pattern is matched by the regular expression \"\\S+\" (sequences of everything except whitespace).\n",
    "\n",
    "You can check it for yourself, running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "kXictahcV55_",
    "outputId": "65f329d1-02ea-4a39-e59b-ec007f5dec4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['._punct', '_space', 'and_cconj', 'have_aux', 'almost_adv', 'resolve_verb', 'to_part', 'go_verb', 'wffik_propn', ',_punct']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r\"\\S+\")\n",
    "print(pattern.findall(df.iloc[0].SentenceProcessed)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nP4PhKbaYGZO"
   },
   "source": [
    "We can convert, as an example, the first hundred sentences using the `.fit_transform()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTtht5BYYRM1"
   },
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform(df.iloc[:100].SentenceProcessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RApr05yaYd7P"
   },
   "source": [
    "Now, what does the `dtm` variable (an abbreviation for \"document term-matrix\") contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rWnSTDuSYthm",
    "outputId": "9f4987bf-8355-4876-89e5-a9211e09d4b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 325)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY5xK4Zm2TZP"
   },
   "source": [
    "The `.shape` attribute returns the dimensions of the matrix. It has 100 rows (because we selected the first 100 sentences) and 325 columns. \n",
    "\n",
    "Each column represents one feature. To inspect the features, use `.get_feature_names()` attached to the `CountVectorizer`. \n",
    "\n",
    "You see that the number of features corresponds to the number of columns in `dtm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CX_t7tMDZkDp",
    "outputId": "efe21446-ccdd-4a94-c192-5fc40a72c1d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXxv6kU_43x4"
   },
   "source": [
    "The features are n-grams (of length 1 and 2) consisting of lemma_part-of-speech pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "4WAdczpjZSR7",
    "outputId": "b93133e5-9bd9-4d5f-84d7-dbe43f327ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be_aux so_adv', 'be_aux the_det', 'be_aux to_part', 'become_verb', 'before_adp', 'bell_noun', 'both_det', 'bring_verb', 'but_cconj', 'but_cconj -pron-_pron']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzd2aD844LEj"
   },
   "source": [
    "To inspect a document in vectorized form, we can convert it to a sparse `numpy.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "aEV_9T3LZApv",
    "outputId": "9b881928-09a2-4cb7-b7e9-91c4a4b2a076"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x325 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 38 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEWv1ON-5Lta"
   },
   "source": [
    "The vector below is the numerical presentation of the first sentence in our DataFrame. This is the format in which we feed the text to the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "1HdJwZhVZMY_",
    "outputId": "c38941be-e831-46b5-8678-5b9f323b3025"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 6, 0, 1, 0, 0, 0, 0, 0,\n",
       "        2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA014UXZ6thZ"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "At this point, you should understand how to read, preprocess and vectorize your corpus. Completing these steps allows you to finally train a text classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eht3A1PSatJH"
   },
   "source": [
    "## Creating a train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "366FzhBF3H1z"
   },
   "source": [
    "As mentioned during the introduction, supervised learning consists of training and testing a model. We build a model with training data and consequently evaluate how well it performs on unseen examples. \n",
    "\n",
    "Therefore, we first split our data into a train and test set. Luckily, we've done most of the work for you, by adding the `split` column.\n",
    "\n",
    "The code below creates two variables (of the pandas.DataFrame type) containing the train and test sentences with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FL1mtsMVZqYf"
   },
   "outputs": [],
   "source": [
    "train = df[df.split=='train'] \n",
    "test = df[df.split=='test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4TMBuDi6vLQ"
   },
   "source": [
    "In total, we use 75% for training and 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "3aH2ZXQBZzwm",
    "outputId": "1f325c20-5026-458a-a1f8-ca49bfbe87e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295, 8) (98, 8)\n",
      "0.7506361323155216\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,test.shape)\n",
    "print(train.shape[0]/df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05n2oVa37Hxz"
   },
   "source": [
    "When we vectorize the data with `.fit_transform()`, we only look at the training examples.  \n",
    "\n",
    "Please remember that the model is not allowed to see examples from the test set (otherwise you are cheating!). We won't touch the test examples until the very end of the classification process. \n",
    "\n",
    "To transform the training sentences, we create an instance of the `CountVectorizer` class and define how we'd like to transform the text by specifying arguments such as `min_df` and `ngram_range`.\n",
    "\n",
    "Feel free to change these settings later on and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ06ZeSkaKtA"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, # discard words th\n",
    "                             max_df=0.9,\n",
    "                             ngram_range=(1,2),\n",
    "                             token_pattern=r\"\\S+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIqa7oCi9eim"
   },
   "source": [
    "Below, we apply `.fit_transform()` on the processed sentences in our DataFrame. This returns a document-term matrix, which we store in `X_train`.\n",
    "\n",
    "`y_train` contains the correct or actual label for each sentence (row) in `X_train`. These labels were obtained via human annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onsRxYAxZ_Oi"
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train.SentenceProcessed)\n",
    "y_train = train.Animacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6Ov_HMiY98wA",
    "outputId": "86ff3307-35f7-46c2-ef7e-afd5b88d7b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295, 982) (295,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w-lB1kmaxLw"
   },
   "source": [
    "## Selecting an Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPYVnskRb8mG"
   },
   "source": [
    "We are almost there. Almost all ingredients are in place, except, probably, the most important one: the **learning algorithm**. \n",
    "\n",
    "We have to select the algorithm, that will allow us to learn the relation between features and labels. \n",
    "\n",
    "For this example, we selected a Naive Bayes classifier. Even though rather old, is still often used in the Digital Humanities and provides a competitive baseline.\n",
    "\n",
    "We won't have time to discuss the algorithm in detail. For those who are interested, the Naive Bayes algorithm adheres to the following formula:\n",
    "\n",
    "![Naive Bayes Algorithm](https://wikimedia.org/api/rest_v1/media/math/render/svg/52bd0ca5938da89d7f9bf388dc7edcbd546c118e)\n",
    "\n",
    "![Expansion of Naive Bayes Algorithm](https://wikimedia.org/api/rest_v1/media/math/render/svg/6150f41afac2076bad6e326ebbdb96fa9ee4ca82)\n",
    "\n",
    "This may look complicated, but the math is rather straightforward. we compute the probability of label given the words `x` in a text `C_k` (`P(C_k|x)`). By slightly manipulating Bayes rule, this probability is equal to the probability of `C_k` (how often does the label occur in the training set) multiplied by the probability of seeing the word `x_i` in documents with labels `C_k` (`P(x_i|C_k)`).   \n",
    "\n",
    "For more information consult the [Wikipedia page](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) or the [NLTK handbook](https://www.nltk.org/book/ch06.html).\n",
    "\n",
    "There are of course more complicated models, but it's good to give the Naive Bayes classifier a try. It often yields good results and is more transparent than other models (less of a black box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1m4SK71awXX"
   },
   "outputs": [],
   "source": [
    "# import the MultinomialNB class\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV5Glzb5fQRz"
   },
   "source": [
    "After instantiating the model, we call the `.fit()` method. This computes the class probabilities (prior) and conditional probabilities of the words (likelihood). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ldN5xg9Cajv8",
    "outputId": "bc4a624b-f12b-46c6-bdc3-bc8dcd39b22b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDeaKXKxiId7"
   },
   "source": [
    "You can inspect these probabilities, which are hidden in the `.feature_log_prob_` attribute of the variable `clf`.\n",
    "\n",
    "The shape of this matrix is (2,982) as there are two classes and 982 different features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Hy7siwtCiHtk",
    "outputId": "b96f4ba4-2999-49c9-ea43-e19a127be71d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 982)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_log_prob_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "vAnLbVS8xLZa",
    "outputId": "f0c9177f-ddd2-4ffb-e350-5498cd4fd8fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295, 982)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuAW0Bw81jtE"
   },
   "source": [
    "Below we retrieve the conditional probabilities `P(x_i | C_k)` for the noun \"labour\", and see that it will slightly favour the not-animate class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NLr2rbgC0nVQ",
    "outputId": "bc1b5f31-d2c0-4986-90aa-96232cb463c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'labour_noun'"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "OUTDwz620wju",
    "outputId": "cb88faf7-b09b-48bd-a6d1-2ead22151a19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.59931787, -7.72753511])"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_log_prob_[:,500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duLANt1McAqa"
   },
   "source": [
    "# Evaluating the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qnk551c3B29"
   },
   "source": [
    "## Out of sample accuracy\n",
    "\n",
    "We have trained the model and inspected some of its inner workings. But the most important question remains unanswered: how well does it perform in recognizing animacy in text? \n",
    "\n",
    "To answer this question, we gauge the model's accuracy on **examples that it hasn't seen yet** (these examples were not observed during training, i.e. when computing the label priors and conditional probabilities).\n",
    "\n",
    "Before we did this, however, we have to convert the test sentences (which we've set aside earlier) in exactly the **same way as we processed the training examples**. In other words: we have to create a new document-term matrix for the test set, using the same procedure for vectorization.\n",
    "\n",
    "Luckily, this is easy with Python's Sklearn library. We can just reuse the vectorizer we fitted earlier. Instead of `.fit_transform()` we just apply `.transform()` to sentences in the `SentenceProcessed` column.\n",
    "\n",
    "We also create a new array in which we store the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo3ZvcmLaYPt"
   },
   "outputs": [],
   "source": [
    "# transform processed sentences to a document term matrix\n",
    "X_test = vectorizer.transform(test.SentenceProcessed)\n",
    "# create an array with all the labels of the test examples\n",
    "y_test = test.Animacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po80-kxo4RWL"
   },
   "source": [
    "Next, we apply the model (which we fitted during trainig) to the test set. The `.predict()` method is all you need! It returns an array with the predictions for each sentence (which we save in the `pred` variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhQqqNbtbaRQ"
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9NnBaZS5_4X"
   },
   "source": [
    "Below we print the ten first predictions, and compare them with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "X-9A_0PI54lm",
    "outputId": "85d7b026-ef85-43b4-a2eb-bb7cb715bf54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions= [0 1 0 0 0 0 0 0 0 0]\n",
      "Actual labels= [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Predictions=',pred[:10])\n",
    "print('Actual labels=',y_test[:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1EoMMeh6WPl"
   },
   "source": [
    "Not bad! The model was only wrong once, the second sentence, where it predicted animate, while in the fact the sentence was annotated as inanimate (in the literature this is called a False Positive). \n",
    "\n",
    "Just looking at these predictions doesn't get us far. Luckily, there are established metrics that estimate the performance of the model. The most common measure is **accuracy**, which is simply the number of correct predictions divided by the total number of predictions. \n",
    "\n",
    "You may also encounter the **error rate**, which is simply 1 - accuracy.\n",
    "\n",
    "Other commonly used metrics are precision, recall and f1-score. We won't discuss them here, but please inspect their Wikipedia pages.\n",
    "\n",
    "Sklearn provides us with a convenient function, `classification_report`, that returns a summary of the output with all these metrics. It only expects the predictions and actual labels as arguments.\n",
    "\n",
    "Below we printed the classification report, and observe that we obtained close to 80% accuracy!\n",
    "\n",
    "Not bad? Can you do better? Please, scroll down if you want to play with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "OYbtSlVzbkkO",
    "outputId": "06ff302b-1128-4eb9-95ac-9975eb3739ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80        55\n",
      "           1       0.74      0.79      0.76        43\n",
      "\n",
      "    accuracy                           0.79        98\n",
      "   macro avg       0.78      0.79      0.78        98\n",
      "weighted avg       0.79      0.79      0.79        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7KEj5H4-HoP"
   },
   "source": [
    "## Classifying other examples.\n",
    "\n",
    "After training you can deploy the classifier and apply it to any sentence. The only condition is that all texts should processed and vectorized. \n",
    "\n",
    "Fortunately, given that we alread wrote all these functions and trained all the models, this is a rather easy task.\n",
    "\n",
    "If you want to experiment yourself, you can easily change the string after the `sentence_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "wTL645D5-tkx",
    "outputId": "d098e976-3eab-4d40-fe10-fa6a6216b2cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_det machine_noun be_aux a_det very_adv smart_adj ,_punct -pron-_pron write_verb many_adj book_noun and_cconj speak_verb like_sconj a_det philosopher_noun ._punct\n"
     ]
    }
   ],
   "source": [
    "sentence_new = 'The machine was a very smart, it wrote many books and spoke like a philosopher.'\n",
    "# process sentence\n",
    "sentence_new_proc = refined_preprocess(sentence_new)\n",
    "print(sentence_new_proc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HXS23jf_rdN"
   },
   "source": [
    "After preprocessing the example sentence  (each token is now a lemma_part-of-speech pair), we can vectorize it using the `transform()` method attached to the `vectorizer` fitted on the training data. This method expects a list of documents, for this reason, we put the sentence between square brackets.\n",
    "\n",
    "You'll observe that the new document-term matrix has exactly the same number of columns as `X_train`. If these dimensions are different, you've done something wrong and the following steps will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tr-SXsIpAKcr",
    "outputId": "fdb2bfa7-99cf-4666-fc2a-ee24fe007e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 982)\n"
     ]
    }
   ],
   "source": [
    "X_new = vectorizer.transform([sentence_new_proc])\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l56GbfUUAn01"
   },
   "source": [
    "Now we apply `.predict()` to the vectorized sentence, and, wow, it's correct! The classifier did its work properly.\n",
    "\n",
    "For sure, this model is far from perfect. Experiment with other examples and try to understand in which scenario it works, and when it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YNHxbgcG_1EP",
    "outputId": "261b91d5-5e16-497b-e7f3-809c6d9d0c15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_new)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sijl7t9G-W6r"
   },
   "source": [
    "## Inspecting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54iC7eREByC6"
   },
   "source": [
    "Lastly, we can interrogate the model itself more systematically, something which we've already played with when inspecting the conditional probabilities. Don't worry if the code below is not very understandable, it shouldn't, but you can still run it.\n",
    "\n",
    "What it does is finding and printing the features with the highest probabilities for each of the two classes. In other words: it returns you the expression that the model finds most useful for predicting animacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "foI_6gUldMNz",
    "outputId": "7df06949-961c-47cd-e0ef-c0ba230f0b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work_verb with_adp' 'again_adv ,_punct' 'yet_cconj' 'direct_verb'\n",
      " 'teach_verb' 'money_noun' 'not_part have_aux' 'real_adj' 'thin_adj'\n",
      " 'cold_adj' 'obey_verb' 'really_adv' ';_punct that_sconj'\n",
      " 'know_verb ,_punct' 'talk_verb' '-pron-_pron make_verb'\n",
      " ',_punct not_part' '-pron-_pron as_sconj' 'a_det mere_adj'\n",
      " 'soldier_noun ,_punct']\n",
      "['supply_verb' 'the_det fire_noun' 'extent_noun ,_punct' 'extent_noun'\n",
      " 'prove_verb' 'water_noun' 'difficulty_noun' 'cost_noun' 'space_noun'\n",
      " 'expense_noun' 'manufacture_verb' 'several_adj' 'the_det water_noun'\n",
      " 'of_adp machinery_noun' 'apparatus_noun ,_punct' 'apparatus_noun'\n",
      " 'supply_noun' 'surface_noun' 'work_noun ,_punct' 'coal_noun']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "neg_class_prob_sorted = clf.feature_log_prob_[0, :].argsort()\n",
    "pos_class_prob_sorted = clf.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "print(np.take(vectorizer.get_feature_names(), neg_class_prob_sorted[:20]))\n",
    "print(np.take(vectorizer.get_feature_names(), pos_class_prob_sorted[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRA_rP2ScKyE"
   },
   "source": [
    "## Experimenting with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "B5dzSDc1cjLb",
    "outputId": "14f636ae-6dfd-47fa-81fe-6640d1107614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75        50\n",
      "           1       0.74      0.71      0.72        48\n",
      "\n",
      "    accuracy                           0.73        98\n",
      "   macro avg       0.73      0.73      0.73        98\n",
      "weighted avg       0.73      0.73      0.73        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=1,kernel='rbf',class_weight='balanced')\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "-d9UUus5erfz",
    "outputId": "6af6ad46-92ce-46f0-d3a6-113be149f678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79        55\n",
      "           1       0.72      0.77      0.74        43\n",
      "\n",
      "    accuracy                           0.77        98\n",
      "   macro avg       0.76      0.77      0.76        98\n",
      "weighted avg       0.77      0.77      0.77        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gFM-UdB6u6P"
   },
   "source": [
    "# Putting everything together\n",
    "\n",
    "The code cells below put each step together into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcAaXTgKfcWI"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quySe7kffqOv"
   },
   "outputs": [],
   "source": [
    "ps = lambda x: ' '.join([t.lemma_.lower() for t in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5GZ9QHMeTAg"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('playing_animacy_data.tsv',sep='\\t',index_col=False)\n",
    "df['SentenceProcessed'] = df.TextSnippet.apply(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lflH7tggFWp"
   },
   "outputs": [],
   "source": [
    "train = df[df.split=='test']\n",
    "test = df[df.split=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gH_a0vNfOAg"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, \n",
    "                             max_df=0.9,\n",
    "                             ngram_range=(1,3),\n",
    "                             token_pattern=r\"\\S+\")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train.SentenceProcessed)\n",
    "y_train = train.Animacy\n",
    "\n",
    "X_test = vectorizer.transform(test.SentenceProcessed)\n",
    "y_test = test.Animacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kuvzinx4gYL0",
    "outputId": "b35f4e5f-6557-4bed-924e-00608264a2a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 321) (295, 321)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "IIgUJaFMgiRH",
    "outputId": "e4a4a934-d63a-4e58-a182-4c1978ad20ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "_MUhmJt-fahb",
    "outputId": "5157bda8-670b-4a6d-f9c0-7a27acaf768e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73       160\n",
      "           1       0.68      0.69      0.68       135\n",
      "\n",
      "    accuracy                           0.71       295\n",
      "   macro avg       0.71      0.71      0.71       295\n",
      "weighted avg       0.71      0.71      0.71       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "print(classification_report(pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MclDVf7GgyY8"
   },
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "T2T_SupervisedLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
