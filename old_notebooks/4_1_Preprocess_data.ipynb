{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning\n",
    "\n",
    "\n",
    "This notebook provides a concise introduction to supervised learning in Python.\n",
    "\n",
    "You'll learn:\n",
    "- the basic components of a supervised classification pipeline\n",
    "- how to load and preprocess your data\n",
    "- how to vectorize data\n",
    "\n",
    "Supervised learning is probably the most common type of machine learning. In this scenario, **we want to \"teach\" a machine to learn from labelled examples**.\n",
    "\n",
    "When applied to textual data, we want a computer to learn classification, based on a set of labelled examples.\n",
    "\n",
    "Image your corpus looks like this:\n",
    "\n",
    "```python\n",
    "train_corpus = [[\"I am happy happy !\", \"Pos\"],\n",
    "          [\"I am sad sad\", \"Neg\"],\n",
    "          [\"he is not happy\", \"Neg\"],\n",
    "          [\"that is not bad\", \"Pos\"],\n",
    "          [\"urrrrrggghh :'(\", \"Neg\"],\n",
    "          ['this is AWESOME ! ! !',\"Pos\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "With these data—a small set of examples, with very short exclamations, and labels ('Neg' and 'Pos')—we want to teach the computer to recognize emotion in texts (a task commonly referred to as **emotion mining**). \n",
    "\n",
    "To build an emotion classifier, we apply an algorithm that learns the relation between the content of a document (think of words) and the label. This step is called **training** or **fitting** the model. \n",
    "\n",
    "To goal of training is to detect a pattern in the documents that \"betray\" the label. This pattern is commonly referred to as the **signal** (words like \"happy\" and \"sad\"), other words, that don't convey emotions, are **noise** (\"he\", \"I\"). \n",
    "\n",
    "Machine learning models are engineered to efficiently distinguish signal from noise. In the above example, it will learn that the word \"happy\" corresponds with the `'Pos'` label, while sad is associated with `'Neg`'.\n",
    "\n",
    "Training on labelled data creates a text classification model. This model is able to **predict the label** of a document given a text. The variable `clf` refers to a classification model. \n",
    "\n",
    "```python\n",
    "clf.predict(['Haha , he is happy !'])\n",
    "```\n",
    "\n",
    "Hopefully, it returns the label 'Pos' (if the model is properly fitted).\n",
    "\n",
    "To establish if our model works, we set aside a small sample of our labelled data for testing purposes: this sample is called the **test set**. We want to know how well our model performs on examples it hasn't seen during training, to determine its **out of sample accuracy**.\n",
    "\n",
    "```python\n",
    "test_corpus_text = ['he is not sad',\n",
    "               \t\t\t'the dog is happy',\n",
    "               \t\t\t'the puppy is sad']\n",
    "\n",
    "test_corpus_labels = ['Pos','Pos','Neg']\n",
    "\n",
    "pred = clf.predict([test_corpus])\n",
    "\n",
    "```\n",
    "\n",
    "The variable `pred` contains the predicted labels\n",
    "\n",
    "```python\n",
    "['Neg','Pos','Neg']\n",
    "```\n",
    "\n",
    "You can see that the model got the first sentence wrong (it saw 'sad' and probably missed it was preceded by a negation ('not'). So confusing!).\n",
    "\n",
    "Now we can compute the out of sample **accuracy** on the test set by comparing actual labels (`test_corpus_labels`) with predictions (`pred`). The model was correct in two or the three times, meaning it has an accuracy of 2/3 = 66.7%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(2/3*100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text data \n",
    "\n",
    "### Why preprocessing?\n",
    "\n",
    "In supervised text classification, we want to a model to find textual **patterns** that are predictive of a document's label, i.e., we want the algorithm to learn how tokens in a document correspond with the label.\n",
    "\n",
    "While machine learning models are mostly strong at recognizing such patterns in your data, **they can not do all the work for you**.\n",
    "\n",
    "The way you \"feed\" the data to the model does have an  impact on how well it will perform (i.e. predict the correct label given the (transformed) content of a document).\n",
    "\n",
    "Each task is different, and you need to adjust the preprocessing steps to the concepts you want to detect in your data.\n",
    "\n",
    "Please ask yourself, what aspects of the text help distinguishing the target categories:\n",
    "\n",
    "- **Capitals**: if names are an important feature, you don't want to lowercase your character. However for emotion detection, the difference between \"Hamburger\" and \"hamburger\" is less relevant.\n",
    "- **Parts-of-Speech**: emotion often resides in adjectives and adverbs, nouns are indicative of topic. You could discard all words of a particular part-of-speech to remove \"noise\".\n",
    "\n",
    "In the end, what works well is often an empirical question, but you can't examine all possible scenarios. Working on the basis of **intuitions** and assumptions is valid, as long as you are explicit about them.\n",
    "\n",
    "Machine learning is always influenced and manipulated by **human intervention**.\n",
    "\n",
    "But without further ado, let's go ahead with preprocessing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing texts with Pandas\n",
    "\n",
    "In what follows we use the `.apply()` method (attached to the DataFrame object) to preprocess our sentences. This section builds on the previous presentation on spaCy.\n",
    "\n",
    "`.apply()` applies (what's in a name!) a function (entered as an argument between the parentheses) to each cell in a column.\n",
    "\n",
    "For example, we can convert all strings to lowercase using the `str.lower()` method.\n",
    "\n",
    "Normally `.lower()` is applied to a string object as in the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"CONVERT mE tO LoWERcAsE, PLEASE!\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.lower(\"CONVERT mE tO LoWERcAsE, PLEASE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TextSnippet.apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lowercasing is an inbuilt method attached the columns of the DataFrame (which are of type `pandas.Series`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TextSnippet.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can use more string methods, to see which ones are available inspect the various help and documentation functions provided by Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?df.TextSnippet.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lowercasing text isn't enough. We'd like to use more of the NLP candy Mariona shared with us in a previous Notebook. This can be easily done by building a preprocessing function that combines various steps. \n",
    "\n",
    "Below we build the skeleton for such a function. it doesn't do anything yet, but shows how to document your code by using:\n",
    "- [`typing`](https://docs.python.org/3/library/typing.html): type hints in the creation of the function \n",
    "- Docstring: a summary of what the function does, what it expects as input and returns as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OK, let's add some more spaCy functionality. We want to:\n",
    "- lowercase the sentence\n",
    "- split it into tokens\n",
    "- get the lemma of each token\n",
    "- return the tokenized and lemmatized text as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spaCy library\n",
    "import spacy\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence: str = ''):\n",
    "  \"\"\"preprocessing function that takes a string as input\n",
    "  perform lowercasing, tokenization and lemmatization\n",
    "  and returns the converted sentence as a string.\n",
    "  Arguments:\n",
    "    sentence (str): input sentence\n",
    "  Returns:\n",
    "    a converted sentence as a str object\n",
    "  \"\"\"\n",
    "  # the nlp function perform tokenization under the hood\n",
    "  sentence_nlp = nlp(sentence)\n",
    "  # use a list comprehension to collect all lemmas in a list \n",
    "  # lowercase the lemma\n",
    "  sentence = [token.lemma_.lower() for token in sentence_nlp]\n",
    "  # convert the list of lemmas to string\n",
    "  sentence = ' '.join(sentence)\n",
    "  # return the converted string\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nice! To inspect the magic performed by this simple function, let's see how it handles the first sentence of our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = str(df.iloc[0].TextSnippet)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Taking a closer look\n",
    "\n",
    "If you are not familiar with Python, you may wonder why\n",
    "- there is no `for` loop as was the case in many of the earlier examples. We used a **list comprehension**, which has a more concise syntax and is faster. If the function is difficult to comprehend, I created an \"extended edition\" that writes out each step in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_extended(sentence: str = ''):\n",
    "  \"\"\"preprocessing function that takes a string as input\n",
    "  perform lowercasing, tokenization and lemmatization\n",
    "  and returns the converted sentence as a string.\n",
    "  Arguments:\n",
    "    sentence (str): input sentence\n",
    "  Returns:\n",
    "    a converted sentence as a str object\n",
    "  \"\"\"\n",
    "  # the nlp function perform tokenization under the hood\n",
    "  sentence_nlp = nlp(sentence)\n",
    "  \n",
    "  # create an empty in list where we save lowercased lemmas\n",
    "  sentence_out = []\n",
    "  # iterate over all tokens\n",
    "  for token in sentence_nlp:\n",
    "    lemma = token.lemma_\n",
    "    lemma_lower = lemma.lower()\n",
    "    sentence_out.append(lemma_lower)\n",
    "  \n",
    "  # convert the list of lemmas to string\n",
    "  sentence_out = ' '.join(sentence_out)\n",
    "  # return the converted string\n",
    "  return sentence_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- However, you could make the code even more concise with a `lambda` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_short = lambda x: ' '.join([token.lemma_.lower() for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_short(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can refine the preprocessing by attaching a part-of-speech tag to each lemma. Below I show the \"long version\", to make clear what is going on at each step, but you could rewrite the whole function in a onelambdaliner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refined_preprocess(sentence: str = ''):\n",
    "  \"\"\"preprocessing function that takes a string as input\n",
    "  perform lowercasing, tokenization, lemmatization, and p-o-s tagging\n",
    "  and returns the converted sentence (in which each lemma is associated \n",
    "  with the p-o-s tag) as a string.\n",
    "  Arguments:\n",
    "    sentence (str): input sentence\n",
    "  Returns:\n",
    "    a converted sentence as a str object\n",
    "  \"\"\"\n",
    "  # the nlp function perform tokenization under the hood\n",
    "  sentence_nlp = nlp(sentence)\n",
    "  # create an empty list in which you save processed tokens\n",
    "  sentence_out = []\n",
    "  # iterate over all tokens in the sentence_nlp object\n",
    "  for token in sentence_nlp:\n",
    "    # get the lemma and part-of-speech tag as tuple\n",
    "    lemma_pos = (token.lemma_,token.pos_)\n",
    "    # convert tuple to a string\n",
    "    lemma_pos_str = '_'.join(lemma_pos)\n",
    "    # lowercase the string\n",
    "    lemma_pos_lower = lemma_pos_str.lower()\n",
    "    # add lowercased string to sentence out list\n",
    "    sentence_out.append(lemma_pos_lower)\n",
    "  # convert the list of lemmas to string\n",
    "  sentence_out = ' '.join(sentence_out)\n",
    "  # return the converted string\n",
    "  return sentence_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_preprocess(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the short version of the refined preprocess function\n",
    "# it is concise, but is it still readable?\n",
    "rfs = lambda x: ' '.join(['_'.join((t.lemma_,t.pos_)).lower() for t in nlp(x)])\n",
    "rfs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentenceProcessed'] = df[\"TextSnippet\"].apply(refined_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
