{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Processing texts\n",
    "\n",
    "In this notebook you learn to process and extract information from texts. We continue with the sonnet, but as promised, scale up soon. \n",
    "\n",
    "This notebook focusses on extracting basic information from texts, such as the number of sentences and words. We also show how to use external libraries for more refined enrichment (finding named entities or zoom in on specific word categories (nouns, verbs)). We discuss how this could be relevant to historians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Strings are sequences of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have basic understanding of how to read and manipulate textual data in Python. Now we can turn to more directly useful and realistic applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[add wget]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou, contracted to thine own bright eyes,\n",
      "Feed'st thy light's flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thyself thy foe, to thy sweet self too cruel:\n",
      "Thou that art now the world's fresh ornament,\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content,\n",
      "And tender churl mak'st waste in niggarding:\n",
      "Pity the world, or else this glutton be,\n",
      "To eat the world's due, by the grave and thee.\n"
     ]
    }
   ],
   "source": [
    "path = \"example_data/notebook_3/shakespeare_sonnet_i.txt\"\n",
    "sonnet = open(path,'r').read()\n",
    "print(sonnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have encountered a few string methods that allow you to manipulate texts. `str.lowercase()` for example, converts capitals to lower case.\n",
    "\n",
    "\n",
    "### -- Exercise\n",
    "Lowercase the sonnet and store it in a variable called `sonnet_lower`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this comment and add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While to a human reader (looking at the document in very formally) the sonnet clearly consist of multiple lines and has contains many words (such a surprise) this isn't obvious to the computer ingesting the document. At this stage the computer has no understanding of those basic elements of language, the concept of lines, sentences and words have to be made explicit before we can even start to encode the meaning of texts with computational means (to be discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let's define, because what are words anyway?\n",
    "Generally we make the distinction between `type` and `token`. We follow the definition [Smith, N.A., 2019](https://arxiv.org/pdf/1902.06006.pdf). \n",
    "- \"A word **token** is a word observed in a piece of text.\" \n",
    "- \"A word **type** is a distinct word, in the abstract, rather than a specific instance. Every word token is said to “belong” to its type.\"\n",
    "\n",
    " Example:\n",
    " > The sentence \"two teas and two coffees\" contains 5 tokens and 4 types (two appears twice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the introduction, text comes initially as unstructured data, as a sequence of characters which we have manipulate to process properly. To make this clear, we can revisit the index notation to inspect the basic elements of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you notice `sonnet[0]` doesn't return the first word but the first character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seemingly straightforward way to transform the string to tokens is by splitting the text by white spaces. In this scenario we perceive white spaces as boundaries between tokens. Luckily Python provides us with a tool to do just that. The `str.split()` method will use the white spaces to split a string into a list of tokens. Run the code below, and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'fairest', 'creatures', 'we', 'desire', 'increase,', 'That', 'thereby', \"beauty's\", 'rose', 'might', 'never', 'die,', 'But', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease,', 'His', 'tender', 'heir', 'might', 'bear', 'his', 'memory:', 'But', 'thou,', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes,', \"Feed'st\", 'thy', \"light's\", 'flame', 'with', 'self-substantial', 'fuel,', 'Making', 'a', 'famine', 'where', 'abundance', 'lies,', 'Thyself', 'thy', 'foe,', 'to', 'thy', 'sweet', 'self', 'too', 'cruel:', 'Thou', 'that', 'art', 'now', 'the', \"world's\", 'fresh', 'ornament,', 'And', 'only', 'herald', 'to', 'the', 'gaudy', 'spring,', 'Within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content,', 'And', 'tender', 'churl', \"mak'st\", 'waste', 'in', 'niggarding:', 'Pity', 'the', 'world,', 'or', 'else', 'this', 'glutton', 'be,', 'To', 'eat', 'the', \"world's\", 'due,', 'by', 'the', 'grave', 'and', 'thee.']\n"
     ]
    }
   ],
   "source": [
    "tokens = sonnet.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks different to what we have encountered before:\n",
    "- The output is enclosed by square brackets\n",
    "- the quotation marks are now (approximately) around the individual words and not the whole string\n",
    "- words are separated by commas\n",
    "\n",
    "What happened here is the following: split takes a string and returns a list of tokens. A `list` is another Python data type, such as strings, which we will be using a lot in the remainder of this course. The `intermezzo` provides more information, but we discuss them also here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Python list is an \"is an ordered collection of values\" ([Wentworth, et al. 2012](https://openbookproject.net/thinkcs/python/english3e/lists.html)). It is container that keeps several elements (also called) items in a particular order. Documents are often presented as a list, i.e. as a sequence of tokens in a specific order. \n",
    "\n",
    "Each element in the list implicitly indexed by place, i.e. you can retrieve an items by its position, for example the first and last word of the sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thee.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `len()` we can count the number of items the list contains (notice how this is different from the number of characters a string contains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we called the variable in which we save the string we split `tokens`, upon closer inspection you may notice that some elements in this list aren't technically tokens as they also include some punctuation marks. If we look at items at position 5, 8 and 41 the difficulty of converting a string to a list of tokens becomes apparent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('increase,', \"beauty's\", 'self-substantial')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[5],tokens[8],tokens[41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `'increase,'` is clearly a token followed by a punctuation mark, `\"self-substantial\"` is more complex. It depends on how you interpret and process such compounds (read it as one word, or split it into two, `\"self\"` and `\"substantial\"`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, you don't have to worry too much about the subtleties unless you really want to! What makes Python so convenient are the many external libraries that provide you tools (in the form of function) that help you with more complex tasks.\n",
    "\n",
    "Below we look at a very popular (but maybe outdated at this point) tool called the Natural Language Toolkit (NLTK). Later we discuss a few other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is a Python library for natural language processing, it was built to make certain like tokenization easier. The syntax below is unfamiliar and the intermezzo points to a more elaborate explanation. What this line of coude actually is does is importing tool (a function with the name `word_tokenize`) into our Notebook. This function is stored in the library (in `nltk.tokenize`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing `word_tokenize` we can apply it to our sonnet and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'fairest', 'creatures', 'we', 'desire', 'increase', ',', 'That', 'thereby', 'beauty', \"'s\", 'rose', 'might', 'never', 'die', ',', 'But', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', ',', 'His', 'tender', 'heir', 'might', 'bear', 'his', 'memory', ':', 'But', 'thou', ',', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', ',', \"Feed'st\", 'thy', 'light', \"'s\", 'flame', 'with', 'self-substantial', 'fuel', ',', 'Making', 'a', 'famine', 'where', 'abundance', 'lies', ',', 'Thyself', 'thy', 'foe', ',', 'to', 'thy', 'sweet', 'self', 'too', 'cruel', ':', 'Thou', 'that', 'art', 'now', 'the', 'world', \"'s\", 'fresh', 'ornament', ',', 'And', 'only', 'herald', 'to', 'the', 'gaudy', 'spring', ',', 'Within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content', ',', 'And', 'tender', 'churl', \"mak'st\", 'waste', 'in', 'niggarding', ':', 'Pity', 'the', 'world', ',', 'or', 'else', 'this', 'glutton', 'be', ',', 'To', 'eat', 'the', 'world', \"'s\", 'due', ',', 'by', 'the', 'grave', 'and', 'thee', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_nltk = word_tokenize(sonnet)\n",
    "print(tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Exercise: \n",
    "\n",
    "The previous example returns a different number of tokens. Inspect the difference between splitting by white spaces and NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with lowercasing, tokenization is an essential step in the text processing pipeline. Now we can start investigating the sonnet in more detail, for example by counting words. The easiest way of doing this is using a `Counter()` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'from': 1,\n",
       "         'fairest': 1,\n",
       "         'creatures': 1,\n",
       "         'we': 1,\n",
       "         'desire': 1,\n",
       "         'increase': 1,\n",
       "         ',': 14,\n",
       "         'that': 2,\n",
       "         'thereby': 1,\n",
       "         'beauty': 1,\n",
       "         \"'s\": 4,\n",
       "         'rose': 1,\n",
       "         'might': 2,\n",
       "         'never': 1,\n",
       "         'die': 1,\n",
       "         'but': 2,\n",
       "         'as': 1,\n",
       "         'the': 6,\n",
       "         'riper': 1,\n",
       "         'should': 1,\n",
       "         'by': 2,\n",
       "         'time': 1,\n",
       "         'decease': 1,\n",
       "         'his': 2,\n",
       "         'tender': 2,\n",
       "         'heir': 1,\n",
       "         'bear': 1,\n",
       "         'memory': 1,\n",
       "         ':': 3,\n",
       "         'thou': 2,\n",
       "         'contracted': 1,\n",
       "         'to': 4,\n",
       "         'thine': 2,\n",
       "         'own': 2,\n",
       "         'bright': 1,\n",
       "         'eyes': 1,\n",
       "         \"feed'st\": 1,\n",
       "         'thy': 4,\n",
       "         'light': 1,\n",
       "         'flame': 1,\n",
       "         'with': 1,\n",
       "         'self-substantial': 1,\n",
       "         'fuel': 1,\n",
       "         'making': 1,\n",
       "         'a': 1,\n",
       "         'famine': 1,\n",
       "         'where': 1,\n",
       "         'abundance': 1,\n",
       "         'lies': 1,\n",
       "         'thyself': 1,\n",
       "         'foe': 1,\n",
       "         'sweet': 1,\n",
       "         'self': 1,\n",
       "         'too': 1,\n",
       "         'cruel': 1,\n",
       "         'art': 1,\n",
       "         'now': 1,\n",
       "         'world': 3,\n",
       "         'fresh': 1,\n",
       "         'ornament': 1,\n",
       "         'and': 3,\n",
       "         'only': 1,\n",
       "         'herald': 1,\n",
       "         'gaudy': 1,\n",
       "         'spring': 1,\n",
       "         'within': 1,\n",
       "         'bud': 1,\n",
       "         'buriest': 1,\n",
       "         'content': 1,\n",
       "         'churl': 1,\n",
       "         \"mak'st\": 1,\n",
       "         'waste': 1,\n",
       "         'in': 1,\n",
       "         'niggarding': 1,\n",
       "         'pity': 1,\n",
       "         'or': 1,\n",
       "         'else': 1,\n",
       "         'this': 1,\n",
       "         'glutton': 1,\n",
       "         'be': 1,\n",
       "         'eat': 1,\n",
       "         'due': 1,\n",
       "         'grave': 1,\n",
       "         'thee': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "path = \"example_data/notebook_3/shakespeare_sonnet_i.txt\"\n",
    "sonnet = open(path,'r').read()\n",
    "sonnet_lowercase = sonnet.lower()\n",
    "tokens = word_tokenize(sonnet_lowercase)\n",
    "word_counts = Counter(tokens)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, here we skip many of the subtleties and technicalities, but what is important to understand here is that a `Counter()` maps tokens to their frequencies. Such mapping is usually handled by **dictionaries**, for example a very simple translation dictionary would look like:\n",
    "\n",
    "```python\n",
    "{'one':'einz',\n",
    " 'two':'zwei'}\n",
    "```\n",
    "\n",
    "Note the curly brackets, indicating a different data type. \n",
    "Words at the left of the colon are called **keys**, those at those at the right are **values**, each key-value is called an **item**.\n",
    "\n",
    "You can assign a dictionary to a variable and then retrieve the value for a given key as shown in the example below. Please note that we are using here square brackets again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "einz\n"
     ]
    }
   ],
   "source": [
    "english2german = {'one':'einz', 'two':'zwei'}\n",
    "print(english2german['one'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please consult the link in the breakout for more information about dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Counter()` objects are in many ways similar to dictionaries, you can retrieve the frequency for a given word by looking up the value for a specific key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts['and']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the word doesn't appear in the text, it returns `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-32949625bfbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hello'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_counts' is not defined"
     ]
    }
   ],
   "source": [
    "word_counts['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the `Counter()` has a few useful methods that make life easier, for example we can print the `n` most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 14),\n",
       " ('the', 6),\n",
       " (\"'s\", 4),\n",
       " ('to', 4),\n",
       " ('thy', 4),\n",
       " (':', 3),\n",
       " ('world', 3),\n",
       " ('and', 3),\n",
       " ('that', 2),\n",
       " ('might', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will notice, the most frequent words often don't "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Breakout`\n",
    "- `''.join()`\n",
    "- libraries and imports\n",
    "- dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Text Processing with SpaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While NLTK is convenient, and still used in DH, other libraries have emerged and are slowly pushing the state-of-the art. We will have a closer look at SpaCy a more powerful (and fast!) tool for automatic language analysis. Similar to NLTK we have to import the library at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SpaCy we first have to lead a model, which is trained on a specific language for specific tasks: tokenization, lemmatization and more. In this sense SpaCy works somewhat different than NLTK: with SpaCy we apply many different types of linguistic analysis and enrichment at once. Whereas in NLTK you would invoke seperate function. \n",
    "\n",
    "The code below makes this distinction more clear. We load the model and save it in `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, an example text is assigned to `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"A trifling incident thus served to settle a victory.  Now-a days, a soldier is so much of a machine that he seems simply to go through certain evolutions, in which there is no opportunity for the display of personal bravery or cowardice.  He does not know what is going on in other parts of the field, and has no real knowledge, till all be over, whether the day has been lost or won.”\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply call `nlp` passing `paragraph` as an argument. The returns an instance of the class `spacy.tokens.doc.Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(paragraph)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to lists, we can retrieve individual elements from `doc` using index notations.\n",
    "Let's have a closer look at the third element, the word incident in `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incident"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `help()` function reveals the many attributes that belong to each individual token. These are attributes are created by the model SpaCy applied to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Token object:\n",
      "\n",
      "class Token(builtins.object)\n",
      " |  An individual token – i.e. a word, punctuation symbol, whitespace,\n",
      " |  etc.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/token\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bytes__(...)\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      The number of unicode characters in the token, i.e. `token.text`.\n",
      " |      \n",
      " |      RETURNS (int): The number of unicode characters in the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#len\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |  \n",
      " |  check_flag(...)\n",
      " |      Check the value of a boolean flag.\n",
      " |      \n",
      " |      flag_id (int): The ID of the flag attribute.\n",
      " |      RETURNS (bool): Whether the flag is set.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#check_flag\n",
      " |  \n",
      " |  is_ancestor(...)\n",
      " |      Check whether this token is a parent, grandparent, etc. of another\n",
      " |      in the dependency tree.\n",
      " |      \n",
      " |      descendant (Token): Another token.\n",
      " |      RETURNS (bool): Whether this token is the ancestor of the descendant.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#is_ancestor\n",
      " |  \n",
      " |  nbor(...)\n",
      " |      Get a neighboring token.\n",
      " |      \n",
      " |      i (int): The relative position of the token to get. Defaults to 1.\n",
      " |      RETURNS (Token): The token at position `self.doc[self.i+i]`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#nbor\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |      similarity using an average of word vectors.\n",
      " |      \n",
      " |      other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |          `Span`, `Token` and `Lexeme` objects.\n",
      " |      RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#similarity\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_extension(...) from builtins.type\n",
      " |      Look up a previously registered extension by name.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (tuple): A `(default, method, getter, setter)` tuple.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#get_extension\n",
      " |  \n",
      " |  has_extension(...) from builtins.type\n",
      " |      Check whether an extension has been registered.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (bool): Whether the extension has been registered.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#has_extension\n",
      " |  \n",
      " |  remove_extension(...) from builtins.type\n",
      " |      Remove a previously registered extension.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (tuple): A `(default, method, getter, setter)` tuple of the\n",
      " |          removed extension.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#remove_extension\n",
      " |  \n",
      " |  set_extension(...) from builtins.type\n",
      " |      Define a custom attribute which becomes available as `Token._`.\n",
      " |      \n",
      " |      name (unicode): Name of the attribute to set.\n",
      " |      default: Optional default value of the attribute.\n",
      " |      getter (callable): Optional getter function.\n",
      " |      setter (callable): Optional setter function.\n",
      " |      method (callable): Optional method for method extension.\n",
      " |      force (bool): Force overwriting existing attribute.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#set_extension\n",
      " |      USAGE: https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  ancestors\n",
      " |      A sequence of this token's syntactic ancestors.\n",
      " |      \n",
      " |      YIELDS (Token): A sequence of ancestor tokens such that\n",
      " |          `ancestor.is_ancestor(self)`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#ancestors\n",
      " |  \n",
      " |  children\n",
      " |      A sequence of the token's immediate syntactic children.\n",
      " |      \n",
      " |      YIELDS (Token): A child token such that `child.head==self`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#children\n",
      " |  \n",
      " |  cluster\n",
      " |      RETURNS (int): Brown cluster ID.\n",
      " |  \n",
      " |  conjuncts\n",
      " |      A sequence of coordinated tokens, including the token itself.\n",
      " |      \n",
      " |      RETURNS (tuple): The coordinated tokens.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#conjuncts\n",
      " |  \n",
      " |  dep\n",
      " |      RETURNS (uint64): ID of syntactic dependency label.\n",
      " |  \n",
      " |  dep_\n",
      " |      RETURNS (unicode): The syntactic dependency label.\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  ent_id\n",
      " |      RETURNS (uint64): ID of the entity the token is an instance of,\n",
      " |      if any.\n",
      " |  \n",
      " |  ent_id_\n",
      " |      RETURNS (unicode): ID of the entity the token is an instance of,\n",
      " |      if any.\n",
      " |  \n",
      " |  ent_iob\n",
      " |      IOB code of named entity tag. `1=\"I\", 2=\"O\", 3=\"B\"`. 0 means no tag\n",
      " |      is assigned.\n",
      " |      \n",
      " |      RETURNS (uint64): IOB code of named entity tag.\n",
      " |  \n",
      " |  ent_iob_\n",
      " |      IOB code of named entity tag. \"B\" means the token begins an entity,\n",
      " |      \"I\" means it is inside an entity, \"O\" means it is outside an entity,\n",
      " |      and \"\" means no entity tag is set. \"B\" with an empty ent_type\n",
      " |      means that the token is blocked from further processing by NER.\n",
      " |      \n",
      " |      RETURNS (unicode): IOB code of named entity tag.\n",
      " |  \n",
      " |  ent_kb_id\n",
      " |      RETURNS (uint64): Named entity KB ID.\n",
      " |  \n",
      " |  ent_kb_id_\n",
      " |      RETURNS (unicode): Named entity KB ID.\n",
      " |  \n",
      " |  ent_type\n",
      " |      RETURNS (uint64): Named entity type.\n",
      " |  \n",
      " |  ent_type_\n",
      " |      RETURNS (unicode): Named entity type.\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#has_vector\n",
      " |  \n",
      " |  head\n",
      " |      The syntactic parent, or \"governor\", of this token.\n",
      " |      \n",
      " |      RETURNS (Token): The token predicted by the parser to be the head of\n",
      " |          the current token.\n",
      " |  \n",
      " |  i\n",
      " |  \n",
      " |  idx\n",
      " |      RETURNS (int): The character offset of the token within the parent\n",
      " |      document.\n",
      " |  \n",
      " |  is_alpha\n",
      " |      RETURNS (bool): Whether the token consists of alpha characters.\n",
      " |      Equivalent to `token.text.isalpha()`.\n",
      " |  \n",
      " |  is_ascii\n",
      " |      RETURNS (bool): Whether the token consists of ASCII characters.\n",
      " |      Equivalent to `[any(ord(c) >= 128 for c in token.text)]`.\n",
      " |  \n",
      " |  is_bracket\n",
      " |      RETURNS (bool): Whether the token is a bracket.\n",
      " |  \n",
      " |  is_currency\n",
      " |      RETURNS (bool): Whether the token is a currency symbol.\n",
      " |  \n",
      " |  is_digit\n",
      " |      RETURNS (bool): Whether the token consists of digits. Equivalent to\n",
      " |      `token.text.isdigit()`.\n",
      " |  \n",
      " |  is_left_punct\n",
      " |      RETURNS (bool): Whether the token is a left punctuation mark.\n",
      " |  \n",
      " |  is_lower\n",
      " |      RETURNS (bool): Whether the token is in lowercase. Equivalent to\n",
      " |      `token.text.islower()`.\n",
      " |  \n",
      " |  is_oov\n",
      " |      RETURNS (bool): Whether the token is out-of-vocabulary.\n",
      " |  \n",
      " |  is_punct\n",
      " |      RETURNS (bool): Whether the token is punctuation.\n",
      " |  \n",
      " |  is_quote\n",
      " |      RETURNS (bool): Whether the token is a quotation mark.\n",
      " |  \n",
      " |  is_right_punct\n",
      " |      RETURNS (bool): Whether the token is a right punctuation mark.\n",
      " |  \n",
      " |  is_sent_end\n",
      " |      A boolean value indicating whether the token ends a sentence.\n",
      " |      `None` if unknown. Defaults to `True` for the last token in the `Doc`.\n",
      " |      \n",
      " |      RETURNS (bool / None): Whether the token ends a sentence.\n",
      " |          None if unknown.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#is_sent_end\n",
      " |  \n",
      " |  is_sent_start\n",
      " |      A boolean value indicating whether the token starts a sentence.\n",
      " |      `None` if unknown. Defaults to `True` for the first token in the `Doc`.\n",
      " |      \n",
      " |      RETURNS (bool / None): Whether the token starts a sentence.\n",
      " |          None if unknown.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#is_sent_start\n",
      " |  \n",
      " |  is_space\n",
      " |      RETURNS (bool): Whether the token consists of whitespace characters.\n",
      " |      Equivalent to `token.text.isspace()`.\n",
      " |  \n",
      " |  is_stop\n",
      " |      RETURNS (bool): Whether the token is a stop word, i.e. part of a\n",
      " |      \"stop list\" defined by the language data.\n",
      " |  \n",
      " |  is_title\n",
      " |      RETURNS (bool): Whether the token is in titlecase. Equivalent to\n",
      " |      `token.text.istitle()`.\n",
      " |  \n",
      " |  is_upper\n",
      " |      RETURNS (bool): Whether the token is in uppercase. Equivalent to\n",
      " |      `token.text.isupper()`\n",
      " |  \n",
      " |  lang\n",
      " |      RETURNS (uint64): ID of the language of the parent document's\n",
      " |      vocabulary.\n",
      " |  \n",
      " |  lang_\n",
      " |      RETURNS (unicode): Language of the parent document's vocabulary,\n",
      " |      e.g. 'en'.\n",
      " |  \n",
      " |  left_edge\n",
      " |      The leftmost token of this token's syntactic descendents.\n",
      " |      \n",
      " |      RETURNS (Token): The first token such that `self.is_ancestor(token)`.\n",
      " |  \n",
      " |  lefts\n",
      " |      The leftward immediate children of the word, in the syntactic\n",
      " |      dependency parse.\n",
      " |      \n",
      " |      YIELDS (Token): A left-child of the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#lefts\n",
      " |  \n",
      " |  lemma\n",
      " |      RETURNS (uint64): ID of the base form of the word, with no\n",
      " |      inflectional suffixes.\n",
      " |  \n",
      " |  lemma_\n",
      " |      RETURNS (unicode): The token lemma, i.e. the base form of the word,\n",
      " |      with no inflectional suffixes.\n",
      " |  \n",
      " |  lex_id\n",
      " |      RETURNS (int): Sequential ID of the token's lexical type.\n",
      " |  \n",
      " |  like_email\n",
      " |      RETURNS (bool): Whether the token resembles an email address.\n",
      " |  \n",
      " |  like_num\n",
      " |      RETURNS (bool): Whether the token resembles a number, e.g. \"10.9\",\n",
      " |      \"10\", \"ten\", etc.\n",
      " |  \n",
      " |  like_url\n",
      " |      RETURNS (bool): Whether the token resembles a URL.\n",
      " |  \n",
      " |  lower\n",
      " |      RETURNS (uint64): ID of the lowercase token text.\n",
      " |  \n",
      " |  lower_\n",
      " |      RETURNS (unicode): The lowercase token text. Equivalent to\n",
      " |      `Token.text.lower()`.\n",
      " |  \n",
      " |  morph\n",
      " |  \n",
      " |  n_lefts\n",
      " |      The number of leftward immediate children of the word, in the\n",
      " |      syntactic dependency parse.\n",
      " |      \n",
      " |      RETURNS (int): The number of leftward immediate children of the\n",
      " |          word, in the syntactic dependency parse.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#n_lefts\n",
      " |  \n",
      " |  n_rights\n",
      " |      The number of rightward immediate children of the word, in the\n",
      " |      syntactic dependency parse.\n",
      " |      \n",
      " |      RETURNS (int): The number of rightward immediate children of the\n",
      " |          word, in the syntactic dependency parse.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#n_rights\n",
      " |  \n",
      " |  norm\n",
      " |      RETURNS (uint64): ID of the token's norm, i.e. a normalised form of\n",
      " |      the token text. Usually set in the language's tokenizer exceptions\n",
      " |      or norm exceptions.\n",
      " |  \n",
      " |  norm_\n",
      " |      RETURNS (unicode): The token's norm, i.e. a normalised form of the\n",
      " |      token text. Usually set in the language's tokenizer exceptions or\n",
      " |      norm exceptions.\n",
      " |  \n",
      " |  orth\n",
      " |      RETURNS (uint64): ID of the verbatim text content.\n",
      " |  \n",
      " |  orth_\n",
      " |      RETURNS (unicode): Verbatim text content (identical to\n",
      " |      `Token.text`). Exists mostly for consistency with the other\n",
      " |      attributes.\n",
      " |  \n",
      " |  pos\n",
      " |      RETURNS (uint64): ID of coarse-grained part-of-speech tag.\n",
      " |  \n",
      " |  pos_\n",
      " |      RETURNS (unicode): Coarse-grained part-of-speech tag.\n",
      " |  \n",
      " |  prefix\n",
      " |      RETURNS (uint64): ID of a length-N substring from the start of the\n",
      " |      token. Defaults to `N=1`.\n",
      " |  \n",
      " |  prefix_\n",
      " |      RETURNS (unicode): A length-N substring from the start of the token.\n",
      " |      Defaults to `N=1`.\n",
      " |  \n",
      " |  prob\n",
      " |      RETURNS (float): Smoothed log probability estimate of token type.\n",
      " |  \n",
      " |  rank\n",
      " |      RETURNS (int): Sequential ID of the token's lexical type, used to\n",
      " |      index into tables, e.g. for word vectors.\n",
      " |  \n",
      " |  right_edge\n",
      " |      The rightmost token of this token's syntactic descendents.\n",
      " |      \n",
      " |      RETURNS (Token): The last token such that `self.is_ancestor(token)`.\n",
      " |  \n",
      " |  rights\n",
      " |      The rightward immediate children of the word, in the syntactic\n",
      " |      dependency parse.\n",
      " |      \n",
      " |      YIELDS (Token): A right-child of the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#rights\n",
      " |  \n",
      " |  sent\n",
      " |      RETURNS (Span): The sentence span that the token is a part of.\n",
      " |  \n",
      " |  sent_start\n",
      " |  \n",
      " |  sentiment\n",
      " |      RETURNS (float): A scalar value indicating the positivity or\n",
      " |      negativity of the token.\n",
      " |  \n",
      " |  shape\n",
      " |      RETURNS (uint64): ID of the token's shape, a transform of the\n",
      " |      tokens's string, to show orthographic features (e.g. \"Xxxx\", \"dd\").\n",
      " |  \n",
      " |  shape_\n",
      " |      RETURNS (unicode): Transform of the tokens's string, to show\n",
      " |      orthographic features. For example, \"Xxxx\" or \"dd\".\n",
      " |  \n",
      " |  string\n",
      " |      Deprecated: Use Token.text_with_ws instead.\n",
      " |  \n",
      " |  subtree\n",
      " |      A sequence containing the token and all the token's syntactic\n",
      " |      descendants.\n",
      " |      \n",
      " |      YIELDS (Token): A descendent token such that\n",
      " |          `self.is_ancestor(descendent) or token == self`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#subtree\n",
      " |  \n",
      " |  suffix\n",
      " |      RETURNS (uint64): ID of a length-N substring from the end of the\n",
      " |      token. Defaults to `N=3`.\n",
      " |  \n",
      " |  suffix_\n",
      " |      RETURNS (unicode): A length-N substring from the end of the token.\n",
      " |      Defaults to `N=3`.\n",
      " |  \n",
      " |  tag\n",
      " |      RETURNS (uint64): ID of fine-grained part-of-speech tag.\n",
      " |  \n",
      " |  tag_\n",
      " |      RETURNS (unicode): Fine-grained part-of-speech tag.\n",
      " |  \n",
      " |  tensor\n",
      " |  \n",
      " |  text\n",
      " |      RETURNS (unicode): The original verbatim text of the token.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      RETURNS (unicode): The text content of the span (with trailing\n",
      " |      whitespace).\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the token's semantics.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#vector\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the token's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#vector_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  whitespace_\n",
      " |      RETURNS (unicode): The trailing whitespace character, if present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example is the part-of-speech, the syntactic category to which the token belongs. In this case, SpaCy predicted *\"incident\"* is a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemma is the standardized form of a token. For example, the plural of noun is reduced to a singular, and verb forms are brought back to the infinitif form. For example, \"served\" has the lemma \"serve\", \"revolutions\" has the lemma \"revolution\".\n",
    "\n",
    "Why is this useful? Well it depends on what you want to do. Similar to lowercasing, lemmatization reduces the complexity of text, tokens that otherwise have different surface forms are the identitical, we simplifies for example when we want to count words or compute relations between tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'serve'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[4].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evolution'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[32].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the lemmatized text, we first create an new `list` variable. This will be an empty `list`, but as we iterate over the elements in `doc` as add the lemma of each token (hidden in the `.lemma_` atrribute. \n",
    "\n",
    "Even though this technique (of initializing an empty `list`) is maybe confusing at first, we will repeat it often in the following Notebook. Please take your time to understand the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'trifle', 'incident', 'thus', 'serve', 'to', 'settle', 'a', 'victory', '.', ' ', 'now', '-', 'a', 'day', ',', 'a', 'soldier', 'be', 'so', 'much', 'of', 'a', 'machine', 'that', '-PRON-', 'seem', 'simply', 'to', 'go', 'through', 'certain', 'evolution', ',', 'in', 'which', 'there', 'be', 'no', 'opportunity', 'for', 'the', 'display', 'of', 'personal', 'bravery', 'or', 'cowardice', '.', ' ', '-PRON-', 'do', 'not', 'know', 'what', 'be', 'go', 'on', 'in', 'other', 'part', 'of', 'the', 'field', ',', 'and', 'have', 'no', 'real', 'knowledge', ',', 'till', 'all', 'be', 'over', ',', 'whether', 'the', 'day', 'have', 'be', 'lose', 'or', 'win', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "lemmas = []\n",
    "\n",
    "for token in doc:\n",
    "    lemmas.append(token.lemma_)\n",
    "    \n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we can harvest the part-of-speech of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'VERB', 'NOUN', 'ADV', 'VERB', 'PART', 'VERB', 'DET', 'NOUN', 'PUNCT', 'SPACE', 'ADV', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'AUX', 'ADV', 'ADJ', 'ADP', 'DET', 'NOUN', 'SCONJ', 'PRON', 'VERB', 'ADV', 'PART', 'VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT', 'ADP', 'DET', 'PRON', 'AUX', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'CCONJ', 'NOUN', 'PUNCT', 'SPACE', 'PRON', 'AUX', 'PART', 'VERB', 'PRON', 'AUX', 'VERB', 'ADP', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'AUX', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'SCONJ', 'DET', 'AUX', 'ADV', 'PUNCT', 'SCONJ', 'DET', 'NOUN', 'AUX', 'AUX', 'VERB', 'CCONJ', 'VERB', 'PUNCT', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "pos = []\n",
    "\n",
    "for token in doc:\n",
    "    pos.append(token.pos_)\n",
    "    \n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'DET'), ('trifle', 'VERB'), ('incident', 'NOUN'), ('thus', 'ADV'), ('serve', 'VERB'), ('to', 'PART'), ('settle', 'VERB'), ('a', 'DET'), ('victory', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE'), ('now', 'ADV'), ('-', 'PUNCT'), ('a', 'DET'), ('day', 'NOUN'), (',', 'PUNCT'), ('a', 'DET'), ('soldier', 'NOUN'), ('be', 'AUX'), ('so', 'ADV'), ('much', 'ADJ'), ('of', 'ADP'), ('a', 'DET'), ('machine', 'NOUN'), ('that', 'SCONJ'), ('-PRON-', 'PRON'), ('seem', 'VERB'), ('simply', 'ADV'), ('to', 'PART'), ('go', 'VERB'), ('through', 'ADP'), ('certain', 'ADJ'), ('evolution', 'NOUN'), (',', 'PUNCT'), ('in', 'ADP'), ('which', 'DET'), ('there', 'PRON'), ('be', 'AUX'), ('no', 'DET'), ('opportunity', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('display', 'NOUN'), ('of', 'ADP'), ('personal', 'ADJ'), ('bravery', 'NOUN'), ('or', 'CCONJ'), ('cowardice', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE'), ('-PRON-', 'PRON'), ('do', 'AUX'), ('not', 'PART'), ('know', 'VERB'), ('what', 'PRON'), ('be', 'AUX'), ('go', 'VERB'), ('on', 'ADP'), ('in', 'ADP'), ('other', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('field', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('have', 'AUX'), ('no', 'DET'), ('real', 'ADJ'), ('knowledge', 'NOUN'), (',', 'PUNCT'), ('till', 'SCONJ'), ('all', 'DET'), ('be', 'AUX'), ('over', 'ADV'), (',', 'PUNCT'), ('whether', 'SCONJ'), ('the', 'DET'), ('day', 'NOUN'), ('have', 'AUX'), ('be', 'AUX'), ('lose', 'VERB'), ('or', 'CCONJ'), ('win', 'VERB'), ('.', 'PUNCT'), ('\"', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "lemma_pos = []\n",
    "for token in doc:\n",
    "    lemma_pos.append((token.lemma_,token.pos_))\n",
    "    \n",
    "print(lemma_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination of lemmatization and part-of-speech tagging is quite common. It remove certain distinction (verb tense and plurals) but foregrounds that otherwise would have treated as the same word: for example the distinction between `fine` as noun and adjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "But "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['incident', 'victory', 'days', 'soldier', 'machine', 'evolutions', 'opportunity', 'display', 'bravery', 'cowardice', 'parts', 'field', 'knowledge', 'day']\n"
     ]
    }
   ],
   "source": [
    "nouns = []\n",
    "for token in doc:  \n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token.text)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['incident', 'victory', 'days', 'soldier', 'much', 'machine', 'certain', 'evolutions', 'opportunity', 'display', 'personal', 'bravery', 'cowardice', 'other', 'parts', 'field', 'real', 'knowledge', 'day']\n"
     ]
    }
   ],
   "source": [
    "sel = []\n",
    "for token in doc:  \n",
    "    if token.pos_ in [\"NOUN\",\"ADJ\"]:\n",
    "        sel.append(token.text)\n",
    "print(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy has a lot more to offer, and for example you can find Named Entities (places, persons and organisation) in texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 0 7 GPE\n",
      "Berlin 44 50 GPE\n",
      "Kaspar 68 74 PERSON\n",
      "Stanford 106 114 ORG\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Germany is a wonderful country. The city of Berlin is great! Do you Kaspar is still listening? He want to Stanford.\")\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Sentence Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
