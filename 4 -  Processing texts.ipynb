{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kasparvonbeelen/ghi_python/blob/main/4%20-%20%20Processing%20texts.ipynb)\n",
    "\n",
    "\n",
    "# 4 Processing texts\n",
    "\n",
    "\n",
    "## Text Mining for Historians (with Python)\n",
    "## A Gentle Introduction to Working with Textual Data in Python\n",
    "\n",
    "### Created by Kaspar Beelen and Luke Blaxill\n",
    "\n",
    "### For the German Historical Institute, London\n",
    "\n",
    "<img align=\"left\" src=\"https://www.ghil.ac.uk/typo3conf/ext/wacon_ghil/Resources/Public/Images/institute_icon_small.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you learn to process and extract information from texts. We continue with the sonnet, but as promised, scale up soon. \n",
    "\n",
    "This notebook focuses on extracting basic information from texts. We show how to use external libraries for more refined enrichment (finding named entities or zoom in on specific word categories (nouns, verbs)).\n",
    "\n",
    "At the end of this Notebook you'll be able to\n",
    "- Tokenize texts\n",
    "- Apply functions from Natural Language Toolkit and SpaCy for more advance processing (part-of-speech tagging)\n",
    "- Count the frequency of tokens\n",
    "- Have some understanding of Python `list` and `dict` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have basic understanding of how to read and manipulate textual data in Python. Now we can turn to more directly useful and realistic applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Run this cell to download data used in examples below\n",
    "from pathlib import Path\n",
    "Path(\"working_data\").mkdir(exist_ok=True)\n",
    "!wget https://raw.githubusercontent.com/kasparvonbeelen/ghi_python/main/example_data/notebook_3/shakespeare_sonnet_i.txt -O working_data/shakespeare_sonnet_i.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"working_data/shakespeare_sonnet_i.txt\" # declare path/ variable assignment\n",
    "sonnet = open(path,'r').read() # open and read document\n",
    "print(sonnet) # print document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While to a human reader—looking at the document very formally—the sonnet contains multiple words and lines (such a surprise) this isn't obvious to a computer ingesting the document. At this stage, Python represents the sonnet as a sequence of characters; it has no understanding of word boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are words anyway?\n",
    "Generally, we will make a distinction between `types` and `tokens` following the definition of [Smith, N.A., 2019](https://arxiv.org/pdf/1902.06006.pdf). \n",
    "- \"A word **token** is a word observed in a piece of text.\" \n",
    "- \"A word **type** is a distinct word, in the abstract, rather than a specific instance. Every word token is said to “belong” to its type.\"\n",
    "\n",
    " Example:\n",
    " > The sentence \"two teas and two coffees\" contains 5 tokens and 4 types (two appears twice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the introduction, textual data is unstructured. We have to manipulate and transform the sequence of characters in order to work with the content in meaningful ways.\n",
    "\n",
    "Let's start with detecting word boundaries, and covert the string of characters to list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you notice `sonnet[0]` doesn't return the first word but the first character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seemingly straightforward way to transform a string into tokens is by splitting the text by white spaces. In this scenario, we perceive white spaces as boundaries between tokens. Luckily Python provides us with a tool to do just that. The `str.split()` method will use the white spaces to split a string into a list of tokens. Run the code below, and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sonnet.split() # split by white space save resulting list in tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `str.split()` looks different to what we have encountered before:\n",
    "- The output is enclosed by square brackets `[]`\n",
    "- The quotation marks are now (approximately) around the individual words and not the whole string\n",
    "- Words are separated by commas\n",
    "\n",
    "What happened here is the following: split takes a string and returns a `list` (of tokens). A `list` is another Python data type, which we will be using a lot in the remainder of this course. The `break out` provides more information, but we discuss the most important aspect here as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Python list \"is an ordered collection of values\" ([Wentworth, et al. 2012](https://openbookproject.net/thinkcs/python/english3e/lists.html)). It is a container that keeps several elements (also called) items in a particular order. Documents are often presented as a list, i.e. as a sequence of tokens in a specific order. \n",
    "\n",
    "Each element in the list implicitly indexed by place, i.e. you can retrieve items by their position (for example, the first and last word of the sonnet with `[0]` and `[-1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`len()` counts the number of items in a list (notice how this is different from the number of characters in a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Breakout`\n",
    "- [lists in Python](break_out/lists.ipynb)\n",
    "- [`' '.join()`](https://www.w3schools.com/python/ref_string_join.asp) **[Under construction]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We called the variable in which we saved the split string  `tokens`, but upon closer inspection, you notice that some elements aren't technically tokens. Some include, for example, some punctuation marks. If we look at items at position 5, 8 or 41, the difficulty of converting a string to a list of tokens becomes apparent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[5],tokens[8],tokens[41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `'increase,'` is a token followed by a punctuation mark, `\"self-substantial\"` is more complex. It depends on how you interpret and process such compounds (read it as one word, or split it into two, `\"self\"` and `\"substantial\"`, but are hyphens always token boundaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, you don't have to worry too much about the subtleties (unless you want to!) because Python comes with many convenient external libraries that provide you plethora of tools (in the form of function) that help you with complex processing tasks.\n",
    "\n",
    "Here we take a closer look at a popular tool called the \"Natural Language Toolkit\" (abbreviated as NLTK). (Later, we discuss a few other options.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is a Python library for natural language processing, it was built to make processing text data (such as tokenization) easier. You don't have to write your functions all the time. \n",
    "\n",
    "The syntax below is unfamiliar and the `break out` points to a more elaborate explanation. The line of code imports a tool (in this case a function with the name `word_tokenize`) into our Notebook. This function is stored in `nltk.tokenize`. \n",
    "\n",
    "You don't have to import this function every time you use it, only once when running the notebook suffices (unless you restart the Kernel or Runtime, because then everything gets deleted from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kbeelen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing `word_tokenize`, we can apply it to our sonnet and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nltk = word_tokenize(sonnet)\n",
    "print(tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokens_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Exercise: \n",
    "\n",
    "The previous example returns a different number of tokens. Inspect the difference between splitting by white spaces and using NLTK's `word_tokenize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to lowercasing, tokenization is an essential step in the text processing pipeline. We can investigate the sonnet in more detail, for example, by counting words. The easiest way of doing this is using a `Counter()` object (which we also have to import). Counter maps the types in list to their frequency.\n",
    "\n",
    "At this point, we have all skills to put together a small word counting program.\n",
    "- import the required libraries (line 1 and 2)\n",
    "- create a variable with the name path that points to the location of the sonnet\n",
    "- read the document\n",
    "- lowercase the document\n",
    "- tokenize the document\n",
    "- count words\n",
    "- print word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # import Counter\n",
    "from nltk.tokenize import word_tokenize # import tool for tokenization\n",
    "path = \"working_data/shakespeare_sonnet_i.txt\" # tell python where the document is stored\n",
    "sonnet = open(path,'r').read() # open and read the document\n",
    "sonnet_lowercase = sonnet.lower() # lowercase and store result in sonnet_lower\n",
    "tokens = word_tokenize(sonnet_lowercase) # tokenize and store the list in \n",
    "word_counts = Counter(tokens) # map types to the frequency\n",
    "print(word_counts) # print word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip many of the subtleties and technicalities here, but what is important to understand is that a `Counter()` maps word types (that occur in a document) to their frequencies. \n",
    "\n",
    "Mapping of keys (word types) to values (counts) is handled by a data type called **dictionaries** (`Counter` is a variant on a dictionary, with a few more useful methods, but as far as we are concerned, they are largely identical).\n",
    "\n",
    "In Python, we could create a  simple translation dictionary, which looks like:\n",
    "\n",
    "```python\n",
    "{'one':'einz',\n",
    " 'two':'zwei'}\n",
    "```\n",
    "\n",
    "Note the **curly brackets**, indicating a different data type (lists have square brackets, strings use quotation marks). \n",
    "Words at the left of the colon are called **keys**, those at the right are **values**, each key-value pair is called an **item**.\n",
    "\n",
    "You can assign a dictionary to a variable and then **lookup** the value for a specific key, as shown in the example below. Please note that we are using here square brackets again (similar to how indexing looks up the item at a specific position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english2german = {'one':'einz', 'two':'zwei'} # create and english to german dictionary\n",
    "print(english2german['one']) # print the translation of 'one'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please consult the link in the breakout for more information about dictionaries.\n",
    "\n",
    "## `Breakout`\n",
    "- [dictionaries in Python](break_out/dictionaries.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our example. As said earlier, `Counter()` objects are similar to dictionaries: you can look up the frequency of a word by entering a key and print the associated value. We can print the frequency of \"and\" in our sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_counts['and'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the word doesn't appear in the text, it returns `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Counter()` has more useful methods that make life easier: the `.most_common` method prints the `n` most common words. For example, we can print the ten most frequent words in our sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Text Processing with SpaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While NLTK is a convenient tool and often used in DH, a few other libraries have emerged and are slowly pushing the state-of-the-art in the field. We will have a closer look at SpaCy, a powerful (and fast!) tool for automatic language analysis. Similar to NLTK we have to import the library at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # import the SpaCy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SpaCy we first have to load a model. This model is trained on a specific language and handles many different tasks: tokenization, lemmatization and more. In this sense, SpaCy works somewhat different than NLTK: with SpaCy we apply many different types of linguistic analysis and enrichment at once. Whereas in NLTK you would invoke a separate function. \n",
    "\n",
    "The code below makes this distinction more clear. We load the model and save it in the variable `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # Load English model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, an example text is assigned to `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"A trifling incident thus served to settle a victory.  \n",
    "            Now-a days, a soldier is so much of a machine that he seems simply to go through certain evolutions, in which there is no opportunity for the display of personal bravery or cowardice.  \n",
    "            He does not know what is going on in other parts of the field, and has no real knowledge, till all be over, whether the day has been lost or won.”\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply call `nlp` passing `paragraph` as an argument. The returns an instance of the class `spacy.tokens.doc.Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(paragraph)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to lists, we can retrieve individual elements from `doc` using the index notation.\n",
    "Let's have a closer look at the third element, the word incident in `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[2])\n",
    "print(type(doc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `help()` function reveals the attributes and methods attached to individual tokens. \n",
    "\n",
    "An attribute is a value that belongs to an object. In the example below, each token has a boolean `is_digit` attribute that is `True` if the token consists of digits.  \n",
    "\n",
    "The general syntax for attributes is: \n",
    "`object.attribute`\n",
    "\n",
    "This looks similar to the dot notation for methods.\n",
    "`object.method()`\n",
    "\n",
    "The main syntactic difference is the use of parentheses. \n",
    "\n",
    "Whereas methods perform some operation on an object (and often return a value) an attribute is just a value attached to an object.\n",
    "\n",
    "Apologies if this sounds confusing! After working through some examples you will quickly become familiar with attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `help` to \"incident\" token, and scroll down to the line **\"Data descriptors defined here:\"**\n",
    "Here you find a list of attributes attached to the SpaCy `Token` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy performs a syntactic analysis on the text and determines the part-of-speech of each token, which is then stored under `.pos_` attribute. In the case of *\"incident\"* the part-of-speech is a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[2].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also takes care of lemmatization. The lemma is the standardized form of a token. For example, plural nouns are reduced to a singular, and verbs forms are brought back to their infinitive. For example, \"served\" has the lemma \"serve\", \"revolutions\" has the lemma \"revolution\".\n",
    "\n",
    "Lemmas are attached to the `.lemma_` attribute of a token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[4].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[32].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this useful? It depends on what you want to do. Similar to lowercasing, lemmatization reduces the complexity of (or normalizes) a text: tokens that otherwise have different surface forms are reduced to the same token. If you want to search for (or would like to know the frequency of) a word, normalization is often helpful—you'd capture, for example, \"revolution\" and \"revolutions\" at the same time—unless you are particularly interested verb conjugation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the lemmatized text, we first create an new `list` variable. This will be an empty `list`, but as we iterate over the elements in `doc` as add the lemma of each token (hidden in the `.lemma_` atrribute. \n",
    "\n",
    "Even though this technique (of initializing an empty `list`) is maybe confusing at first, we will repeat it often in the following Notebook. Please take your time to understand the code below, as it contains a few new elements.\n",
    "\n",
    "- creation of an empty `list`\n",
    "- `for` loop\n",
    "- appending items to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "\n",
    "for t in doc:\n",
    "    lemmas.append(t.lemma_)\n",
    "    \n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iteration is performed by a `for` loop: in the example above we iterate of each `Token` in `doc`. `t` takes the value of each item in turn. We can use `print()` to make this visible. Iteration amounts to repeating the same operation to each element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `for` loop traverses through all items in doc and for each token `t` in doc, we can now repeat the same operation.\n",
    "- We access the `.lemma_` attribute of `t`\n",
    "- and append the lemma to the list `lemmas`. `lists.append(item)` adds items to list we initialized at the start of the cell.\n",
    "\n",
    "It doesn't matter what name you use for the **loop variable** `t` (the variable between `for` and `in`) as long as you use it consistently. The examples below hopefully make this clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abracadabra in doc: # you can give the loop variable any name you want\n",
    "    print(abracadabra) # but you should use it consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc: # otherwise things will go wrong\n",
    "    print(r) # and this line will raise a NameError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also the spacing, lines that end with a colon are followed by indented lines. This part of the Python syntax, removing the indentation will rais an `IndentationError`. Run the code below to convince yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc: \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we can harvest the part-of-speech of each token. Note that we repeat exactly the same procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [] # create an empty list with name pos\n",
    "\n",
    "for token in doc: # iterate over all items in doc\n",
    "    pos.append(token.pos_) # append the .pos_ attribute to pos\n",
    "    \n",
    "print(pos) # print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_pos = []\n",
    "for token in doc:\n",
    "    lemma_pos.append((token.lemma_,token.pos_))\n",
    "    \n",
    "print(lemma_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination of lemmatization and part-of-speech tagging is quite common. It remove certain distinction (verb tense and plurals) but foregrounds that otherwise would have treated as the same word: for example the distinction between `fine` as noun and adjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy has a lot more to offer, and for example you can find Named Entities (places, persons and organisation) in texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(\"Germany is a wonderful country. The city of Berlin is great! Do you Kaspar is still listening? He went to Stanford.\")\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Breakout`:\n",
    "- [Indentation](break_out/indentation.ipynb)\n",
    "- [`for` loop](break_out/loops.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Exercise:\n",
    "\n",
    "The code below will load \"A Tale of Two Cities\" into your Notebook.\n",
    "- apply the SpaCy `nlp` to this text\n",
    "- collect all named entities in a list `ner`\n",
    "- Count the named entities \n",
    "- print the ten most frequent entities\n",
    "- repeat the above but this time collect the `.label_` atribute of a named entity\n",
    "- compute and print the frequency of these labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "book = requests.get('https://www.gutenberg.org/files/98/98-0.txt').content.decode('utf-8') # download book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Exercise\n",
    "\n",
    "You can also use SpaCy for other languages then English. The code below will install a German model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the \"Die Leiden des jungen Werther\" from gutenberg.org and save it in `werther`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "werther = requests.get('https://www.gutenberg.org/cache/epub/2407/pg2407.txt').content.decode('utf-8') # download book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_de = nlp_de(werther)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the Named Entity recognition to `werther`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
