{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Corpus Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook explores various tools for analysing and comparing texts at the corpus level. As such, these are your first ventures into \"macro-analysis\" with Python. The methods described here are especially powerful in combination with the techniques for content selection explained in Notebook 5 **Corpus Creation**..\n",
    "\n",
    "More specifically we will have a closer look at:\n",
    "\n",
    "- **Keyword in Context Analysis**: Similar to concordance in AntConc\n",
    "- **Collocations**: Compute which tokens tend to co-occur together\n",
    "- **Feature selection**: Compute which tokens are distinctive for a subset of texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Keyword in Context\n",
    "\n",
    "Computers are excellent in indexing, organzing and retrieving information. However, interpreting information, extracting meaning from text is still a difficult task. Keyword-in-Context (KWIC) analysis, brings together the best of both worlds: the retrieval power of machines, with the close-reading skills of the historian. KWIC or concardance centres a corpus on sepecific query term, with `n` words to the left and the right. In fact, this is one of the earliest application of digital analysis of historical texts.\n",
    "\n",
    "In this section we investigate reports of the London Medical Officers of Health, the [London's Pulse corpus](https://wellcomelibrary.org/moh/). \n",
    "\n",
    "> The reports were produced each year by the Medical Officer of Health (MOH) of a district and set out the work done by his public health and sanitary officers. The reports provided vital data on birth and death rates, infant mortality, incidence of infectious and other diseases, and a general statement on the health of the population. \n",
    "\n",
    "Source: https://wellcomelibrary.org/moh/about-the-reports/about-the-medical-officer-of-health-reports/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start we importing the necessary libraries. Some of the code is explained in previous Notebooks, so won't discuss it into too much detail here.\n",
    "\n",
    "The tools we need are:\n",
    "- `nltk`: Natural Language Toolkint: for tokenization and concordance\n",
    "- `pathlib`: a library for managing files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # import natural language toolkit\n",
    "from pathlib import Path # import Path object from pathlib\n",
    "from nltk.tokenize import wordpunct_tokenize # import word_tokenize function from nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCityofWestminster.1901.b18247660.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1902.b18247672.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1903.b18247684.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1904.b18247696.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1905.b18247702.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1906.b18247714.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1907.b18247726.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1908.b18247738.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1909.b1824774x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1910.b18247751.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1911.b18247763.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1912.b18247775.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1913.b18247787.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1914.b18247799.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1915.b18247805.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1917.b18247817.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1920.b18247829.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1921.b18247830.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1922.b18247842.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1923.b18247854.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1924.b18247866.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1925.b18247878.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1926.b1824788x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1927.b18247891.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1928.b18247908.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1929.b1824791x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1930.b18247921.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1931.b18247933.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1932.b18247945.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1933.b18247957.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1934.b18247969.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1935.b18247970.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1936.b18247982.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1937.b18247994.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1938.b18248007.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1939.b18248019.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1940.b18248020.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1941.b18248032.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1942.b18248044.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1943.b18248056.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1944.b18248068.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1945.b1824807x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1947.b18248093.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1948.b1824810x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1949.b18248111.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1950.b18248123.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1951.b18248135.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1952.b18248147.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1953.b18248159.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1954.b18248160.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1955.b18248172.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1956.b18248184.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1957.b18248196.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1958.b18248202.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1959.b18248214.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1960.b18248226.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1961.b18248238.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1962.b1824824x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1963.b18248251.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1964.b18248263.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1965.b18248275.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1966.b18248287.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1967.b18248299.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1968.b18248305.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1969.b18248317.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1970.b18248329.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mCityofWestminster.1971.b18248330.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1893.b17950454.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1893.b17997835.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1894.b17999157.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1896.b19885039.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1896.b19885040.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1897.b18222869.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1898.b18222833.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1898.b18222882.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1899.b18222894.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1916.b18120854.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1918.b18120866.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplar.1919.b18120878.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarDistrictBowandStratford.1900.b18245730.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1901.b18245766.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1902.b18245778.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1903.b1824578x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1904.b18245791.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1905.b18245808.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1906.b1824581x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1907.b18245821.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1908.b18245833.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1909.b18245845.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1910.b18245857.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1911.b18245869.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1912.b18245870.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1913.b18245882.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1914.b18245894.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1915.b18245900.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1917.b18245912.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1920.b18245924.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1921.b18245936.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1922.b18245948.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1923.b1824595x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1924.b18245961.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1925.b18245973.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1926.b18245985.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1927.b18245997.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1928.b1824600x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1929.b18246011.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1930.b18246023.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1931.b18246035.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1932.b18246047.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1933.b18246059.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1936.b18246084.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1937.b18246096.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1938.b18246102.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1939.b18246114.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1940.b18246126.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1941.b18246138.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1942.b1824614x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1943.b18246151.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1944.b18246163.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1945.b18246175.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1946.b18246187.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1947.b18246199.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1949.b18246217.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1950.b18246229.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1951.b18246230.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1952.b18246242.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1953.b18246254.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1954.b18246266.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1955.b18246278.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1956.b1824628x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1957.b18246291.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1958.b18246308.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1959.b1824631x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1960.b18246321.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1961.b18246333.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1962.b18246345.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1963.b18246357.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarMetropolitanBorough.1964.b18246369.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarandBromley.1895.b18245742.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mPoplarandBromley.1900.b18245754.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1857.b18248342.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1857.b18248354.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1858.b18248366.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1858.b18248378.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1859.b1824838x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1860.b18248391.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1861.b18248408.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1888.b20057064.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1889.b20057076.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1891.b2005709x.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1892.b20057106.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1893.b18018312.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1894.b18018324.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1895.b19874364.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1896.b18038207.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1897.b19874352.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1898.b19874340.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1899.b18223011.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mWestminster.1900.b19823228.txt\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/MOH/python/ # list all files in data/MOH/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored in the following folder structure:\n",
    "\n",
    "```\n",
    "data\n",
    "|___ MOH\n",
    "     |___ python\n",
    "          |____ CityofWestminster.1901.b18247660.txt\n",
    "          |____ ...\n",
    "```\n",
    "\n",
    "The code below:\n",
    "- harvests all path to `.txt` files in `data/MOH/python`\n",
    "- converts the result to a `list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "moh_reports_paths = list(Path('data/MOH/python').glob('*.txt')) # get all txt files in data/MOH/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the paths to ten document with list slicing: `[:10]` means, get document from index positions `0` till `9`. (i.e. the first ten items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data/MOH/python/PoplarMetropolitanBorough.1945.b18246175.txt'), PosixPath('data/MOH/python/CityofWestminster.1932.b18247945.txt'), PosixPath('data/MOH/python/CityofWestminster.1921.b18247830.txt'), PosixPath('data/MOH/python/PoplarandBromley.1900.b18245754.txt'), PosixPath('data/MOH/python/Poplar.1919.b18120878.txt'), PosixPath('data/MOH/python/PoplarMetropolitanBorough.1920.b18245924.txt'), PosixPath('data/MOH/python/CityofWestminster.1907.b18247726.txt'), PosixPath('data/MOH/python/CityofWestminster.1906.b18247714.txt'), PosixPath('data/MOH/python/CityofWestminster.1903.b18247684.txt'), PosixPath('data/MOH/python/PoplarMetropolitanBorough.1902.b18245778.txt')]\n"
     ]
    }
   ],
   "source": [
    "print(moh_reports_paths[:10]) # print the first ten items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know where all the files are located, we can apply the following steps:\n",
    "- create an empty list variable where we will store the tokens of the corpus (line 3)\n",
    "- iterate over the collected paths (line 5)\n",
    "- read the text file (line 6)\n",
    "- lowercase the text (line 6)\n",
    "- tokenize the string (line 7): this converts the string to a list of tokens\n",
    "- iterate over tokens (line 8)\n",
    "- test if a token is contain only alphabetic characters (line 9)\n",
    "- add token to the list if line 9 evaluates to True (line 10)\n",
    "\n",
    "The general flow of the program is similar to what we've seen before: we create an empty list (or other object) where we store specific information from a text collection, in this case all alphabetic tokens.\n",
    "\n",
    "We use one more notebook functionalities here\n",
    "- `%%time` print how long the cell took to run\n",
    "\n",
    "It could take a few seconds for the cell to run, so please be a bit pit patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected 6641589 tokens\n",
      "CPU times: user 4.53 s, sys: 215 ms, total: 4.74 s\n",
      "Wall time: 4.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus = [] # inititialize an empty list where we will store the MOH reports\n",
    "\n",
    "for p in moh_reports_paths: # iterate over the paths to MOH reports, p will take the value of each item in moh_reports_paths \n",
    "    text_lower = open(r).read().lower() # read the text files and lowercase the string\n",
    "    tokens = wordpunct_tokenize(text_lower) # tokenize the string\n",
    "    for token in tokens: # iterate over the tokens\n",
    "        if token.isalpha(): # test if token only contains alphabetic characteris\n",
    "            corpus.append(token) # if the above test evaluates to True, append token to the corpus list\n",
    "print('collected', len(corpus),'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this small program works perfectly fine, it's not the most efficient code. The example below is a bit more, especially if you're confronted with lots of text files. \n",
    "\n",
    "- the `with open` statement is a convenient way of handling the opening and closing of files, to make sure you don't keep all information in memory, which would slow down the execution of your program\n",
    "- line 8 shows a list comprehension, this actually similar to a for loop, but faster and more concise.\n",
    "\n",
    "We won't spend too much time discussing list comprehensions, the examples below should suffice for now. We write a small programs that collects odd numbers. First we generate a list of numbers with `range(10)`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the output of range(10)\n",
    "list(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the we test for division by 2: `%` is the modulus operator, \"which returns the remainder after dividing the left-hand operand by right-hand operand\". It `n % 2` evaluates to `0` if a number `n` can be divided by `2`. In Python `0` is equal to `False`, meaning if `n % 2` evaluates to `0` we won't append the number to `odd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n",
      "CPU times: user 249 µs, sys: 165 µs, total: 414 µs\n",
      "Wall time: 322 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# program for find odd numbers\n",
    "numbers = range(10) # get numbers 0 to 9\n",
    "odd = [] # empty list where we store even numbers\n",
    "for k in numbers: # iterate over numbers\n",
    "    if k % 2: # test if number if divisible by 2\n",
    "        odd.append(k) # if True append\n",
    "print(odd) # print number of tokens collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved with just one line of code using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "odd = [k for k in range(10) if k % 2]\n",
    "print(odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Exercise\n",
    "\n",
    "To see differences in performance, do the follwoing:\n",
    "\n",
    "- remove the `print()` statement\n",
    "- crank up the size of the list, i.e. change range(10) to range(1000000).\n",
    "- compare the **Wall time** of these cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now returning to the actual example: Run the slightly better code and observe that it produces the same output, just faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected 6641589 tokens\n",
      "CPU times: user 3.92 s, sys: 236 ms, total: 4.16 s\n",
      "Wall time: 4.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus = [] # inititialize an empty list where we will store the MOH reports\n",
    "\n",
    "for p in moh_reports_paths: # iterate over the paths to MOH reports, p will take the value of each item in moh_reports_paths \n",
    "    with open(r) as in_doc: # make sure to close the document after opening it\n",
    "        tokens = wordpunct_tokenize(in_doc.read().lower())\n",
    "        corpus.extend([t for t in tokens if t.isalpha()]) # list comprehension    \n",
    "print('collected', len(corpus),'tokens') # print number of tokens collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting all tokens in a `list` we can convert this of another data type, a NLTK `Text` object. The cell below shows the results of the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'nltk.text.Text'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus))\n",
    "nltk_corpus = nltk.text.Text(corpus) # convert the list of tokens to a nltk.text.Text object\n",
    "print(type(nltk_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this useful? Well the `Text` object comes with many useful methods for corpus exploration. To inspect all the tools attached to a `Text` object, apply the `help()` function to `nltk_corpus` or (`help(nltk.text.Text)` would do the same trick). You have to scroll down a bit (ignore all methods starting with `__`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Text in module nltk.text object:\n",
      "\n",
      "class Text(builtins.object)\n",
      " |  Text(tokens, name=None)\n",
      " |  \n",
      " |  A wrapper around a sequence of simple (string) tokens, which is\n",
      " |  intended to support initial exploration of texts (via the\n",
      " |  interactive console).  Its methods perform a variety of analyses\n",
      " |  on the text's contexts (e.g., counting, concordancing, collocation\n",
      " |  discovery), and display the results.  If you wish to write a\n",
      " |  program which makes use of these analyses, then you should bypass\n",
      " |  the ``Text`` class, and use the appropriate analysis function or\n",
      " |  class directly instead.\n",
      " |  \n",
      " |  A ``Text`` is typically initialized from a given document or\n",
      " |  corpus.  E.g.:\n",
      " |  \n",
      " |  >>> import nltk.corpus\n",
      " |  >>> from nltk.text import Text\n",
      " |  >>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, i)\n",
      " |  \n",
      " |  __init__(self, tokens, name=None)\n",
      " |      Create a Text object.\n",
      " |      \n",
      " |      :param tokens: The source text.\n",
      " |      :type tokens: sequence of str\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self)\n",
      " |  \n",
      " |  collocations(self, num=20, window_size=2)\n",
      " |      Print collocations derived from the text, ignoring stopwords.\n",
      " |      \n",
      " |      :seealso: find_collocations\n",
      " |      :param num: The maximum number of collocations to print.\n",
      " |      :type num: int\n",
      " |      :param window_size: The number of tokens spanned by a collocation (default=2)\n",
      " |      :type window_size: int\n",
      " |  \n",
      " |  common_contexts(self, words, num=20)\n",
      " |      Find contexts where the specified words appear; list\n",
      " |      most frequent common contexts first.\n",
      " |      \n",
      " |      :param word: The word used to seed the similarity search\n",
      " |      :type word: str\n",
      " |      :param num: The number of words to generate (default=20)\n",
      " |      :type num: int\n",
      " |      :seealso: ContextIndex.common_contexts()\n",
      " |  \n",
      " |  concordance(self, word, width=79, lines=25)\n",
      " |      Prints a concordance for ``word`` with the specified context window.\n",
      " |      Word matching is not case-sensitive.\n",
      " |      \n",
      " |      :param word: The target word\n",
      " |      :type word: str\n",
      " |      :param width: The width of each line, in characters (default=80)\n",
      " |      :type width: int\n",
      " |      :param lines: The number of lines to display (default=25)\n",
      " |      :type lines: int\n",
      " |      \n",
      " |      :seealso: ``ConcordanceIndex``\n",
      " |  \n",
      " |  concordance_list(self, word, width=79, lines=25)\n",
      " |      Generate a concordance for ``word`` with the specified context window.\n",
      " |      Word matching is not case-sensitive.\n",
      " |      \n",
      " |      :param word: The target word\n",
      " |      :type word: str\n",
      " |      :param width: The width of each line, in characters (default=80)\n",
      " |      :type width: int\n",
      " |      :param lines: The number of lines to display (default=25)\n",
      " |      :type lines: int\n",
      " |      \n",
      " |      :seealso: ``ConcordanceIndex``\n",
      " |  \n",
      " |  count(self, word)\n",
      " |      Count the number of times this word appears in the text.\n",
      " |  \n",
      " |  dispersion_plot(self, words)\n",
      " |      Produce a plot showing the distribution of the words through the text.\n",
      " |      Requires pylab to be installed.\n",
      " |      \n",
      " |      :param words: The words to be plotted\n",
      " |      :type words: list(str)\n",
      " |      :seealso: nltk.draw.dispersion_plot()\n",
      " |  \n",
      " |  findall(self, regexp)\n",
      " |      Find instances of the regular expression in the text.\n",
      " |      The text is a list of tokens, and a regexp pattern to match\n",
      " |      a single token must be surrounded by angle brackets.  E.g.\n",
      " |      \n",
      " |      >>> print('hack'); from nltk.book import text1, text5, text9\n",
      " |      hack...\n",
      " |      >>> text5.findall(\"<.*><.*><bro>\")\n",
      " |      you rule bro; telling you bro; u twizted bro\n",
      " |      >>> text1.findall(\"<a>(<.*>)<man>\")\n",
      " |      monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      " |      mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      " |      pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      " |      brave; brave; brave\n",
      " |      >>> text9.findall(\"<th.*>{3,}\")\n",
      " |      thread through those; the thought that; that the thing; the thing\n",
      " |      that; that that thing; through these than through; them that the;\n",
      " |      through the thick; them that they; thought that the\n",
      " |      \n",
      " |      :param regexp: A regular expression\n",
      " |      :type regexp: str\n",
      " |  \n",
      " |  generate(self, words)\n",
      " |      Issues a reminder to users following the book online\n",
      " |  \n",
      " |  index(self, word)\n",
      " |      Find the index of the first occurrence of the word in the text.\n",
      " |  \n",
      " |  plot(self, *args)\n",
      " |      See documentation for FreqDist.plot()\n",
      " |      :seealso: nltk.prob.FreqDist.plot()\n",
      " |  \n",
      " |  readability(self, method)\n",
      " |  \n",
      " |  similar(self, word, num=20)\n",
      " |      Distributional similarity: find other words which appear in the\n",
      " |      same contexts as the specified word; list most similar words first.\n",
      " |      \n",
      " |      :param word: The word used to seed the similarity search\n",
      " |      :type word: str\n",
      " |      :param num: The number of words to generate (default=20)\n",
      " |      :type num: int\n",
      " |      :seealso: ContextIndex.similar_words()\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  vocab(self)\n",
      " |      :seealso: nltk.prob.FreqDist\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk_corpus) # show methods attached to the nltk.text.Text object or nltk_corpus variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at `.concordance()`. According to the official documentation this method \n",
    "> Prints a concordance for ``word`` with the specified context window. Word matching is not case-sensitive.\n",
    "\n",
    "It take multiple arguments:\n",
    "    - word: query term\n",
    "    - width: the context window, i.e. determines the number of character printed \n",
    "    - lines: determines the number of lines (i.e. KWIC examples) returns\n",
    "The first line of the output states total number of hits for the query term (`Displaying * of * matches:`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example code below print the context of the word **\"poor\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 636 matches:\n",
      "nt a renewal of a liquor licence in view of the poor conditions maintained at the establishment the \n",
      "e possible to assess the influence of a good or poor impression of training e g a poor impression ma\n",
      " of a good or poor impression of training e g a poor impression may sometimes act as a stimulus pre \n",
      "as of the programme and d the effect of good or poor experience although perhaps too early to do mor\n",
      "nt a renewal of a liquor licence in view of the poor conditions maintained at the establishment the \n",
      "e possible to assess the influence of a good or poor impression of training e g a poor impression ma\n",
      " of a good or poor impression of training e g a poor impression may sometimes act as a stimulus pre \n",
      "as of the programme and d the effect of good or poor experience although perhaps too early to do mor\n",
      "nt a renewal of a liquor licence in view of the poor conditions maintained at the establishment the \n",
      "e possible to assess the influence of a good or poor impression of training e g a poor impression ma\n"
     ]
    }
   ],
   "source": [
    "nltk_corpus.concordance('poor',width=100,lines=10) # print the context of poor, window = 100 character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --Exercise\n",
    "\n",
    "Compare \"poor\" between City of Westminster and Poplar. \n",
    "\n",
    "**[TO DO: explain exercise]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While KWIC analysis is useful for investigating the context of words, it is a method that doesn't scale well: it helps with the close reading of around 100 words, but when examples run in the thousands it becomes more difficult. Collocations can help to quantify the semantics of term, or how the meaning of words is different betwen corpora or subsamples of a corpus.\n",
    "\n",
    "Collocations, as explained in the AntConc section are multi-word expression containing words that tend to co-occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK `Text` object has `collocations()` function. Below we print and explain the documentation.\n",
    "\n",
    "> collocations(self, num=20, window_size=2)\n",
    "    Print collocations derived from the text, ignoring stopwords.\n",
    "    \n",
    "It has the following parameters:\n",
    "> `:param num:` The maximum number of collocations to print.\n",
    "\n",
    "The number of collocations to print (if not specified it will print 20)\n",
    "\n",
    "> `:param window_size:` The number of tokens spanned by a collocation (default=2)\n",
    "\n",
    "If `window_size=2` collocations will only include bigrams (words occuring next to each other). But sometimes we wish to include longer intervals, to make co-occurence of words withing a broader window more visible, this allows us to go beyond multiword expressions and study the distribution of words in a corpus more generally. For example, we could look if \"men\" and \"women\" are discussed in each other's context (within a span of 10), even if they don't appear next to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public health; city council; family planning; child health; medical\n",
      "officer; health inspectors; health department; table page; ante natal;\n",
      "local authority; dental school; social services; social workers; legal\n",
      "proceedings; social worker; live births; inner london; city hall; old\n",
      "people; boys girls\n"
     ]
    }
   ],
   "source": [
    "nltk_corpus.collocations(window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family planning; public inspectors; city council; public health; child\n",
      "health; ante natal; boys girls; dental school; medical officer; local\n",
      "authority; table page; live births; salmonella salmonella; malignant\n",
      "neoplasm; boys boys; health inspectors; girls boys; health department;\n",
      "social services; councillor mrs\n"
     ]
    }
   ],
   "source": [
    "nltk_corpus.collocations(window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the `.collocations()` method is an easy tool for quickly computing collocations, it's functionality remains rather limited. The cells below will inspect the collocation functions of NLTK in a bit more detail, giving you a bit more power of and precision.\n",
    "\n",
    "Before we start we import all the tools `nltk.collocations` provides. This is handled by the `import *`, similar to a wildcard, it matches and loads everthing in `nltk.collocations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to select an association measure this to compute the \"strength\" with which two tokens are attracted to each other. In general collocations are words that appear frequently together (within a certain window size), but are unlikely to appear in general (outside this window size). This explains why \"the wine\" is not a collocation while \"red wine\" is.\n",
    "\n",
    "NLTK provides us with different measures, which you can print and investigate in more detail. Many of the functions refer to the classic NLP Handbook of Manning and Schütze, [\"Foundations of statistical natural language processing\"](https://nlp.stanford.edu/fsnlp/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BigramAssocMeasures in module nltk.metrics.association object:\n",
      "\n",
      "class BigramAssocMeasures(NgramAssocMeasures)\n",
      " |  A collection of bigram association measures. Each association measure\n",
      " |  is provided as a function with three arguments::\n",
      " |  \n",
      " |      bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
      " |  \n",
      " |  The arguments constitute the marginals of a contingency table, counting\n",
      " |  the occurrences of particular events in a corpus. The letter i in the\n",
      " |  suffix refers to the appearance of the word in question, while x indicates\n",
      " |  the appearance of any word. Thus, for example:\n",
      " |  \n",
      " |      n_ii counts (w1, w2), i.e. the bigram being scored\n",
      " |      n_ix counts (w1, *)\n",
      " |      n_xi counts (*, w2)\n",
      " |      n_xx counts (*, *), i.e. any bigram\n",
      " |  \n",
      " |  This may be shown with respect to a contingency table::\n",
      " |  \n",
      " |              w1    ~w1\n",
      " |           ------ ------\n",
      " |       w2 | n_ii | n_oi | = n_xi\n",
      " |           ------ ------\n",
      " |      ~w2 | n_io | n_oo |\n",
      " |           ------ ------\n",
      " |           = n_ix        TOTAL = n_xx\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BigramAssocMeasures\n",
      " |      NgramAssocMeasures\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  chi_sq(n_ii, n_ix_xi_tuple, n_xx) from abc.ABCMeta\n",
      " |      Scores bigrams using chi-square, i.e. phi-sq multiplied by the number\n",
      " |      of bigrams, as in Manning and Schutze 5.3.3.\n",
      " |  \n",
      " |  fisher(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less\n",
      " |      sensitive to small counts than PMI or Chi Sq, but also more expensive\n",
      " |      to compute. Requires scipy.\n",
      " |  \n",
      " |  phi_sq(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using phi-square, the square of the Pearson correlation\n",
      " |      coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  dice(n_ii, n_ix_xi_tuple, n_xx)\n",
      " |      Scores bigrams using Dice's coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  jaccard(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Jaccard index.\n",
      " |  \n",
      " |  likelihood_ratio(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.\n",
      " |  \n",
      " |  pmi(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams by pointwise mutual information, as in Manning and\n",
      " |      Schutze 5.4.\n",
      " |  \n",
      " |  poisson_stirling(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Poisson-Stirling measure.\n",
      " |  \n",
      " |  student_t(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using Student's t test with independence hypothesis\n",
      " |      for unigrams, as in Manning and Schutze 5.3.1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  mi_like(*marginals, **kwargs)\n",
      " |      Scores ngrams using a variant of mutual information. The keyword\n",
      " |      argument power sets an exponent (default 3) for the numerator. No\n",
      " |      logarithm of the result is calculated.\n",
      " |  \n",
      " |  raw_freq(*marginals)\n",
      " |      Scores ngrams by their frequency\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bigram_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method pmi in module nltk.metrics.association:\n",
      "\n",
      "pmi(*marginals) method of abc.ABCMeta instance\n",
      "    Scores ngrams by pointwise mutual information, as in Manning and\n",
      "    Schutze 5.4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pmi` is a rather straightforward metric, in the case of bigrams\n",
    "- compute the total number of tokens in a corpus, assume this is `n` (3435)\n",
    "- compute the probability of  `a` and `b` appearing as a bigram. If the bigram (a,b) occurs 10 times, the probability (P(a,b) is 10/3435)\n",
    "- compuate the probability of observing `a` and `b`. For exampe a appears `30` times and b `45`, this becomes (30/3435) * (45/3435)\n",
    "- log this value\n",
    "![pmi](https://miro.medium.com/max/930/1*OoI8_cZQwYGJEUjzozBOCw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6692787866546315"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import log2\n",
    "nom = 10/3435\n",
    "denom = (30/3435) * (45/3435)\n",
    "mpi = log2(nom/denom)\n",
    "mpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rank collocations by their PMI scores, we use the `.from_words()` method to the `nltk_corpus` (or any list of tokens). The result of this operation is stored in `finder` which we can subsequently use for printing collocations. Note that the results below look somewhat strange, these aren't very meaningful collocates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aeration', 'cycle'),\n",
       " ('albicans', 'vincents'),\n",
       " ('altogether', 'buckingham'),\n",
       " ('amplified', 'music'),\n",
       " ('anyone', 'demonstrating'),\n",
       " ('appendicitis', 'intestinal'),\n",
       " ('appreciable', 'drop'),\n",
       " ('arranges', 'placement'),\n",
       " ('artesia', 'adreno'),\n",
       " ('ashpits', 'dustbins')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(nltk_corpus)\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are rather spurious. If, for example `a` and `b` both appear only once and next to each other, the PMI score will be very high, but this is not necessarily a very meaningful collocation, more a rare artefact of the data.\n",
    "\n",
    "We filter by ngram frequency, removing in our case all bigrams that appear less than 3 time with `.apply_freq_filter()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method apply_freq_filter in module nltk.collocations:\n",
      "\n",
      "apply_freq_filter(min_freq) method of nltk.collocations.BigramCollocationFinder instance\n",
      "    Removes candidate ngrams which have frequency less than min_freq.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(finder.apply_freq_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aeration', 'cycle'),\n",
       " ('albicans', 'vincents'),\n",
       " ('altogether', 'buckingham'),\n",
       " ('amplified', 'music'),\n",
       " ('anyone', 'demonstrating'),\n",
       " ('appendicitis', 'intestinal'),\n",
       " ('appreciable', 'drop'),\n",
       " ('arranges', 'placement'),\n",
       " ('artesia', 'adreno'),\n",
       " ('ashpits', 'dustbins')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.apply_freq_filter(3)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aeration', 'cycle'),\n",
       " ('albicans', 'vincents'),\n",
       " ('altogether', 'buckingham'),\n",
       " ('amplified', 'music'),\n",
       " ('anyone', 'demonstrating'),\n",
       " ('appendicitis', 'intestinal'),\n",
       " ('appreciable', 'drop'),\n",
       " ('arranges', 'placement'),\n",
       " ('artesia', 'adreno'),\n",
       " ('ashpits', 'dustbins')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to change the window size, but the larger the window size the longer the computation takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abs', 'hrs'),\n",
       " ('abuse', 'passers'),\n",
       " ('accent', 'preservation'),\n",
       " ('accum', 'buckeburg'),\n",
       " ('accumulation', 'salts'),\n",
       " ('acutely', 'incontinent'),\n",
       " ('adapt', 'outmoded'),\n",
       " ('adapt', 'wooden'),\n",
       " ('adheres', 'brick'),\n",
       " ('adjusting', 'reality')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(nltk_corpus, window_size = 5)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly you can focus on collocations that contains a specific token, i.e. for example get all collocations with the token \"poor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('poor', 'impression'),\n",
       " ('poor', 'experience'),\n",
       " ('poor', 'conditions'),\n",
       " ('or', 'poor'),\n",
       " ('a', 'poor'),\n",
       " ('the', 'poor')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def token_filter(*w):\n",
    "#     return 'poor' not in w\n",
    "\n",
    "token_filter = lambda *w: 'poor' not in w\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(nltk_corpus)\n",
    "finder.apply_ngram_filter(token_filter)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last section of this Notebook takes aims at contrasting corpora and find tokens (or word patterns) that distinguish on set of documents from another. This may help us discovering that is particular about the language of specific group (such as a political party) or period. We continue with the example of the MOsH reports, but compare the language of different boroughs, the affluent Westminster with the industrial, and considerable poorer Poplar.\n",
    "\n",
    "The code below should look familiar but we made a few changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57850c0d978b4afaaf2218187aac45d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = [] # save corpus here\n",
    "labels = [] # save labels here\n",
    "\n",
    "\n",
    "for r in moh_reports: # iterate over documents\n",
    "    with open(r) as in_doc: # open document (also take care close it later)\n",
    "        if 'westminster' in r.name.lower(): # check if westeminster appear in the file name\n",
    "            labels.append(1) # if so, append 1 to labels\n",
    "        else: # if not\n",
    "            labels.append(0) # append 0 to labels\n",
    "\n",
    "        corpus.append(in_doc.read().lower()) # append the lowercase document to corpus\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check number of labels and documents are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 159\n"
     ]
    }
   ],
   "source": [
    "print(len(labels),len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process text: lemmatize keep only adj and noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install external library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TextFeatureSelection\n",
      "  Downloading https://files.pythonhosted.org/packages/42/3d/351dcabf4198218a4b7421e6f6069eb089af6f5642e8fdd5d95f11904726/TextFeatureSelection-0.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (from TextFeatureSelection) (0.22.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from TextFeatureSelection) (0.25.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from TextFeatureSelection) (1.17.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn->TextFeatureSelection) (0.13.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn->TextFeatureSelection) (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas->TextFeatureSelection) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->TextFeatureSelection) (2018.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->TextFeatureSelection) (1.11.0)\n",
      "Installing collected packages: TextFeatureSelection\n",
      "Successfully installed TextFeatureSelection-0.0.12\n"
     ]
    }
   ],
   "source": [
    "!pip install TextFeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word list</th>\n",
       "      <th>word occurence count</th>\n",
       "      <th>Proportional Difference</th>\n",
       "      <th>Mutual Information</th>\n",
       "      <th>Chi Square</th>\n",
       "      <th>Information Gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>103</td>\n",
       "      <td>-0.009709</td>\n",
       "      <td>0.094959</td>\n",
       "      <td>2.463282</td>\n",
       "      <td>0.004326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>000</td>\n",
       "      <td>149</td>\n",
       "      <td>0.073826</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>0.150191</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>1.185538</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0001</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>2.595483</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>000163</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.854210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42232</td>\n",
       "      <td>¾gallons</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>1.185538</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42233</td>\n",
       "      <td>¾ths</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>1.185538</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42234</td>\n",
       "      <td>ægis</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.854210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42235</td>\n",
       "      <td>æration</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>1.185538</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42236</td>\n",
       "      <td>œsophagus</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>1.185538</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42237 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word list  word occurence count  Proportional Difference  \\\n",
       "0             00                   103                -0.009709   \n",
       "1            000                   149                 0.073826   \n",
       "2         000000                     1                -1.000000   \n",
       "3           0001                     3                 1.000000   \n",
       "4         000163                     1                 1.000000   \n",
       "...          ...                   ...                      ...   \n",
       "42232   ¾gallons                     1                -1.000000   \n",
       "42233       ¾ths                     1                -1.000000   \n",
       "42234       ægis                     1                 1.000000   \n",
       "42235    æration                     1                -1.000000   \n",
       "42236  œsophagus                     1                -1.000000   \n",
       "\n",
       "       Mutual Information  Chi Square  Information Gain  \n",
       "0                0.094959    2.463282          0.004326  \n",
       "1                0.008605    0.150191          0.000266  \n",
       "2                0.778445    1.185538          0.001507  \n",
       "3                    -inf    2.595483          0.000000  \n",
       "4                    -inf    0.854210          0.000000  \n",
       "...                   ...         ...               ...  \n",
       "42232            0.778445    1.185538          0.001507  \n",
       "42233            0.778445    1.185538          0.001507  \n",
       "42234                -inf    0.854210          0.000000  \n",
       "42235            0.778445    1.185538          0.001507  \n",
       "42236            0.778445    1.185538          0.001507  \n",
       "\n",
       "[42237 rows x 6 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TextFeatureSelection import TextFeatureSelection\n",
    "fsOBJ=TextFeatureSelection(target=labels,input_doc_list=corpus)\n",
    "result_df=fsOBJ.getScore()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word list</th>\n",
       "      <th>word occurence count</th>\n",
       "      <th>Proportional Difference</th>\n",
       "      <th>Mutual Information</th>\n",
       "      <th>Chi Square</th>\n",
       "      <th>Information Gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30606</td>\n",
       "      <td>pop</td>\n",
       "      <td>59</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>110.515890</td>\n",
       "      <td>0.184282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9432</td>\n",
       "      <td>bow</td>\n",
       "      <td>89</td>\n",
       "      <td>-0.640449</td>\n",
       "      <td>0.580268</td>\n",
       "      <td>106.152339</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21070</td>\n",
       "      <td>horseferry</td>\n",
       "      <td>71</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>-3.484235</td>\n",
       "      <td>102.313762</td>\n",
       "      <td>0.239720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8788</td>\n",
       "      <td>bessborough</td>\n",
       "      <td>67</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>98.289813</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42219</td>\n",
       "      <td>zymotic</td>\n",
       "      <td>94</td>\n",
       "      <td>-0.553191</td>\n",
       "      <td>0.525609</td>\n",
       "      <td>93.326942</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26433</td>\n",
       "      <td>millbank</td>\n",
       "      <td>62</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>86.266363</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15282</td>\n",
       "      <td>dock</td>\n",
       "      <td>66</td>\n",
       "      <td>-0.787879</td>\n",
       "      <td>0.666327</td>\n",
       "      <td>85.911216</td>\n",
       "      <td>0.149069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41176</td>\n",
       "      <td>wes</td>\n",
       "      <td>64</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>-3.380438</td>\n",
       "      <td>84.840438</td>\n",
       "      <td>0.205713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22037</td>\n",
       "      <td>india</td>\n",
       "      <td>67</td>\n",
       "      <td>-0.761194</td>\n",
       "      <td>0.651290</td>\n",
       "      <td>82.833472</td>\n",
       "      <td>0.144441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30188</td>\n",
       "      <td>pimlico</td>\n",
       "      <td>63</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>-3.364690</td>\n",
       "      <td>82.552451</td>\n",
       "      <td>0.201071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18111</td>\n",
       "      <td>fines</td>\n",
       "      <td>91</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>-1.093357</td>\n",
       "      <td>79.850990</td>\n",
       "      <td>0.139906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31245</td>\n",
       "      <td>procured</td>\n",
       "      <td>70</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>0.624294</td>\n",
       "      <td>79.780218</td>\n",
       "      <td>0.141947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14529</td>\n",
       "      <td>devons</td>\n",
       "      <td>47</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>78.605431</td>\n",
       "      <td>0.118591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26460</td>\n",
       "      <td>millwall</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.959184</td>\n",
       "      <td>0.757825</td>\n",
       "      <td>77.262501</td>\n",
       "      <td>0.118218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33936</td>\n",
       "      <td>ruston</td>\n",
       "      <td>46</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>76.252152</td>\n",
       "      <td>0.114306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17897</td>\n",
       "      <td>ferry</td>\n",
       "      <td>68</td>\n",
       "      <td>-0.705882</td>\n",
       "      <td>0.619380</td>\n",
       "      <td>74.205624</td>\n",
       "      <td>0.129299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24601</td>\n",
       "      <td>limehouse</td>\n",
       "      <td>45</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>73.940159</td>\n",
       "      <td>0.110155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11484</td>\n",
       "      <td>cleansings</td>\n",
       "      <td>45</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>73.940159</td>\n",
       "      <td>0.110155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7989</td>\n",
       "      <td>await</td>\n",
       "      <td>64</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>-2.281826</td>\n",
       "      <td>73.305433</td>\n",
       "      <td>0.165213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34547</td>\n",
       "      <td>seamen</td>\n",
       "      <td>65</td>\n",
       "      <td>-0.723077</td>\n",
       "      <td>0.629409</td>\n",
       "      <td>71.698892</td>\n",
       "      <td>0.122070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word list  word occurence count  Proportional Difference  \\\n",
       "30606          pop                    59                -1.000000   \n",
       "9432           bow                    89                -0.640449   \n",
       "21070   horseferry                    71                 0.971831   \n",
       "8788   bessborough                    67                 1.000000   \n",
       "42219      zymotic                    94                -0.553191   \n",
       "26433     millbank                    62                 1.000000   \n",
       "15282         dock                    66                -0.787879   \n",
       "41176          wes                    64                 0.968750   \n",
       "22037        india                    67                -0.761194   \n",
       "30188      pimlico                    63                 0.968254   \n",
       "18111        fines                    91                 0.692308   \n",
       "31245     procured                    70                -0.714286   \n",
       "14529       devons                    47                -1.000000   \n",
       "26460     millwall                    49                -0.959184   \n",
       "33936       ruston                    46                -1.000000   \n",
       "17897        ferry                    68                -0.705882   \n",
       "24601    limehouse                    45                -1.000000   \n",
       "11484   cleansings                    45                -1.000000   \n",
       "7989         await                    64                 0.906250   \n",
       "34547       seamen                    65                -0.723077   \n",
       "\n",
       "       Mutual Information  Chi Square  Information Gain  \n",
       "30606            0.778445  110.515890          0.184282  \n",
       "9432             0.580268  106.152339          0.000000  \n",
       "21070           -3.484235  102.313762          0.239720  \n",
       "8788                 -inf   98.289813          0.000000  \n",
       "42219            0.525609   93.326942          0.000000  \n",
       "26433                -inf   86.266363          0.000000  \n",
       "15282            0.666327   85.911216          0.149069  \n",
       "41176           -3.380438   84.840438          0.205713  \n",
       "22037            0.651290   82.833472          0.144441  \n",
       "30188           -3.364690   82.552451          0.201071  \n",
       "18111           -1.093357   79.850990          0.139906  \n",
       "31245            0.624294   79.780218          0.141947  \n",
       "14529            0.778445   78.605431          0.118591  \n",
       "26460            0.757825   77.262501          0.118218  \n",
       "33936            0.778445   76.252152          0.114306  \n",
       "17897            0.619380   74.205624          0.129299  \n",
       "24601            0.778445   73.940159          0.110155  \n",
       "11484            0.778445   73.940159          0.110155  \n",
       "7989            -2.281826   73.305433          0.165213  \n",
       "34547            0.629409   71.698892          0.122070  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[result_df['word occurence count'] > 5].sort_values('Chi Square',ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sort_values in module pandas.core.frame:\n",
      "\n",
      "sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last') method of pandas.core.frame.DataFrame instance\n",
      "    Sort by the values along either axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "            by : str or list of str\n",
      "                Name or list of names to sort by.\n",
      "    \n",
      "                - if `axis` is 0 or `'index'` then `by` may contain index\n",
      "                  levels and/or column labels\n",
      "                - if `axis` is 1 or `'columns'` then `by` may contain column\n",
      "                  levels and/or index labels\n",
      "    \n",
      "                .. versionchanged:: 0.23.0\n",
      "                   Allow specifying index or column level names.\n",
      "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "         Axis to be sorted.\n",
      "    ascending : bool or list of bool, default True\n",
      "         Sort ascending vs. descending. Specify list for multiple sort\n",
      "         orders.  If this is a list of bools, must match the length of\n",
      "         the by.\n",
      "    inplace : bool, default False\n",
      "         If True, perform operation in-place.\n",
      "    kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n",
      "         Choice of sorting algorithm. See also ndarray.np.sort for more\n",
      "         information.  `mergesort` is the only stable algorithm. For\n",
      "         DataFrames, this option is only applied when sorting on a single\n",
      "         column or label.\n",
      "    na_position : {'first', 'last'}, default 'last'\n",
      "         Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n",
      "         end.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    sorted_obj : DataFrame or None\n",
      "        DataFrame with sorted values if inplace=False, None otherwise.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({\n",
      "    ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n",
      "    ...     'col2': [2, 1, 9, 8, 7, 4],\n",
      "    ...     'col3': [0, 1, 9, 4, 2, 3],\n",
      "    ... })\n",
      "    >>> df\n",
      "        col1 col2 col3\n",
      "    0   A    2    0\n",
      "    1   A    1    1\n",
      "    2   B    9    9\n",
      "    3   NaN  8    4\n",
      "    4   D    7    2\n",
      "    5   C    4    3\n",
      "    \n",
      "    Sort by col1\n",
      "    \n",
      "    >>> df.sort_values(by=['col1'])\n",
      "        col1 col2 col3\n",
      "    0   A    2    0\n",
      "    1   A    1    1\n",
      "    2   B    9    9\n",
      "    5   C    4    3\n",
      "    4   D    7    2\n",
      "    3   NaN  8    4\n",
      "    \n",
      "    Sort by multiple columns\n",
      "    \n",
      "    >>> df.sort_values(by=['col1', 'col2'])\n",
      "        col1 col2 col3\n",
      "    1   A    1    1\n",
      "    0   A    2    0\n",
      "    2   B    9    9\n",
      "    5   C    4    3\n",
      "    4   D    7    2\n",
      "    3   NaN  8    4\n",
      "    \n",
      "    Sort Descending\n",
      "    \n",
      "    >>> df.sort_values(by='col1', ascending=False)\n",
      "        col1 col2 col3\n",
      "    4   D    7    2\n",
      "    5   C    4    3\n",
      "    2   B    9    9\n",
      "    0   A    2    0\n",
      "    1   A    1    1\n",
      "    3   NaN  8    4\n",
      "    \n",
      "    Putting NAs first\n",
      "    \n",
      "    >>> df.sort_values(by='col1', ascending=False, na_position='first')\n",
      "        col1 col2 col3\n",
      "    3   NaN  8    4\n",
      "    4   D    7    2\n",
      "    5   C    4    3\n",
      "    2   B    9    9\n",
      "    0   A    2    0\n",
      "    1   A    1    1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(result_df.sort_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix With Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2 = SelectKBest(chi2, k=10)\n",
    "X = ch2.fit_transform(X, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('borough', 6827.175533272762),\n",
       " ('bow', 6681.346216439548),\n",
       " ('bromley', 6861.134136366376),\n",
       " ('city', 4592.729181914567),\n",
       " ('east', 1499.0376786761663),\n",
       " ('poplar', 11888.857471790638),\n",
       " ('road', 8510.875738951223),\n",
       " ('see', 2314.6724275246893),\n",
       " ('street', 4330.436649540313),\n",
       " ('westminster', 5105.364636488248)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = [(feature_names[i],ch2.scores_[i])for i\n",
    "                    in ch2.get_support(indices=True)]\n",
    "selected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
