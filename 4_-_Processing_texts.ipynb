{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/kasparvonbeelen/ghi_python/main?labpath=4_-_Processing_texts.ipynb)\n",
    "\n",
    "\n",
    "# 4 Processing texts\n",
    "\n",
    "\n",
    "## Text Mining for Historians (with Python)\n",
    "## A Gentle Introduction to Working with Textual Data in Python\n",
    "\n",
    "### Created by Kaspar Beelen and Luke Blaxill\n",
    "\n",
    "### For the German Historical Institute, London\n",
    "\n",
    "<img align=\"left\" src=\"https://www.ghil.ac.uk/typo3conf/ext/wacon_ghil/Resources/Public/Images/institute_icon_small.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you learn to process and extract information from texts. We continue with the sonnet, but as promised, scale up soon. \n",
    "\n",
    "This notebook focuses on extracting basic information from texts. We show how to use external libraries for more refined enrichment (finding named entities or zoom in on specific word categories (nouns, verbs)).\n",
    "\n",
    "At the end of this Notebook you'll be able to\n",
    "- Tokenize texts\n",
    "- Apply functions from Natural Language Toolkit and SpaCy for more advance processing (part-of-speech tagging)\n",
    "- Count the frequency of tokens\n",
    "- Have some understanding of Python `list` and `dict` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have basic understanding of how to read and manipulate textual data in Python. Now we can turn to more directly useful and realistic applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou, contracted to thine own bright eyes,\n",
      "Feed'st thy light's flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thyself thy foe, to thy sweet self too cruel:\n",
      "Thou that art now the world's fresh ornament,\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content,\n",
      "And tender churl mak'st waste in niggarding:\n",
      "Pity the world, or else this glutton be,\n",
      "To eat the world's due, by the grave and thee.\n"
     ]
    }
   ],
   "source": [
    "path = \"example_data/notebook_3/shakespeare_sonnet_i.txt\" # declare path/ variable assignment\n",
    "sonnet = open(path,'r').read() # open and read document\n",
    "print(sonnet) # print document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While to a human reader—looking at the document very formally—the sonnet contains multiple words and lines (such a surprise) this isn't obvious to a computer ingesting the document. At this stage, Python represents the sonnet as a sequence of characters; it has no understanding of word boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are words anyway?\n",
    "Generally, we will make a distinction between `types` and `tokens` following the definition of [Smith, N.A., 2019](https://arxiv.org/pdf/1902.06006.pdf). \n",
    "- \"A word **token** is a word observed in a piece of text.\" \n",
    "- \"A word **type** is a distinct word, in the abstract, rather than a specific instance. Every word token is said to “belong” to its type.\"\n",
    "\n",
    " Example:\n",
    " > The sentence \"two teas and two coffees\" contains 5 tokens and 4 types (two appears twice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the introduction, textual data is unstructured. We have to manipulate and transform the sequence of characters in order to work with the content in meaningful ways.\n",
    "\n",
    "Let's start with detecting word boundaries, and covert the string of characters to list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you notice `sonnet[0]` doesn't return the first word but the first character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seemingly straightforward way to transform a string into tokens is by splitting the text by white spaces. In this scenario, we perceive white spaces as boundaries between tokens. Luckily Python provides us with a tool to do just that. The `str.split()` method will use the white spaces to split a string into a list of tokens. Run the code below, and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'fairest', 'creatures', 'we', 'desire', 'increase,', 'That', 'thereby', \"beauty's\", 'rose', 'might', 'never', 'die,', 'But', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease,', 'His', 'tender', 'heir', 'might', 'bear', 'his', 'memory:', 'But', 'thou,', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes,', \"Feed'st\", 'thy', \"light's\", 'flame', 'with', 'self-substantial', 'fuel,', 'Making', 'a', 'famine', 'where', 'abundance', 'lies,', 'Thyself', 'thy', 'foe,', 'to', 'thy', 'sweet', 'self', 'too', 'cruel:', 'Thou', 'that', 'art', 'now', 'the', \"world's\", 'fresh', 'ornament,', 'And', 'only', 'herald', 'to', 'the', 'gaudy', 'spring,', 'Within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content,', 'And', 'tender', 'churl', \"mak'st\", 'waste', 'in', 'niggarding:', 'Pity', 'the', 'world,', 'or', 'else', 'this', 'glutton', 'be,', 'To', 'eat', 'the', \"world's\", 'due,', 'by', 'the', 'grave', 'and', 'thee.']\n"
     ]
    }
   ],
   "source": [
    "tokens = sonnet.split() # split by white space save resulting list in tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `str.split()` looks different to what we have encountered before:\n",
    "- The output is enclosed by square brackets `[]`\n",
    "- The quotation marks are now (approximately) around the individual words and not the whole string\n",
    "- Words are separated by commas\n",
    "\n",
    "What happened here is the following: split takes a string and returns a `list` (of tokens). A `list` is another Python data type, which we will be using a lot in the remainder of this course. The `break out` provides more information, but we discuss the most important aspect here as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Python list \"is an ordered collection of values\" ([Wentworth, et al. 2012](https://openbookproject.net/thinkcs/python/english3e/lists.html)). It is a container that keeps several elements (also called) items in a particular order. Documents are often presented as a list, i.e. as a sequence of tokens in a specific order. \n",
    "\n",
    "Each element in the list implicitly indexed by place, i.e. you can retrieve items by their position (for example, the first and last word of the sonnet with `[0]` and `[-1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thee.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`len()` counts the number of items in a list (notice how this is different from the number of characters in a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Breakout`\n",
    "- [lists in Python](break_out/lists.ipynb)\n",
    "\n",
    "Read more on [`' '.join()`](https://www.w3schools.com/python/ref_string_join.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We called the variable in which we saved the split string  `tokens`, but upon closer inspection, you notice that some elements aren't technically tokens. Some include, for example, some punctuation marks. If we look at items at position 5, 8 or 41, the difficulty of converting a string to a list of tokens becomes apparent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('increase,', \"beauty's\", 'self-substantial')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[5],tokens[8],tokens[41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `'increase,'` is a token followed by a punctuation mark, `\"self-substantial\"` is more complex. It depends on how you interpret and process such compounds (read it as one word, or split it into two, `\"self\"` and `\"substantial\"`, but are hyphens always token boundaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, you don't have to worry too much about the subtleties (unless you want to!) because Python comes with many convenient external libraries that provide you plethora of tools (in the form of function) that help you with complex processing tasks.\n",
    "\n",
    "Here we take a closer look at a popular tool called the \"Natural Language Toolkit\" (abbreviated as NLTK). (Later, we discuss a few other options.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is a Python library for natural language processing, it was built to make processing text data (such as tokenization) easier. You don't have to write your functions all the time. \n",
    "\n",
    "The syntax below is unfamiliar and the `break out` points to a more elaborate explanation. The line of code imports a tool (in this case a function with the name `word_tokenize`) into our Notebook. This function is stored in `nltk.tokenize`. \n",
    "\n",
    "You don't have to import this function every time you use it, only once when running the notebook suffices (unless you restart the Kernel or Runtime, because then everything gets deleted from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kbeelen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing `word_tokenize`, we can apply it to our sonnet and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'fairest', 'creatures', 'we', 'desire', 'increase', ',', 'That', 'thereby', 'beauty', \"'s\", 'rose', 'might', 'never', 'die', ',', 'But', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', ',', 'His', 'tender', 'heir', 'might', 'bear', 'his', 'memory', ':', 'But', 'thou', ',', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', ',', \"Feed'st\", 'thy', 'light', \"'s\", 'flame', 'with', 'self-substantial', 'fuel', ',', 'Making', 'a', 'famine', 'where', 'abundance', 'lies', ',', 'Thyself', 'thy', 'foe', ',', 'to', 'thy', 'sweet', 'self', 'too', 'cruel', ':', 'Thou', 'that', 'art', 'now', 'the', 'world', \"'s\", 'fresh', 'ornament', ',', 'And', 'only', 'herald', 'to', 'the', 'gaudy', 'spring', ',', 'Within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content', ',', 'And', 'tender', 'churl', \"mak'st\", 'waste', 'in', 'niggarding', ':', 'Pity', 'the', 'world', ',', 'or', 'else', 'this', 'glutton', 'be', ',', 'To', 'eat', 'the', 'world', \"'s\", 'due', ',', 'by', 'the', 'grave', 'and', 'thee', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_nltk = word_tokenize(sonnet)\n",
    "print(tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Exercise: \n",
    "\n",
    "The previous example returns a different number of tokens. Inspect the difference between splitting by white spaces and using NLTK's `word_tokenize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to lowercasing, tokenization is an essential step in the text processing pipeline. We can investigate the sonnet in more detail, for example, by counting words. The easiest way of doing this is using a `Counter()` object (which we also have to import). Counter maps the types in list to their frequency.\n",
    "\n",
    "At this point, we have all skills to put together a small word counting program.\n",
    "- import the required libraries (line 1 and 2)\n",
    "- create a variable with the name path that points to the location of the sonnet\n",
    "- read the document\n",
    "- lowercase the document\n",
    "- tokenize the document\n",
    "- count words\n",
    "- print word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({',': 14, 'the': 6, \"'s\": 4, 'to': 4, 'thy': 4, ':': 3, 'world': 3, 'and': 3, 'that': 2, 'might': 2, 'but': 2, 'by': 2, 'his': 2, 'tender': 2, 'thou': 2, 'thine': 2, 'own': 2, 'from': 1, 'fairest': 1, 'creatures': 1, 'we': 1, 'desire': 1, 'increase': 1, 'thereby': 1, 'beauty': 1, 'rose': 1, 'never': 1, 'die': 1, 'as': 1, 'riper': 1, 'should': 1, 'time': 1, 'decease': 1, 'heir': 1, 'bear': 1, 'memory': 1, 'contracted': 1, 'bright': 1, 'eyes': 1, \"feed'st\": 1, 'light': 1, 'flame': 1, 'with': 1, 'self-substantial': 1, 'fuel': 1, 'making': 1, 'a': 1, 'famine': 1, 'where': 1, 'abundance': 1, 'lies': 1, 'thyself': 1, 'foe': 1, 'sweet': 1, 'self': 1, 'too': 1, 'cruel': 1, 'art': 1, 'now': 1, 'fresh': 1, 'ornament': 1, 'only': 1, 'herald': 1, 'gaudy': 1, 'spring': 1, 'within': 1, 'bud': 1, 'buriest': 1, 'content': 1, 'churl': 1, \"mak'st\": 1, 'waste': 1, 'in': 1, 'niggarding': 1, 'pity': 1, 'or': 1, 'else': 1, 'this': 1, 'glutton': 1, 'be': 1, 'eat': 1, 'due': 1, 'grave': 1, 'thee': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter # import Counter\n",
    "from nltk.tokenize import word_tokenize # import tool for tokenization\n",
    "path = \"example_data/notebook_3/shakespeare_sonnet_i.txt\" # tell python where the document is stored\n",
    "sonnet = open(path,'r').read() # open and read the document\n",
    "sonnet_lowercase = sonnet.lower() # lowercase and store result in sonnet_lower\n",
    "tokens = word_tokenize(sonnet_lowercase) # tokenize and store the list in \n",
    "word_counts = Counter(tokens) # map types to the frequency\n",
    "print(word_counts) # print word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip many of the subtleties and technicalities here, but what is important to understand is that a `Counter()` maps word types (that occur in a document) to their frequencies. \n",
    "\n",
    "Mapping of keys (word types) to values (counts) is handled by a data type called **dictionaries** (`Counter` is a variant on a dictionary, with a few more useful methods, but as far as we are concerned, they are largely identical).\n",
    "\n",
    "In Python, we could create a  simple translation dictionary, which looks like:\n",
    "\n",
    "```python\n",
    "{'one':'einz',\n",
    " 'two':'zwei'}\n",
    "```\n",
    "\n",
    "Note the **curly brackets**, indicating a different data type (lists have square brackets, strings use quotation marks). \n",
    "Words at the left of the colon are called **keys**, those at the right are **values**, each key-value pair is called an **item**.\n",
    "\n",
    "You can assign a dictionary to a variable and then **lookup** the value for a specific key, as shown in the example below. Please note that we are using here square brackets again (similar to how indexing looks up the item at a specific position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "einz\n"
     ]
    }
   ],
   "source": [
    "english2german = {'one':'einz', 'two':'zwei'} # create and english to german dictionary\n",
    "print(english2german['one']) # print the translation of 'one'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please consult the link in the breakout for more information about dictionaries.\n",
    "\n",
    "## `Breakout`\n",
    "- [dictionaries in Python](break_out/dictionaries.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our example. As said earlier, `Counter()` objects are similar to dictionaries: you can look up the frequency of a word by entering a key and print the associated value. We can print the frequency of \"and\" in our sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(word_counts['and'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the word doesn't appear in the text, it returns `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Counter()` has more useful methods that make life easier: the `.most_common` method prints the `n` most common words. For example, we can print the ten most frequent words in our sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 14),\n",
       " ('the', 6),\n",
       " (\"'s\", 4),\n",
       " ('to', 4),\n",
       " ('thy', 4),\n",
       " (':', 3),\n",
       " ('world', 3),\n",
       " ('and', 3),\n",
       " ('that', 2),\n",
       " ('might', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Text Processing with SpaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While NLTK is a convenient tool and often used in DH, a few other libraries have emerged and are slowly pushing the state-of-the-art in the field. We will have a closer look at SpaCy, a powerful (and fast!) tool for automatic language analysis. Similar to NLTK we have to import the library at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # import the SpaCy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SpaCy we first have to load a model. This model is trained on a specific language and handles many different tasks: tokenization, lemmatization and more. In this sense, SpaCy works somewhat different than NLTK: with SpaCy we apply many different types of linguistic analysis and enrichment at once. Whereas in NLTK you would invoke a separate function. \n",
    "\n",
    "The code below makes this distinction more clear. We load the model and save it in the variable `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # Load English model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, an example text is assigned to `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"A trifling incident thus served to settle a victory.  \n",
    "            Now-a days, a soldier is so much of a machine that he seems simply to go through certain evolutions, in which there is no opportunity for the display of personal bravery or cowardice.  \n",
    "            He does not know what is going on in other parts of the field, and has no real knowledge, till all be over, whether the day has been lost or won.”\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply call `nlp` passing `paragraph` as an argument. The returns an instance of the class `spacy.tokens.doc.Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(paragraph)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to lists, we can retrieve individual elements from `doc` using the index notation.\n",
    "Let's have a closer look at the third element, the word incident in `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incident\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(doc[2])\n",
    "print(type(doc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `help()` function reveals the attributes and methods attached to individual tokens. \n",
    "\n",
    "An attribute is a value that belongs to an object. In the example below, each token has a boolean `is_digit` attribute that is `True` if the token consists of digits.  \n",
    "\n",
    "The general syntax for attributes is: \n",
    "`object.attribute`\n",
    "\n",
    "This looks similar to the dot notation for methods.\n",
    "`object.method()`\n",
    "\n",
    "The main syntactic difference is the use of parentheses. \n",
    "\n",
    "Whereas methods perform some operation on an object (and often return a value) an attribute is just a value attached to an object.\n",
    "\n",
    "Apologies if this sounds confusing! After working through some examples you will quickly become familiar with attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `help` to \"incident\" token, and scroll down to the line **\"Data descriptors defined here:\"**\n",
    "Here you find a list of attributes attached to the SpaCy `Token` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Token object:\n",
      "\n",
      "class Token(builtins.object)\n",
      " |  An individual token – i.e. a word, punctuation symbol, whitespace,\n",
      " |  etc.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/token\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bytes__(...)\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      The number of unicode characters in the token, i.e. `token.text`.\n",
      " |      \n",
      " |      RETURNS (int): The number of unicode characters in the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#len\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |  \n",
      " |  check_flag(...)\n",
      " |      Check the value of a boolean flag.\n",
      " |      \n",
      " |      flag_id (int): The ID of the flag attribute.\n",
      " |      RETURNS (bool): Whether the flag is set.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#check_flag\n",
      " |  \n",
      " |  is_ancestor(...)\n",
      " |      Check whether this token is a parent, grandparent, etc. of another\n",
      " |      in the dependency tree.\n",
      " |      \n",
      " |      descendant (Token): Another token.\n",
      " |      RETURNS (bool): Whether this token is the ancestor of the descendant.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#is_ancestor\n",
      " |  \n",
      " |  nbor(...)\n",
      " |      Get a neighboring token.\n",
      " |      \n",
      " |      i (int): The relative position of the token to get. Defaults to 1.\n",
      " |      RETURNS (Token): The token at position `self.doc[self.i+i]`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#nbor\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |      similarity using an average of word vectors.\n",
      " |      \n",
      " |      other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |          `Span`, `Token` and `Lexeme` objects.\n",
      " |      RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#similarity\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_extension(...) from builtins.type\n",
      " |      Look up a previously registered extension by name.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (tuple): A `(default, method, getter, setter)` tuple.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#get_extension\n",
      " |  \n",
      " |  has_extension(...) from builtins.type\n",
      " |      Check whether an extension has been registered.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (bool): Whether the extension has been registered.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#has_extension\n",
      " |  \n",
      " |  remove_extension(...) from builtins.type\n",
      " |      Remove a previously registered extension.\n",
      " |      \n",
      " |      name (unicode): Name of the extension.\n",
      " |      RETURNS (tuple): A `(default, method, getter, setter)` tuple of the\n",
      " |          removed extension.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#remove_extension\n",
      " |  \n",
      " |  set_extension(...) from builtins.type\n",
      " |      Define a custom attribute which becomes available as `Token._`.\n",
      " |      \n",
      " |      name (unicode): Name of the attribute to set.\n",
      " |      default: Optional default value of the attribute.\n",
      " |      getter (callable): Optional getter function.\n",
      " |      setter (callable): Optional setter function.\n",
      " |      method (callable): Optional method for method extension.\n",
      " |      force (bool): Force overwriting existing attribute.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#set_extension\n",
      " |      USAGE: https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  ancestors\n",
      " |      A sequence of this token's syntactic ancestors.\n",
      " |      \n",
      " |      YIELDS (Token): A sequence of ancestor tokens such that\n",
      " |          `ancestor.is_ancestor(self)`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#ancestors\n",
      " |  \n",
      " |  children\n",
      " |      A sequence of the token's immediate syntactic children.\n",
      " |      \n",
      " |      YIELDS (Token): A child token such that `child.head==self`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#children\n",
      " |  \n",
      " |  cluster\n",
      " |      RETURNS (int): Brown cluster ID.\n",
      " |  \n",
      " |  conjuncts\n",
      " |      A sequence of coordinated tokens, including the token itself.\n",
      " |      \n",
      " |      RETURNS (tuple): The coordinated tokens.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#conjuncts\n",
      " |  \n",
      " |  dep\n",
      " |      RETURNS (uint64): ID of syntactic dependency label.\n",
      " |  \n",
      " |  dep_\n",
      " |      RETURNS (unicode): The syntactic dependency label.\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  ent_id\n",
      " |      RETURNS (uint64): ID of the entity the token is an instance of,\n",
      " |      if any.\n",
      " |  \n",
      " |  ent_id_\n",
      " |      RETURNS (unicode): ID of the entity the token is an instance of,\n",
      " |      if any.\n",
      " |  \n",
      " |  ent_iob\n",
      " |      IOB code of named entity tag. `1=\"I\", 2=\"O\", 3=\"B\"`. 0 means no tag\n",
      " |      is assigned.\n",
      " |      \n",
      " |      RETURNS (uint64): IOB code of named entity tag.\n",
      " |  \n",
      " |  ent_iob_\n",
      " |      IOB code of named entity tag. \"B\" means the token begins an entity,\n",
      " |      \"I\" means it is inside an entity, \"O\" means it is outside an entity,\n",
      " |      and \"\" means no entity tag is set. \"B\" with an empty ent_type\n",
      " |      means that the token is blocked from further processing by NER.\n",
      " |      \n",
      " |      RETURNS (unicode): IOB code of named entity tag.\n",
      " |  \n",
      " |  ent_kb_id\n",
      " |      RETURNS (uint64): Named entity KB ID.\n",
      " |  \n",
      " |  ent_kb_id_\n",
      " |      RETURNS (unicode): Named entity KB ID.\n",
      " |  \n",
      " |  ent_type\n",
      " |      RETURNS (uint64): Named entity type.\n",
      " |  \n",
      " |  ent_type_\n",
      " |      RETURNS (unicode): Named entity type.\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#has_vector\n",
      " |  \n",
      " |  head\n",
      " |      The syntactic parent, or \"governor\", of this token.\n",
      " |      \n",
      " |      RETURNS (Token): The token predicted by the parser to be the head of\n",
      " |          the current token.\n",
      " |  \n",
      " |  i\n",
      " |  \n",
      " |  idx\n",
      " |      RETURNS (int): The character offset of the token within the parent\n",
      " |      document.\n",
      " |  \n",
      " |  is_alpha\n",
      " |      RETURNS (bool): Whether the token consists of alpha characters.\n",
      " |      Equivalent to `token.text.isalpha()`.\n",
      " |  \n",
      " |  is_ascii\n",
      " |      RETURNS (bool): Whether the token consists of ASCII characters.\n",
      " |      Equivalent to `[any(ord(c) >= 128 for c in token.text)]`.\n",
      " |  \n",
      " |  is_bracket\n",
      " |      RETURNS (bool): Whether the token is a bracket.\n",
      " |  \n",
      " |  is_currency\n",
      " |      RETURNS (bool): Whether the token is a currency symbol.\n",
      " |  \n",
      " |  is_digit\n",
      " |      RETURNS (bool): Whether the token consists of digits. Equivalent to\n",
      " |      `token.text.isdigit()`.\n",
      " |  \n",
      " |  is_left_punct\n",
      " |      RETURNS (bool): Whether the token is a left punctuation mark.\n",
      " |  \n",
      " |  is_lower\n",
      " |      RETURNS (bool): Whether the token is in lowercase. Equivalent to\n",
      " |      `token.text.islower()`.\n",
      " |  \n",
      " |  is_oov\n",
      " |      RETURNS (bool): Whether the token is out-of-vocabulary.\n",
      " |  \n",
      " |  is_punct\n",
      " |      RETURNS (bool): Whether the token is punctuation.\n",
      " |  \n",
      " |  is_quote\n",
      " |      RETURNS (bool): Whether the token is a quotation mark.\n",
      " |  \n",
      " |  is_right_punct\n",
      " |      RETURNS (bool): Whether the token is a right punctuation mark.\n",
      " |  \n",
      " |  is_sent_end\n",
      " |      A boolean value indicating whether the token ends a sentence.\n",
      " |      `None` if unknown. Defaults to `True` for the last token in the `Doc`.\n",
      " |      \n",
      " |      RETURNS (bool / None): Whether the token ends a sentence.\n",
      " |          None if unknown.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#is_sent_end\n",
      " |  \n",
      " |  is_sent_start\n",
      " |      A boolean value indicating whether the token starts a sentence.\n",
      " |      `None` if unknown. Defaults to `True` for the first token in the `Doc`.\n",
      " |      \n",
      " |      RETURNS (bool / None): Whether the token starts a sentence.\n",
      " |          None if unknown.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#is_sent_start\n",
      " |  \n",
      " |  is_space\n",
      " |      RETURNS (bool): Whether the token consists of whitespace characters.\n",
      " |      Equivalent to `token.text.isspace()`.\n",
      " |  \n",
      " |  is_stop\n",
      " |      RETURNS (bool): Whether the token is a stop word, i.e. part of a\n",
      " |      \"stop list\" defined by the language data.\n",
      " |  \n",
      " |  is_title\n",
      " |      RETURNS (bool): Whether the token is in titlecase. Equivalent to\n",
      " |      `token.text.istitle()`.\n",
      " |  \n",
      " |  is_upper\n",
      " |      RETURNS (bool): Whether the token is in uppercase. Equivalent to\n",
      " |      `token.text.isupper()`\n",
      " |  \n",
      " |  lang\n",
      " |      RETURNS (uint64): ID of the language of the parent document's\n",
      " |      vocabulary.\n",
      " |  \n",
      " |  lang_\n",
      " |      RETURNS (unicode): Language of the parent document's vocabulary,\n",
      " |      e.g. 'en'.\n",
      " |  \n",
      " |  left_edge\n",
      " |      The leftmost token of this token's syntactic descendents.\n",
      " |      \n",
      " |      RETURNS (Token): The first token such that `self.is_ancestor(token)`.\n",
      " |  \n",
      " |  lefts\n",
      " |      The leftward immediate children of the word, in the syntactic\n",
      " |      dependency parse.\n",
      " |      \n",
      " |      YIELDS (Token): A left-child of the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#lefts\n",
      " |  \n",
      " |  lemma\n",
      " |      RETURNS (uint64): ID of the base form of the word, with no\n",
      " |      inflectional suffixes.\n",
      " |  \n",
      " |  lemma_\n",
      " |      RETURNS (unicode): The token lemma, i.e. the base form of the word,\n",
      " |      with no inflectional suffixes.\n",
      " |  \n",
      " |  lex_id\n",
      " |      RETURNS (int): Sequential ID of the token's lexical type.\n",
      " |  \n",
      " |  like_email\n",
      " |      RETURNS (bool): Whether the token resembles an email address.\n",
      " |  \n",
      " |  like_num\n",
      " |      RETURNS (bool): Whether the token resembles a number, e.g. \"10.9\",\n",
      " |      \"10\", \"ten\", etc.\n",
      " |  \n",
      " |  like_url\n",
      " |      RETURNS (bool): Whether the token resembles a URL.\n",
      " |  \n",
      " |  lower\n",
      " |      RETURNS (uint64): ID of the lowercase token text.\n",
      " |  \n",
      " |  lower_\n",
      " |      RETURNS (unicode): The lowercase token text. Equivalent to\n",
      " |      `Token.text.lower()`.\n",
      " |  \n",
      " |  morph\n",
      " |  \n",
      " |  n_lefts\n",
      " |      The number of leftward immediate children of the word, in the\n",
      " |      syntactic dependency parse.\n",
      " |      \n",
      " |      RETURNS (int): The number of leftward immediate children of the\n",
      " |          word, in the syntactic dependency parse.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#n_lefts\n",
      " |  \n",
      " |  n_rights\n",
      " |      The number of rightward immediate children of the word, in the\n",
      " |      syntactic dependency parse.\n",
      " |      \n",
      " |      RETURNS (int): The number of rightward immediate children of the\n",
      " |          word, in the syntactic dependency parse.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#n_rights\n",
      " |  \n",
      " |  norm\n",
      " |      RETURNS (uint64): ID of the token's norm, i.e. a normalised form of\n",
      " |      the token text. Usually set in the language's tokenizer exceptions\n",
      " |      or norm exceptions.\n",
      " |  \n",
      " |  norm_\n",
      " |      RETURNS (unicode): The token's norm, i.e. a normalised form of the\n",
      " |      token text. Usually set in the language's tokenizer exceptions or\n",
      " |      norm exceptions.\n",
      " |  \n",
      " |  orth\n",
      " |      RETURNS (uint64): ID of the verbatim text content.\n",
      " |  \n",
      " |  orth_\n",
      " |      RETURNS (unicode): Verbatim text content (identical to\n",
      " |      `Token.text`). Exists mostly for consistency with the other\n",
      " |      attributes.\n",
      " |  \n",
      " |  pos\n",
      " |      RETURNS (uint64): ID of coarse-grained part-of-speech tag.\n",
      " |  \n",
      " |  pos_\n",
      " |      RETURNS (unicode): Coarse-grained part-of-speech tag.\n",
      " |  \n",
      " |  prefix\n",
      " |      RETURNS (uint64): ID of a length-N substring from the start of the\n",
      " |      token. Defaults to `N=1`.\n",
      " |  \n",
      " |  prefix_\n",
      " |      RETURNS (unicode): A length-N substring from the start of the token.\n",
      " |      Defaults to `N=1`.\n",
      " |  \n",
      " |  prob\n",
      " |      RETURNS (float): Smoothed log probability estimate of token type.\n",
      " |  \n",
      " |  rank\n",
      " |      RETURNS (int): Sequential ID of the token's lexical type, used to\n",
      " |      index into tables, e.g. for word vectors.\n",
      " |  \n",
      " |  right_edge\n",
      " |      The rightmost token of this token's syntactic descendents.\n",
      " |      \n",
      " |      RETURNS (Token): The last token such that `self.is_ancestor(token)`.\n",
      " |  \n",
      " |  rights\n",
      " |      The rightward immediate children of the word, in the syntactic\n",
      " |      dependency parse.\n",
      " |      \n",
      " |      YIELDS (Token): A right-child of the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#rights\n",
      " |  \n",
      " |  sent\n",
      " |      RETURNS (Span): The sentence span that the token is a part of.\n",
      " |  \n",
      " |  sent_start\n",
      " |  \n",
      " |  sentiment\n",
      " |      RETURNS (float): A scalar value indicating the positivity or\n",
      " |      negativity of the token.\n",
      " |  \n",
      " |  shape\n",
      " |      RETURNS (uint64): ID of the token's shape, a transform of the\n",
      " |      tokens's string, to show orthographic features (e.g. \"Xxxx\", \"dd\").\n",
      " |  \n",
      " |  shape_\n",
      " |      RETURNS (unicode): Transform of the tokens's string, to show\n",
      " |      orthographic features. For example, \"Xxxx\" or \"dd\".\n",
      " |  \n",
      " |  string\n",
      " |      Deprecated: Use Token.text_with_ws instead.\n",
      " |  \n",
      " |  subtree\n",
      " |      A sequence containing the token and all the token's syntactic\n",
      " |      descendants.\n",
      " |      \n",
      " |      YIELDS (Token): A descendent token such that\n",
      " |          `self.is_ancestor(descendent) or token == self`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#subtree\n",
      " |  \n",
      " |  suffix\n",
      " |      RETURNS (uint64): ID of a length-N substring from the end of the\n",
      " |      token. Defaults to `N=3`.\n",
      " |  \n",
      " |  suffix_\n",
      " |      RETURNS (unicode): A length-N substring from the end of the token.\n",
      " |      Defaults to `N=3`.\n",
      " |  \n",
      " |  tag\n",
      " |      RETURNS (uint64): ID of fine-grained part-of-speech tag.\n",
      " |  \n",
      " |  tag_\n",
      " |      RETURNS (unicode): Fine-grained part-of-speech tag.\n",
      " |  \n",
      " |  tensor\n",
      " |  \n",
      " |  text\n",
      " |      RETURNS (unicode): The original verbatim text of the token.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      RETURNS (unicode): The text content of the span (with trailing\n",
      " |      whitespace).\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the token's semantics.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#vector\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the token's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#vector_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  whitespace_\n",
      " |      RETURNS (unicode): The trailing whitespace character, if present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy performs a syntactic analysis on the text and determines the part-of-speech of each token, which is then stored under `.pos_` attribute. In the case of *\"incident\"* the part-of-speech is a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also takes care of lemmatization. The lemma is the standardized form of a token. For example, plural nouns are reduced to a singular, and verbs forms are brought back to their infinitive. For example, \"served\" has the lemma \"serve\", \"revolutions\" has the lemma \"revolution\".\n",
    "\n",
    "Lemmas are attached to the `.lemma_` attribute of a token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'serve'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[4].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evolution'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[32].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this useful? It depends on what you want to do. Similar to lowercasing, lemmatization reduces the complexity of (or normalizes) a text: tokens that otherwise have different surface forms are reduced to the same token. If you want to search for (or would like to know the frequency of) a word, normalization is often helpful—you'd capture, for example, \"revolution\" and \"revolutions\" at the same time—unless you are particularly interested verb conjugation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the lemmatized text, we first create an new `list` variable. This will be an empty `list`, but as we iterate over the elements in `doc` as add the lemma of each token (hidden in the `.lemma_` atrribute. \n",
    "\n",
    "Even though this technique (of initializing an empty `list`) is maybe confusing at first, we will repeat it often in the following Notebook. Please take your time to understand the code below, as it contains a few new elements.\n",
    "\n",
    "- creation of an empty `list`\n",
    "- `for` loop\n",
    "- appending items to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'trifle', 'incident', 'thus', 'serve', 'to', 'settle', 'a', 'victory', '.', ' \\n            ', 'now', '-', 'a', 'day', ',', 'a', 'soldier', 'be', 'so', 'much', 'of', 'a', 'machine', 'that', '-PRON-', 'seem', 'simply', 'to', 'go', 'through', 'certain', 'evolution', ',', 'in', 'which', 'there', 'be', 'no', 'opportunity', 'for', 'the', 'display', 'of', 'personal', 'bravery', 'or', 'cowardice', '.', ' \\n            ', '-PRON-', 'do', 'not', 'know', 'what', 'be', 'go', 'on', 'in', 'other', 'part', 'of', 'the', 'field', ',', 'and', 'have', 'no', 'real', 'knowledge', ',', 'till', 'all', 'be', 'over', ',', 'whether', 'the', 'day', 'have', 'be', 'lose', 'or', 'win', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "lemmas = []\n",
    "\n",
    "for t in doc:\n",
    "    lemmas.append(t.lemma_)\n",
    "    \n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iteration is performed by a `for` loop: in the example above we iterate of each `Token` in `doc`. `t` takes the value of each item in turn. We can use `print()` to make this visible. Iteration amounts to repeating the same operation to each element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "trifling\n",
      "incident\n",
      "thus\n",
      "served\n",
      "to\n",
      "settle\n",
      "a\n",
      "victory\n",
      ".\n",
      " \n",
      "            \n",
      "Now\n",
      "-\n",
      "a\n",
      "days\n",
      ",\n",
      "a\n",
      "soldier\n",
      "is\n",
      "so\n",
      "much\n",
      "of\n",
      "a\n",
      "machine\n",
      "that\n",
      "he\n",
      "seems\n",
      "simply\n",
      "to\n",
      "go\n",
      "through\n",
      "certain\n",
      "evolutions\n",
      ",\n",
      "in\n",
      "which\n",
      "there\n",
      "is\n",
      "no\n",
      "opportunity\n",
      "for\n",
      "the\n",
      "display\n",
      "of\n",
      "personal\n",
      "bravery\n",
      "or\n",
      "cowardice\n",
      ".\n",
      " \n",
      "            \n",
      "He\n",
      "does\n",
      "not\n",
      "know\n",
      "what\n",
      "is\n",
      "going\n",
      "on\n",
      "in\n",
      "other\n",
      "parts\n",
      "of\n",
      "the\n",
      "field\n",
      ",\n",
      "and\n",
      "has\n",
      "no\n",
      "real\n",
      "knowledge\n",
      ",\n",
      "till\n",
      "all\n",
      "be\n",
      "over\n",
      ",\n",
      "whether\n",
      "the\n",
      "day\n",
      "has\n",
      "been\n",
      "lost\n",
      "or\n",
      "won\n",
      ".\n",
      "”\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `for` loop traverses through all items in doc and for each token `t` in doc, we can now repeat the same operation.\n",
    "- We access the `.lemma_` attribute of `t`\n",
    "- and append the lemma to the list `lemmas`. `lists.append(item)` adds items to list we initialized at the start of the cell.\n",
    "\n",
    "It doesn't matter what name you use for the **loop variable** `t` (the variable between `for` and `in`) as long as you use it consistently. The examples below hopefully make this clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "trifling\n",
      "incident\n",
      "thus\n",
      "served\n",
      "to\n",
      "settle\n",
      "a\n",
      "victory\n",
      ".\n",
      " \n",
      "            \n",
      "Now\n",
      "-\n",
      "a\n",
      "days\n",
      ",\n",
      "a\n",
      "soldier\n",
      "is\n",
      "so\n",
      "much\n",
      "of\n",
      "a\n",
      "machine\n",
      "that\n",
      "he\n",
      "seems\n",
      "simply\n",
      "to\n",
      "go\n",
      "through\n",
      "certain\n",
      "evolutions\n",
      ",\n",
      "in\n",
      "which\n",
      "there\n",
      "is\n",
      "no\n",
      "opportunity\n",
      "for\n",
      "the\n",
      "display\n",
      "of\n",
      "personal\n",
      "bravery\n",
      "or\n",
      "cowardice\n",
      ".\n",
      " \n",
      "            \n",
      "He\n",
      "does\n",
      "not\n",
      "know\n",
      "what\n",
      "is\n",
      "going\n",
      "on\n",
      "in\n",
      "other\n",
      "parts\n",
      "of\n",
      "the\n",
      "field\n",
      ",\n",
      "and\n",
      "has\n",
      "no\n",
      "real\n",
      "knowledge\n",
      ",\n",
      "till\n",
      "all\n",
      "be\n",
      "over\n",
      ",\n",
      "whether\n",
      "the\n",
      "day\n",
      "has\n",
      "been\n",
      "lost\n",
      "or\n",
      "won\n",
      ".\n",
      "”\n"
     ]
    }
   ],
   "source": [
    "for abracadabra in doc: # you can give the loop variable any name you want\n",
    "    print(abracadabra) # but you should use it consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2_/fcdvqwzs6j75cfr97nggzfn499sjqf/T/ipykernel_38165/2449242135.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# otherwise things will go wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# and this line will raise a NameError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "for t in doc: # otherwise things will go wrong\n",
    "    print(r) # and this line will raise a NameError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also the spacing, lines that end with a colon are followed by indented lines. This part of the Python syntax, removing the indentation will rais an `IndentationError`. Run the code below to convince yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1434422477.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/2_/fcdvqwzs6j75cfr97nggzfn499sjqf/T/ipykernel_38165/1434422477.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(t)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for t in doc: \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we can harvest the part-of-speech of each token. Note that we repeat exactly the same procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'VERB', 'NOUN', 'ADV', 'VERB', 'PART', 'VERB', 'DET', 'NOUN', 'PUNCT', 'SPACE', 'ADV', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'AUX', 'ADV', 'ADJ', 'ADP', 'DET', 'NOUN', 'SCONJ', 'PRON', 'VERB', 'ADV', 'PART', 'VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT', 'ADP', 'DET', 'PRON', 'AUX', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'CCONJ', 'NOUN', 'PUNCT', 'SPACE', 'PRON', 'AUX', 'PART', 'VERB', 'PRON', 'AUX', 'VERB', 'ADP', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'AUX', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'SCONJ', 'DET', 'AUX', 'ADV', 'PUNCT', 'SCONJ', 'DET', 'NOUN', 'AUX', 'AUX', 'VERB', 'CCONJ', 'VERB', 'PUNCT', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "pos = [] # create an empty list with name pos\n",
    "\n",
    "for token in doc: # iterate over all items in doc\n",
    "    pos.append(token.pos_) # append the .pos_ attribute to pos\n",
    "    \n",
    "print(pos) # print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'DET'), ('trifle', 'VERB'), ('incident', 'NOUN'), ('thus', 'ADV'), ('serve', 'VERB'), ('to', 'PART'), ('settle', 'VERB'), ('a', 'DET'), ('victory', 'NOUN'), ('.', 'PUNCT'), (' \\n            ', 'SPACE'), ('now', 'ADV'), ('-', 'PUNCT'), ('a', 'DET'), ('day', 'NOUN'), (',', 'PUNCT'), ('a', 'DET'), ('soldier', 'NOUN'), ('be', 'AUX'), ('so', 'ADV'), ('much', 'ADJ'), ('of', 'ADP'), ('a', 'DET'), ('machine', 'NOUN'), ('that', 'SCONJ'), ('-PRON-', 'PRON'), ('seem', 'VERB'), ('simply', 'ADV'), ('to', 'PART'), ('go', 'VERB'), ('through', 'ADP'), ('certain', 'ADJ'), ('evolution', 'NOUN'), (',', 'PUNCT'), ('in', 'ADP'), ('which', 'DET'), ('there', 'PRON'), ('be', 'AUX'), ('no', 'DET'), ('opportunity', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('display', 'NOUN'), ('of', 'ADP'), ('personal', 'ADJ'), ('bravery', 'NOUN'), ('or', 'CCONJ'), ('cowardice', 'NOUN'), ('.', 'PUNCT'), (' \\n            ', 'SPACE'), ('-PRON-', 'PRON'), ('do', 'AUX'), ('not', 'PART'), ('know', 'VERB'), ('what', 'PRON'), ('be', 'AUX'), ('go', 'VERB'), ('on', 'ADP'), ('in', 'ADP'), ('other', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('field', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('have', 'AUX'), ('no', 'DET'), ('real', 'ADJ'), ('knowledge', 'NOUN'), (',', 'PUNCT'), ('till', 'SCONJ'), ('all', 'DET'), ('be', 'AUX'), ('over', 'ADV'), (',', 'PUNCT'), ('whether', 'SCONJ'), ('the', 'DET'), ('day', 'NOUN'), ('have', 'AUX'), ('be', 'AUX'), ('lose', 'VERB'), ('or', 'CCONJ'), ('win', 'VERB'), ('.', 'PUNCT'), ('\"', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "lemma_pos = []\n",
    "for token in doc:\n",
    "    lemma_pos.append((token.lemma_,token.pos_))\n",
    "    \n",
    "print(lemma_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination of lemmatization and part-of-speech tagging is quite common. It remove certain distinction (verb tense and plurals) but foregrounds that otherwise would have treated as the same word: for example the distinction between `fine` as noun and adjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy has a lot more to offer, and for example you can find Named Entities (places, persons and organisation) in texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 0 7 GPE\n",
      "Berlin 44 50 GPE\n",
      "Kaspar 68 74 PERSON\n",
      "Stanford 106 114 ORG\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Germany is a wonderful country. The city of Berlin is great! Do you Kaspar is still listening? He went to Stanford.\")\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Breakout`:\n",
    "- [Indentation](break_out/indentation.ipynb)\n",
    "- [`for` loop](break_out/loops.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Exercise:\n",
    "\n",
    "The code below will load \"A Tale of Two Cities\" into your Notebook.\n",
    "- apply the SpaCy `nlp` to this text\n",
    "- collect all named entities in a list `ner`\n",
    "- Count the named entities \n",
    "- print the ten most frequent entities\n",
    "- repeat the above but this time collect the `.label_` atribute of a named entity\n",
    "- compute and print the frequency of these labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "book = requests.get('https://www.gutenberg.org/files/98/98-0.txt').content.decode('utf-8') # download book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Exercise\n",
    "\n",
    "You can also use SpaCy for other languages then English. The code below will install a German model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the \"Die Leiden des jungen Werther\" from gutenberg.org and save it in `werther`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "werther = requests.get('https://www.gutenberg.org/cache/epub/2407/pg2407.txt').content.decode('utf-8') # download book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_de = nlp_de(werther)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the Named Entity recognition to `werther`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
